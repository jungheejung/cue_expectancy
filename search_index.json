[["index.html", "52_ROI TTL1 Chapter 1 About 1.1 Usage", " 52_ROI TTL1 Heejung Jung 2023-08-14 Chapter 1 About This is an analysis book written in Markdown. The purpose is to keep track of analyses and summarize findings, while decluttering from result vs. code. 1.1 Usage Each bookdown chapter is an .Rmd file. This .Rmd is migrated from the git repository cue-expectancy and specifically from the folder step02_R. Each .Rmd file was developed as a standalone analysis pipeline. Once validated, the identical .Rmd is migrated to the bookdown folder and edited for bookdown compiling. "],["ch2_expect.html", "Chapter 2 [beh] expectation ~ cue What is the purpose of this notebook? 2.1 Pain 2.2 Vicarious 2.3 Cognitive 2.4 Individual difference analysis", " Chapter 2 [beh] expectation ~ cue What is the purpose of this notebook? Here, I plot the expectation ratings as a function of cue. Main model: lmer(expect_rating ~ cue) Main question: do expectations ratings differ as a function of cue type? If there is a main effect of cue on expectation ratings, does this cue effect differ depending on task type? IV: cue (high / low) DV: expectation rating Code for (taskname in c(&quot;pain&quot;, &quot;vicarious&quot;, &quot;cognitive&quot;)) { dv_keyword &lt;- &quot;expect&quot; model_savefname &lt;- file.path( analysis_dir, paste(&quot;lmer_task-&quot;, taskname, &quot;_rating-&quot;, dv_keyword, &quot;_&quot;, as.character(Sys.Date()), &quot;.txt&quot;, sep = &quot;&quot; ) ) iv &lt;- &quot;param_stimulus_type&quot; dv &lt;- &quot;event02_expect_angle&quot; subject_varkey &lt;- &quot;src_subject_id&quot; print_lmer_output = TRUE exclude &lt;- &quot;sub-0001|sub-0999&quot; # load data, run model, and exclude outliers data &lt;- load_task_social_df(datadir, taskname, subject_varkey, iv, dv, exclude) data$subject = factor(data$src_subject_id) data$stim_name[data$param_stimulus_type == &quot;high_stim&quot;] &lt;- &quot;high&quot; data$stim_name[data$param_stimulus_type == &quot;med_stim&quot;] &lt;- &quot;med&quot; data$stim_name[data$param_stimulus_type == &quot;low_stim&quot;] &lt;- &quot;low&quot; data$stimlin[data$param_stimulus_type == &quot;high_stim&quot;] &lt;- 0.5 data$stimlin[data$param_stimulus_type == &quot;med_stim&quot;] &lt;- 0 data$stimlin[data$param_stimulus_type == &quot;low_stim&quot;] &lt;- -0.5 data$stimquad[data$param_stimulus_type == &quot;high_stim&quot;] &lt;- -0.34 data$stimquad[data$param_stimulus_type == &quot;med_stim&quot;] &lt;- 0.66 data$stimquad[data$param_stimulus_type == &quot;low_stim&quot;] &lt;- -0.34 data$cue_name[data$param_cue_type == &quot;high_cue&quot;] &lt;- &quot;high&quot; data$cue_name[data$param_cue_type == &quot;low_cue&quot;] &lt;- &quot;low&quot; data$cue_con[data$param_cue_type == &quot;high_cue&quot;] &lt;- 0.5 data$cue_con[data$param_cue_type == &quot;low_cue&quot;] &lt;- -0.5 # DATA$levels_ordered &lt;- factor(DATA$param_stimulus_type, levels=c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;)) data$stim_ordered &lt;- factor( data$stim_name, levels = c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;) ) iv &lt;- &quot;stim_ordered&quot; dv &lt;- &quot;event02_expect_angle&quot; print(taskname) model &lt;- lmer(event02_expect_angle ~ cue_con + (1|src_subject_id), data = data) print(summary(model)) } ## [1] &quot;pain&quot; ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: event02_expect_angle ~ cue_con + (1 | src_subject_id) ## Data: data ## ## REML criterion at convergence: 56012.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.1273 -0.6288 -0.0305 0.6197 4.8504 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 831.9 28.84 ## Residual 529.5 23.01 ## Number of obs: 6095, groups: src_subject_id, 114 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 60.9943 2.7230 113.0814 22.40 &lt;2e-16 *** ## cue_con 34.4542 0.5899 5981.2865 58.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## cue_con 0.000 ## [1] &quot;vicarious&quot; ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: event02_expect_angle ~ cue_con + (1 | src_subject_id) ## Data: data ## ## REML criterion at convergence: 57065.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3554 -0.6080 -0.0778 0.5620 6.2802 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 116.5 10.79 ## Residual 293.8 17.14 ## Number of obs: 6656, groups: src_subject_id, 114 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 31.1604 1.0368 112.8038 30.05 &lt;2e-16 *** ## cue_con 33.7887 0.4205 6542.0225 80.36 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## cue_con 0.002 ## [1] &quot;cognitive&quot; ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: event02_expect_angle ~ cue_con + (1 | src_subject_id) ## Data: data ## ## REML criterion at convergence: 58008 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.3780 -0.6120 -0.0800 0.5341 9.3354 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 167.4 12.94 ## Residual 303.3 17.41 ## Number of obs: 6737, groups: src_subject_id, 114 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 33.8866 1.2332 112.1041 27.48 &lt;2e-16 *** ## cue_con 31.7535 0.4246 6622.2852 74.78 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## cue_con 0.000 Code # parameters _____________________________________ # nolint subject_varkey &lt;- &quot;src_subject_id&quot; iv &lt;- &quot;param_cue_type&quot;; iv_keyword &lt;- &quot;cue&quot;; dv &lt;- &quot;event02_expect_angle&quot;; dv_keyword &lt;- &quot;expect&quot; xlab &lt;- &quot;&quot;; ylim = c(0,180); ylab &lt;- &quot;ratings (degree)&quot; subject &lt;- &quot;subject&quot; exclude &lt;- &quot;sub-0001|sub-0003|sub-0004|sub-0005|sub-0025|sub-0999&quot; subjectwise_mean &lt;- &quot;mean_per_sub&quot;; group_mean &lt;- &quot;mean_per_sub_norm_mean&quot;; se &lt;- &quot;se&quot; color_scheme &lt;- if (any(startsWith(dv_keyword, c(&quot;expect&quot;, &quot;Expect&quot;)))) { color_scheme &lt;- c(&quot;#1B9E77&quot;, &quot;#D95F02&quot;) } else { color_scheme &lt;- c(&quot;#4575B4&quot;, &quot;#D73027&quot;) } print_lmer_output &lt;- FALSE ggtitle_phrase &lt;- &quot; - Expectation Rating (degree)&quot; analysis_dir &lt;- file.path(main_dir, &quot;analysis&quot;, &quot;mixedeffect&quot;, &quot;model01_iv-cue_dv-expect&quot;, as.character(Sys.Date())) dir.create(analysis_dir, showWarnings = FALSE, recursive = TRUE) 2.1 Pain For the pain task, what is the effect of cue on expectation ratings? [ INSERT DESCRIPTION ] 2.2 Vicarious For the vicarious task, what is the effect of cue on expectation ratings? [ INSERT DESCRIPTION ] 2.3 Cognitive For the cognitive task, what is the effect of cue on expectation ratings? [ INSERT DESCRIPTION ] 2.4 Individual difference analysis Are cue effects (on expectation ratings) similar across tasks? Using the random slopes of cue effects, here we plot them side by side with all three tasks of pain, cognitive, vicarious. As we can see, there is a high correlation across the random effects of cue across pain-cognitive, pain-vicarious, and cognitive-vicarious. These plots suggest a universal mechansim in the cue-expectancy effect, although some may critic that the cues were identical across tasks, thereby the cue effects are identical due to the stimuli itself, not necessarily a domain-general expectation process. Note: pain Warning: Removed 3 rows containing missing values (geom_point()). vicarious Warning: Removed 1 rows containing missing values (geom_point()). cognitive: Removed 3 rows containing missing values (geom_point()). "],["ch03_cue.html", "Chapter 3 [beh] outcome ~ cue What is the purpose of this notebook? 3.1 Pain 3.2 Vicarious 3.3 Cognitive 3.4 Individual differences analysis: random cue effects 3.5 Individual differences analysis 2: random intercept + random slopes of cue effect", " Chapter 3 [beh] outcome ~ cue What is the purpose of this notebook? Here, I plot the outcome ratings as a function of cue. Main model: lmer(outcome_rating ~ cue) Main question: do outcome ratings differ as a function of cue type? If there is a main effect of cue on outcome ratings, does this cue effect differ depending on task type? IV: cue (high / low) DV: outcome rating FIX: plot statistics in random effect plot - what is broken? Code # parameters _____________________________________ # nolint subject_varkey &lt;- &quot;src_subject_id&quot; iv &lt;- &quot;param_cue_type&quot;; iv_keyword &lt;- &quot;cue&quot;; dv &lt;- &quot;event04_actual_angle&quot;; dv_keyword &lt;- &quot;outcome&quot; xlab &lt;- &quot;&quot;; ylim = c(0,180); ylab &lt;- &quot;ratings (degree)&quot; subject &lt;- &quot;subject&quot; exclude &lt;- &quot;sub-0001|sub-0003|sub-0004|sub-0005|sub-0025|sub-0999&quot; subjectwise_mean &lt;- &quot;mean_per_sub&quot;; group_mean &lt;- &quot;mean_per_sub_norm_mean&quot;; se &lt;- &quot;se&quot; color_scheme &lt;- if (any(startsWith(dv_keyword, c(&quot;expect&quot;, &quot;Expect&quot;)))) { color_scheme &lt;- c(&quot;#1B9E77&quot;, &quot;#D95F02&quot;) } else { color_scheme &lt;- c(&quot;#4575B4&quot;, &quot;#D73027&quot;) } print_lmer_output &lt;- FALSE ggtitle_phrase &lt;- &quot; - Outcome Rating (degree)&quot; analysis_dir &lt;- file.path(main_dir, &quot;analysis&quot;, &quot;mixedeffect&quot;, &quot;model03_iv-cue_dv-outcome&quot;, as.character(Sys.Date())) dir.create(analysis_dir, showWarnings = FALSE, recursive = TRUE) 3.1 Pain For the vicarious task, what is the effect of cue on outcome ratings? [ INSERT DESCRIPTION ] 3.2 Vicarious For the vicarious task, what is the effect of cue on outcome ratings? [ INSERT DESCRIPTION ] 3.3 Cognitive For the cognitive task, what is the effect of cue on outcome ratings? [ INSERT DESCRIPTION ] 3.4 Individual differences analysis: random cue effects Using the random effects from the mixed effects model, I’m plotting the random effect of cue types per task. [ INSERT DESCRIPTION ] Note: * Pain: Warning: Removed 2 rows containing non-finite values (stat_cor()). * Vicarious: Warning: Removed 1 rows containing non-finite values (stat_cor()). * Cognitive: Warning: Removed 2 rows containing non-finite values (stat_cor()). 3.5 Individual differences analysis 2: random intercept + random slopes of cue effect based on Tor’s suggestion, plotting the random efects with the random intercepts as well. not just the cue effects Note: * Pain: Warning: Removed 49 rows containing non-finite values (stat_cor()). * Vicarious: Removed 8 rows containing non-finite values (stat_cor()). * Cognitive: Removed 52 rows containing non-finite values (stat_cor()). "],["ch04_outcome-stim.html", "Chapter 4 [beh] outcome ~ stimulus_intensity What is the purpose of this notebook? 4.1 Pain 4.2 Vicarious 4.3 Cognitive 4.4 for loop 4.5 Lineplot 4.6 individual differences in outcome rating cue effect", " Chapter 4 [beh] outcome ~ stimulus_intensity What is the purpose of this notebook? Here, I plot the outcome ratings as a function of stimulus intensity Main model: lmer(outcome_rating ~ stim) Main question: do outcome ratings differ as a function of stimulus intensity? We should expect to see a linear effect of stimulus intensity. If there is a main effect of cue on expectation ratings, does this cue effect differ depending on task type? IV: stim (high / med / low) DV: outcome rating FIX: plot statistics in random effect plot - what is broken? Code # parameters _____________________________________ # nolint subject_varkey &lt;- &quot;src_subject_id&quot; iv &lt;- &quot;param_stimulus_type&quot;; iv_keyword &lt;- &quot;stim&quot;; dv &lt;- &quot;event04_actual_angle&quot;; dv_keyword &lt;- &quot;outcome&quot; xlab &lt;- &quot;&quot;; ylim = c(0,180); ylab &lt;- &quot;ratings (degree)&quot; subject &lt;- &quot;subject&quot; exclude &lt;- &quot;sub-0001|sub-0003|sub-0004|sub-0005|sub-0025|sub-0999&quot; subjectwise_mean &lt;- &quot;mean_per_sub&quot;; group_mean &lt;- &quot;mean_per_sub_norm_mean&quot;; se &lt;- &quot;se&quot; color_scheme &lt;- if (any(startsWith(dv_keyword, c(&quot;expect&quot;, &quot;Expect&quot;)))) { color_scheme &lt;- c(&quot;#1B9E77&quot;, &quot;#D95F02&quot;) } else { color_scheme &lt;- c(&quot;#4575B4&quot;, &quot;#D73027&quot;) } print_lmer_output &lt;- FALSE ggtitle_phrase &lt;- &quot; - Outcome Rating (degree)&quot; analysis_dir &lt;- file.path(main_dir, &quot;analysis&quot;, &quot;mixedeffect&quot;, paste0(&quot;model04_iv-&quot;,iv_keyword,&quot;_dv-&quot;,dv_keyword), as.character(Sys.Date())) dir.create(analysis_dir, showWarnings = FALSE, recursive = TRUE) 4.1 Pain For the pain task, what is the effect of stimulus intensity on outcome ratings? [ INSERT DESCRIPTION ] ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. 4.2 Vicarious For the vicarious task, what is the effect of stimulus intensity on outcome ratings? [ INSERT DESCRIPTION ] ## Warning: Model failed to converge with 1 negative eigenvalue: -8.5e+01 ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill 4.3 Cognitive For the cognitive task, what is the effect of stimulus intensity on outcome ratings? [ INSERT DESCRIPTION ] ## Warning: Model failed to converge with 1 negative eigenvalue: -1.1e+02 ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill 4.4 for loop ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## as.formula(reformulate(c(iv, sprintf(&quot;(%s|%s)&quot;, iv, subject_keyword)), ## response = dv)) ## Data: df ## ## REML criterion at convergence: 52938.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5365 -0.5608 -0.0002 0.5695 4.6143 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 952.46 30.862 ## param_stimulus_typelow_stim 127.22 11.279 -0.47 ## param_stimulus_typemed_stim 29.79 5.458 -0.24 0.97 ## Residual 448.14 21.169 ## Number of obs: 5851, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 80.3242 2.9886 109.1411 26.88 &lt;2e-16 *** ## param_stimulus_typelow_stim -29.2521 1.2974 107.5783 -22.55 &lt;2e-16 *** ## param_stimulus_typemed_stim -13.7621 0.8652 148.5592 -15.90 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) prm_stmls_typl_ ## prm_stmls_typl_ -0.455 ## prm_stmls_typm_ -0.236 0.718 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill ## Coordinate system already present. Adding new coordinate system, which will ## replace the existing one. ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## as.formula(reformulate(c(iv, sprintf(&quot;(%s|%s)&quot;, iv, subject_keyword)), ## response = dv)) ## Data: df ## ## REML criterion at convergence: 56882.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.5482 -0.5779 -0.1812 0.4475 6.1884 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 283.44 16.836 ## param_stimulus_typelow_stim 172.13 13.120 -0.88 ## param_stimulus_typemed_stim 98.63 9.931 -0.85 1.00 ## Residual 448.25 21.172 ## Number of obs: 6313, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 40.822 1.681 108.598 24.29 &lt;2e-16 *** ## param_stimulus_typelow_stim -24.936 1.426 109.210 -17.49 &lt;2e-16 *** ## param_stimulus_typemed_stim -17.614 1.162 114.541 -15.15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) prm_stmls_typl_ ## prm_stmls_typl_ -0.837 ## prm_stmls_typm_ -0.784 0.862 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill ## Coordinate system already present. Adding new coordinate system, which will ## replace the existing one. ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## as.formula(reformulate(c(iv, sprintf(&quot;(%s|%s)&quot;, iv, subject_keyword)), ## response = dv)) ## Data: df ## ## REML criterion at convergence: 54866.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7173 -0.6283 -0.1660 0.4545 7.0548 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 176.9233 13.3013 ## param_stimulus_typelow_stim 8.2230 2.8676 -0.75 ## param_stimulus_typemed_stim 0.4181 0.6466 0.37 0.33 ## Residual 374.7596 19.3587 ## Number of obs: 6220, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 31.4642 1.3417 109.5625 23.451 &lt;2e-16 *** ## param_stimulus_typelow_stim -8.1551 0.6623 106.7910 -12.313 &lt;2e-16 *** ## param_stimulus_typemed_stim -1.0096 0.6056 718.4229 -1.667 0.0959 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) prm_stmls_typl_ ## prm_stmls_typl_ -0.500 ## prm_stmls_typm_ -0.186 0.465 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill ## Coordinate system already present. Adding new coordinate system, which will ## replace the existing one. 4.5 Lineplot Code library(ggpubr) DATA = as.data.frame(combined_se_calc_cooksd) color = c( &quot;#4575B4&quot;, &quot;#D73027&quot;) LINEIV1 = &quot;stim_ordered&quot; LINEIV2 = &quot;cue_ordered&quot; MEAN = &quot;mean_per_sub_norm_mean&quot; ERROR = &quot;se&quot; dv_keyword = &quot;actual&quot; p1 = plot_lineplot_onefactor(DATA, &#39;pain&#39;, LINEIV1, MEAN, ERROR, color, xlab = &quot;Stimulus intensity&quot; , ylab= &quot;Outcome rating&quot;, ggtitle = &#39;pain&#39; ) p2 = plot_lineplot_onefactor(DATA,&#39;vicarious&#39;, LINEIV1, MEAN, ERROR, color,xlab = &quot;Stimulus intensity&quot; , ylab= &quot;Outcome rating&quot;,ggtitle = &#39;vicarious&#39;) p3 = plot_lineplot_onefactor(DATA, &#39;cognitive&#39;, LINEIV1, MEAN, ERROR, color,xlab = &quot;Stimulus intensity&quot; , ylab= &quot;Outcome rating&quot;,ggtitle = &#39;cognitive&#39;) #grid.arrange(p1, p2, p3, ncol=3 , common.legend = TRUE) ggpubr::ggarrange(p1,p2,p3,ncol = 3, nrow = 1, common.legend = TRUE,legend = &quot;bottom&quot;) Code plot_filename = file.path(analysis_dir, paste(&#39;lineplot_task-all_rating-&#39;,dv_keyword,&#39;.png&#39;, sep = &quot;&quot;)) ggsave(plot_filename, width = 15, height = 6) 4.6 individual differences in outcome rating cue effect [ INSERT DESCRIPTION ] "],["ch05_outcome-cueXstim.html", "Chapter 5 [beh] outcome_rating ~ cue * stim What is the purpose of this notebook? 5.1 model 03 iv-cuecontrast dv-outcome", " Chapter 5 [beh] outcome_rating ~ cue * stim What is the purpose of this notebook? Here, I plot the outcome ratings as a function of cue and stimulus intensity. Main model: lmer(outcome_rating ~ cue * stim) Main question: do outcome ratings differ as a function of cue type and stimulus intensity? If there is a main effect of cue on outcome ratings, does this cue effect differ depending on task type? Is there an interaction between the two factors? IV: cue (high / low) stim (high / med / low) DV: outcome rating 5.1 model 03 iv-cuecontrast dv-outcome 5.1.1 model 03 3-2. individual difference 5.1.2 model 04 iv-cue-stim dv-outcome 5.1.3 Nov 17 lmer Code # stim_con1 &lt;- &quot;stim_con_linear&quot; # stim_con2 &lt;- &quot;stim_con_quad&quot; # iv1 &lt;- &quot;social_cue&quot; # dv &lt;- &quot;event04_actual_angle&quot; fullmodel &lt;- lmer(event04_actual_angle ~ 1+ social_cue + stim_con_linear + stim_con_quad + social_cue:stim_con_linear + social_cue:stim_con_quad + (1+ social_cue + stim_con_linear + stim_con_quad+ social_cue:stim_con_linear | src_subject_id), data=data) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Code summary(fullmodel) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## event04_actual_angle ~ 1 + social_cue + stim_con_linear + stim_con_quad + ## social_cue:stim_con_linear + social_cue:stim_con_quad + (1 + ## social_cue + stim_con_linear + stim_con_quad + social_cue:stim_con_linear | ## src_subject_id) ## Data: data ## ## REML criterion at convergence: 54523.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.4932 -0.6227 -0.1494 0.4685 7.0856 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 160.757 12.679 ## social_cue 27.920 5.284 0.37 ## stim_con_linear 11.126 3.336 0.60 -0.04 ## stim_con_quad 2.765 1.663 0.79 0.24 0.72 ## social_cue:stim_con_linear 3.197 1.788 -0.29 0.68 -0.08 ## Residual 349.846 18.704 ## ## ## ## ## ## -0.15 ## ## Number of obs: 6220, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 28.4134 1.2360 109.1664 22.988 &lt; 2e-16 *** ## social_cue 8.0490 0.7017 106.1701 11.470 &lt; 2e-16 *** ## stim_con_linear 8.1671 0.6657 106.6467 12.268 &lt; 2e-16 *** ## stim_con_quad 3.0904 0.5350 111.0283 5.777 7.05e-08 *** ## social_cue:stim_con_linear 2.5872 1.1738 1373.9978 2.204 0.0277 * ## social_cue:stim_con_quad -1.6740 1.0200 5843.5215 -1.641 0.1008 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) socl_c stm_cn_l stm_cn_q scl_c:stm_cn_l ## social_cue 0.262 ## stim_cn_lnr 0.284 -0.015 ## stim_con_qd 0.232 0.049 0.106 ## scl_c:stm_cn_l -0.041 0.074 -0.002 -0.008 ## scl_c:stm_cn_q -0.001 0.004 -0.002 0.000 0.001 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 5.1.4 model 04 4-2 individual differences in cue effects 5.1.5 model 04 4-3 scatter plot 5.1.6 model 04 4-4 lineplot "],["clinical-trials.html", "Chapter 6 Clinical trials 6.1 cue contrast average across intensity 6.2 cue contrast average across expectation", " Chapter 6 Clinical trials 6.1 cue contrast average across intensity ## [1] &quot;pain&quot; ## [1] 8.203947 ## [1] 0.8871599 ## [1] &quot;high vs. low cue&quot; ## [1] &quot;low&quot; &quot;61.6884121864272&quot; &quot;2.860880140792&quot; ## [1] &quot;high&quot; &quot;70.2234946843967&quot; &quot;2.85365310068339&quot; ## [1] &quot;vicarious&quot; ## [1] 7.69279 ## [1] 0.6584873 ## [1] &quot;high vs. low cue&quot; ## [1] &quot;low&quot; &quot;22.7808026788692&quot; &quot;1.0440409512757&quot; ## [1] &quot;high&quot; &quot;30.636407755966&quot; &quot;1.20480098494488&quot; ## [1] &quot;cognitive&quot; ## [1] 8.019356 ## [1] 0.7038933 ## [1] &quot;high vs. low cue&quot; ## [1] &quot;low&quot; &quot;24.308987672219&quot; &quot;1.19373008209444&quot; ## [1] &quot;high&quot; &quot;32.34623546235&quot; &quot;1.37653031156445&quot; 6.2 cue contrast average across expectation ## [1] &quot;pain&quot; ## [1] 35.05694 ## [1] 1.989724 ## [1] &quot;high vs. low cue&quot; ## [1] &quot;low&quot; &quot;44.6580941421071&quot; &quot;3.02430373086043&quot; ## [1] &quot;high&quot; &quot;79.4644108331637&quot; &quot;2.85584321656255&quot; ## [1] &quot;vicarious&quot; ## [1] 33.25123 ## [1] 1.503149 ## [1] &quot;high vs. low cue&quot; ## [1] &quot;low&quot; &quot;14.9314711535258&quot; &quot;1.00860750130232&quot; ## [1] &quot;high&quot; &quot;48.146271174259&quot; &quot;1.54236667339445&quot; ## [1] &quot;cognitive&quot; ## [1] 30.7638 ## [1] 1.53046 ## [1] &quot;high vs. low cue&quot; ## [1] &quot;low&quot; &quot;18.5956241315907&quot; &quot;1.20836045474955&quot; ## [1] &quot;high&quot; &quot;49.3940294143433&quot; &quot;1.73640570707356&quot; "],["ch06_Jepma.html", "Chapter 7 expect-actual ~ cue * trial 7.1 Overview 7.2 plot 1 - one run, average across participants 7.3 plot 2 - average across participant, but spread all 6 runs in one x axis 7.4 vicarious 7.5 cognitive 7.6 within subject vicarious 7.7 Tor request – only outcome ratings. 3 tasks 7.8 Tor request – only expect ratings. 3 tasks 7.9 lmer 7.10 Do current expectation ratings predict outcome ratings? 7.11 Additional analysis", " Chapter 7 expect-actual ~ cue * trial 7.1 Overview The purpose of this markdown is to benchmark the plots from Jepma et al. (2018). Here, we plot the expectancy ratings and the actual ratings, with the high and low cues – in one panel. Some thoughts, TODOs plot 2. some runs were repeated or omitted for the subjects that have more than 72 trials. I need to identify that list and work on the behavioral data. I need to check whether the counterbalancing was done correctly. load data and combine participant data Code main_dir = dirname(dirname(getwd())) datadir = file.path(main_dir, &#39;data&#39;, &#39;beh&#39;, &#39;beh02_preproc&#39;) # parameters _____________________________________ # nolint subject_varkey &lt;- &quot;src_subject_id&quot; iv &lt;- &quot;param_cue_type&quot; dv &lt;- &quot;event03_RT&quot; dv_keyword &lt;- &quot;RT&quot; xlab &lt;- &quot;&quot; taskname &lt;- &quot;pain&quot; ylab &lt;- &quot;ratings (degree)&quot; subject &lt;- &quot;subject&quot; exclude &lt;- &quot;sub-0001|sub-0003|sub-0004|sub-0005|sub-0025|sub-0999&quot; #/ &quot;sub-0999|sub-0001|sub-0002|sub-0003|sub-0004|sub-0005|sub-0006|sub-0007|sub-0008|sub-0009|sub-0010|sub-0011&quot; # load data _____________________________________ data &lt;- load_task_social_df(datadir, taskname = taskname, subject_varkey = subject_varkey, iv = iv, exclude = exclude) data$event03_RT &lt;- data$event03_stimulusC_reseponseonset - data$event03_stimulus_displayonset # data[&#39;event03_RT&#39;], data.event03_RT - pandas analysis_dir &lt;- file.path(main_dir, &quot;analysis&quot;, &quot;mixedeffect&quot;, &quot;model06_iv-cue-trial_dv-expect-actual&quot;, as.character(Sys.Date())) dir.create(analysis_dir, showWarnings = FALSE, recursive = TRUE) Code summary(data) ## src_subject_id session_id param_task_name param_run_num ## Min. : 2.00 Min. :1.000 Length:6492 Min. :-2.000 ## 1st Qu.: 37.00 1st Qu.:1.000 Class :character 1st Qu.: 2.000 ## Median : 73.00 Median :3.000 Mode :character Median : 3.000 ## Mean : 69.86 Mean :2.595 Mean : 3.462 ## 3rd Qu.:101.00 3rd Qu.:4.000 3rd Qu.: 5.000 ## Max. :133.00 Max. :4.000 Max. : 6.000 ## ## param_counterbalance_ver param_counterbalance_block_num param_cue_type ## Min. :1.000 Min. :1.000 Length:6492 ## 1st Qu.:2.000 1st Qu.:1.000 Class :character ## Median :3.000 Median :2.000 Mode :character ## Mean :3.157 Mean :1.503 ## 3rd Qu.:4.000 3rd Qu.:2.000 ## Max. :5.000 Max. :2.000 ## ## param_stimulus_type param_cond_type param_trigger_onset param_start_biopac ## Length:6492 Min. :1.0 Min. :1.615e+09 Min. :1.615e+09 ## Class :character 1st Qu.:2.0 1st Qu.:1.627e+09 1st Qu.:1.627e+09 ## Mode :character Median :3.5 Median :1.632e+09 Median :1.632e+09 ## Mean :3.5 Mean :1.634e+09 Mean :1.634e+09 ## 3rd Qu.:5.0 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 ## Max. :6.0 Max. :1.657e+09 Max. :1.657e+09 ## ## ITI_onset ITI_biopac ITI_duration event01_cue_onset ## Min. :1.615e+09 Min. :1.615e+09 Min. : 0.00281 Min. :1.615e+09 ## 1st Qu.:1.627e+09 1st Qu.:1.627e+09 1st Qu.: 1.56340 1st Qu.:1.627e+09 ## Median :1.632e+09 Median :1.632e+09 Median : 3.26975 Median :1.632e+09 ## Mean :1.634e+09 Mean :1.634e+09 Mean : 4.44243 Mean :1.634e+09 ## 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 3rd Qu.: 6.65337 3rd Qu.:1.644e+09 ## Max. :1.657e+09 Max. :1.657e+09 Max. :17.07488 Max. :1.657e+09 ## ## event01_cue_biopac event01_cue_type event01_cue_filename ## Min. :1.615e+09 Length:6492 Length:6492 ## 1st Qu.:1.627e+09 Class :character Class :character ## Median :1.632e+09 Mode :character Mode :character ## Mean :1.634e+09 ## 3rd Qu.:1.644e+09 ## Max. :1.657e+09 ## ## ISI01_onset ISI01_biopac ISI01_duration ## Min. :1.615e+09 Min. :1.615e+09 Min. :0.00396 ## 1st Qu.:1.627e+09 1st Qu.:1.627e+09 1st Qu.:0.99133 ## Median :1.632e+09 Median :1.632e+09 Median :1.39215 ## Mean :1.634e+09 Mean :1.634e+09 Mean :1.47844 ## 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 3rd Qu.:1.98314 ## Max. :1.657e+09 Max. :1.657e+09 Max. :2.89685 ## ## event02_expect_displayonset event02_expect_biopac event02_expect_responseonset ## Min. :1.615e+09 Min. :1.615e+09 Min. :1.615e+09 ## 1st Qu.:1.627e+09 1st Qu.:1.627e+09 1st Qu.:1.627e+09 ## Median :1.632e+09 Median :1.632e+09 Median :1.632e+09 ## Mean :1.634e+09 Mean :1.634e+09 Mean :1.634e+09 ## 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 3rd Qu.:1.643e+09 ## Max. :1.657e+09 Max. :1.657e+09 Max. :1.657e+09 ## NA&#39;s :661 ## event02_expect_RT event02_expect_angle event02_expect_angle_label ## Min. :0.6504 Min. : 0.00 Length:6492 ## 1st Qu.:1.6341 1st Qu.: 30.18 Class :character ## Median :2.0517 Median : 58.56 Mode :character ## Mean :2.1397 Mean : 62.94 ## 3rd Qu.:2.5678 3rd Qu.: 90.00 ## Max. :3.9912 Max. :180.00 ## NA&#39;s :661 NA&#39;s :661 ## ISI02_onset ISI02_biopac ISI02_duration ## Min. :1.615e+09 Min. :1.615e+09 Min. : 0.1422 ## 1st Qu.:1.627e+09 1st Qu.:1.627e+09 1st Qu.: 1.8599 ## Median :1.632e+09 Median :1.632e+09 Median : 4.3664 ## Mean :1.634e+09 Mean :1.634e+09 Mean : 4.4542 ## 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 3rd Qu.: 6.2697 ## Max. :1.657e+09 Max. :1.657e+09 Max. :20.0723 ## ## event03_stimulus_type event03_stimulus_displayonset event03_stimulus_biopac ## Length:6492 Min. :1.615e+09 Min. :1.615e+09 ## Class :character 1st Qu.:1.627e+09 1st Qu.:1.627e+09 ## Mode :character Median :1.632e+09 Median :1.632e+09 ## Mean :1.634e+09 Mean :1.634e+09 ## 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 ## Max. :1.657e+09 Max. :1.657e+09 ## ## event03_stimulus_C_stim_match event03_stimulusC_response ## Mode:logical Min. :0 ## NA&#39;s:6492 1st Qu.:0 ## Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## ## event03_stimulusC_responsekeyname event03_stimulusC_reseponseonset ## Mode:logical Min. :0 ## NA&#39;s:6492 1st Qu.:0 ## Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## ## event03_stimulusC_RT ISI03_onset ISI03_biopac ISI03_duration ## Min. :0 Min. :1.615e+09 Min. :1.615e+09 Min. : 0.4788 ## 1st Qu.:0 1st Qu.:1.627e+09 1st Qu.:1.627e+09 1st Qu.: 2.3846 ## Median :0 Median :1.632e+09 Median :1.632e+09 Median : 4.0370 ## Mean :0 Mean :1.634e+09 Mean :1.634e+09 Mean : 4.4870 ## 3rd Qu.:0 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 3rd Qu.: 5.8864 ## Max. :0 Max. :1.657e+09 Max. :1.657e+09 Max. :17.6951 ## ## event04_actual_displayonset event04_actual_biopac event04_actual_responseonset ## Min. :1.615e+09 Min. :1.615e+09 Min. :1.615e+09 ## 1st Qu.:1.627e+09 1st Qu.:1.627e+09 1st Qu.:1.627e+09 ## Median :1.632e+09 Median :1.632e+09 Median :1.631e+09 ## Mean :1.634e+09 Mean :1.634e+09 Mean :1.634e+09 ## 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 3rd Qu.:1.643e+09 ## Max. :1.657e+09 Max. :1.657e+09 Max. :1.657e+09 ## NA&#39;s :638 ## event04_actual_RT event04_actual_angle event04_actual_angle_label ## Min. :0.0168 Min. : 0.00 Length:6492 ## 1st Qu.:1.9197 1st Qu.: 38.80 Class :character ## Median :2.3510 Median : 60.77 Mode :character ## Mean :2.4005 Mean : 66.33 ## 3rd Qu.:2.8512 3rd Qu.: 88.38 ## Max. :3.9930 Max. :180.00 ## NA&#39;s :638 NA&#39;s :641 ## param_end_instruct_onset param_end_biopac param_experiment_duration ## Min. :1.615e+09 Min. :1.615e+09 Min. :398.1 ## 1st Qu.:1.627e+09 1st Qu.:1.627e+09 1st Qu.:398.6 ## Median :1.632e+09 Median :1.632e+09 Median :398.8 ## Mean :1.634e+09 Mean :1.634e+09 Mean :398.8 ## 3rd Qu.:1.644e+09 3rd Qu.:1.644e+09 3rd Qu.:399.0 ## Max. :1.657e+09 Max. :1.657e+09 Max. :399.5 ## ## event03_stimulus_P_trigger event03_stimulus_P_delay_between_medoc ## Length:6492 Min. :0 ## Class :character 1st Qu.:0 ## Mode :character Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## ## event03_stimulus_V_patientid event03_stimulus_V_filename ## Mode:logical Mode:logical ## NA&#39;s:6492 NA&#39;s:6492 ## ## ## ## ## ## event03_stimulus_C_stim_num event03_stimulus_C_stim_filename ## Min. :0 Mode:logical ## 1st Qu.:0 NA&#39;s:6492 ## Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## ## delay_between_medoc subject event03_RT ## Min. :0.01409 98 : 96 Min. :-1.657e+09 ## 1st Qu.:0.03728 6 : 72 1st Qu.:-1.644e+09 ## Median :0.04537 9 : 72 Median :-1.632e+09 ## Mean :0.04818 10 : 72 Mean :-1.634e+09 ## 3rd Qu.:0.05712 18 : 72 3rd Qu.:-1.627e+09 ## Max. :2.03502 29 : 72 Max. :-1.615e+09 ## (Other):6036 Code # data(data, package = &#39;visibly&#39;) myvars &lt;- names(data) %in% c( &quot;event02_expect_angle&quot;, &quot;event02_expect_RT&quot;, &quot;event04_actual_angle&quot;, &quot;event04_actual_RT&quot;, &quot;event01_cue_onset&quot;) newdata &lt;- data[myvars] # numdata &lt;- unlist(lapply(data, is.numeric), use.names = FALSE) data_naomit &lt;- na.omit(newdata) cor_matrix = cor(data_naomit) corr_heat(cor_matrix) ## No FA options specified, using psych package defaults. ## Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : I ## am sorry, to do these rotations requires the GPArotation package to be ## installed Code ISIvars &lt;- names(data) %in% c( &quot;ISI01_duration&quot;, &quot;ISI02_duration&quot;, &quot;ISI03_duration&quot;) ISIdata &lt;- data[ISIvars] # numdata &lt;- unlist(lapply(data, is.numeric), use.names = FALSE) ISIdata_naomit &lt;- na.omit(ISIdata) ISIcor_matrix = cor(ISIdata_naomit) corr_heat(ISIcor_matrix) ## No FA options specified, using psych package defaults. Code car::vif(lm(event04_actual_angle ~ event02_expect_angle + event02_expect_RT + event04_actual_RT, dat = data_naomit)) ## event02_expect_angle event02_expect_RT event04_actual_RT ## 1.019462 1.084419 1.099422 7.2 plot 1 - one run, average across participants Code # subject # run # param_cue # param_stim # rating_type # rating_value data_trial= data %&gt;% arrange(src_subject_id, session_id, param_run_num) %&gt;% group_by(src_subject_id) %&gt;% mutate(trial_index = rep_len(1:12, length.out = n())) Code data_long = data_trial %&gt;% pivot_longer(cols = c(&#39;event02_expect_angle&#39;, &#39;event04_actual_angle&#39;), names_to = &quot;rating_type&quot;, values_to = &quot;rating_value&quot;) Code # # PLOT data_long$cue_name[data_long$param_cue_type == &quot;high_cue&quot;] &lt;- &quot;high cue&quot; ## Warning: Unknown or uninitialised column: `cue_name`. Code data_long$cue_name[data_long$param_cue_type == &quot;low_cue&quot;] &lt;- &quot;low cue&quot; data_long$stim_name[data_long$param_stimulus_type == &quot;high_stim&quot;] &lt;- &quot;high&quot; ## Warning: Unknown or uninitialised column: `stim_name`. Code data_long$stim_name[data_long$param_stimulus_type == &quot;med_stim&quot;] &lt;- &quot;med&quot; data_long$stim_name[data_long$param_stimulus_type == &quot;low_stim&quot;] &lt;- &quot;low&quot; data_long$stim_ordered &lt;- factor( data_long$stim_name, levels = c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;) ) data_long$cue_ordered &lt;- factor( data_long$cue_name, levels = c(&quot;low cue&quot;, &quot;high cue&quot;) ) subject &lt;- &quot;src_subject_id&quot; model_iv1 &lt;- &quot;stim_ordered&quot; model_iv2 &lt;- &quot;cue_ordered&quot; rating &lt;- &quot;rating_type&quot; dv &lt;- &quot;rating_value&quot; trialorder_subjectwise &lt;- meanSummary( data_long, c(subject, model_iv2, rating, &quot;trial_index&quot;), dv ) subjectwise_naomit &lt;- na.omit(trialorder_subjectwise) trialorder_groupwise &lt;- summarySEwithin( data = subjectwise_naomit, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;cue_ordered&quot;, &quot;rating_type&quot;, &quot;trial_index&quot;), idvar = subject ) ## ## Attaching package: &#39;raincloudplots&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## GeomFlatViolin ## Automatically converting the following non-factors to factors: rating_type, trial_index Code trialorder_subjectwise$rating_type_key &lt;- mapvalues(trialorder_subjectwise$rating_type, from = c(&quot;event02_expect_angle&quot;, &quot;event04_actual_angle&quot;), to = c(&quot;expect&quot;, &quot;actual&quot;)) trialorder_groupwise$rating_type_key &lt;- mapvalues(trialorder_groupwise$rating_type, from = c(&quot;event02_expect_angle&quot;, &quot;event04_actual_angle&quot;), to = c(&quot;expect&quot;, &quot;actual&quot;)) actual_trialorder_groupwise &lt;- trialorder_groupwise[which(trialorder_groupwise$rating_type_key == &quot;actual&quot;),] expect_trialorder_groupwise &lt;-trialorder_groupwise[which(trialorder_groupwise$rating_type_key == &quot;expect&quot;),] actual_trialorder_subjectwise &lt;- trialorder_subjectwise[which(trialorder_subjectwise$rating_type_key == &quot;actual&quot;),] expect_trialorder_subjectwise &lt;-trialorder_subjectwise[which(trialorder_subjectwise$rating_type_key == &quot;expect&quot;),] ggplot Code # * dataset: trialorder_groupwise # * x-axis: trial_index (sorted) # * y-axis: rating # * group: cue_ordered, rating_type # * DV: mean_per_sub_norm_mean # * error bar: se iv1 = &quot;trial_index&quot; iv2 = &quot;cue_ordered&quot; data = g &lt;- ggplot( data = trialorder_groupwise, aes(x = trial_index, y = mean_per_sub_norm_mean, color = cue_ordered, group = rating_type_key ) ) + geom_point( data = trialorder_groupwise, aes( shape = as.character(rating_type_key), x =trial_index, y = mean_per_sub_norm_mean, group = rating_type_key, #color = cue_ordered ), #position = position_jitter(width = .05), size = 3 ) + scale_shape_manual(values=c(16, 21))+ # geom_point( # data = trialorder_subjectwise, # aes( # x = as.numeric(trial_index) - .15, # y = mean_per_sub, # color = cue_ordered # ), # position = position_jitter(width = .05), # size = 1, alpha = 0.8, shape = 20 # ) + geom_errorbar( data = trialorder_groupwise, aes( x = as.numeric(trial_index), y = mean_per_sub_norm_mean, group = rating_type_key, colour = cue_ordered, ymin = mean_per_sub_norm_mean - se, ymax = mean_per_sub_norm_mean + se ), width = .01, size = 0.5 ) + scale_color_manual(values = c(&quot;high cue&quot; = &quot;red&quot;, &quot;low cue&quot; = &quot;blue&quot;)) + xlab(&quot;no. of trials&quot;) + ylab(&quot;rating&quot;) + ylim(0,100) + theme_bw() + theme( axis.text.x = element_text(size = 10), axis.text.y = element_text(size = 10), axis.title.x = element_text(size = 15), axis.title.y = element_text(size = 15)) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Code g 7.3 plot 2 - average across participant, but spread all 6 runs in one x axis load data and combine participant data Code main_dir = dirname(dirname(getwd())) datadir = file.path(main_dir, &#39;data&#39;, &#39;beh&#39;, &#39;beh02_preproc&#39;) # parameters _____________________________________ # nolint subject_varkey &lt;- &quot;src_subject_id&quot; iv &lt;- &quot;param_cue_type&quot; dv &lt;- &quot;event03_RT&quot; dv_keyword &lt;- &quot;RT&quot; xlab &lt;- &quot;&quot; taskname &lt;- &quot;pain&quot; ylab &lt;- &quot;ratings (degree)&quot; subject &lt;- &quot;subject&quot; exclude &lt;- &quot;sub-0999|sub-0001|sub-0002|sub-0003|sub-0004|sub-0005|sub-0006|sub-0007|sub-0008|sub-0009|sub-0010|sub-0011&quot; # load data _____________________________________ data &lt;- load_task_social_df(datadir, taskname = taskname, subject_varkey = subject_varkey, iv = iv, exclude = exclude) p2 :: check number of trials per participant Code data_p2= data %&gt;% arrange(src_subject_id ) %&gt;% group_by(src_subject_id) %&gt;% mutate(trial_index = row_number()) # df_clean &lt;- data_p2[complete.cases(data_p2$event02_expect_angle), ] df_clean &lt;- data_p2[complete.cases(data_p2$event04_actual_angle), ] 7.3.0.1 check if every participant has maximum of 72 trials. Anything beyond that is erroneous Code # Assuming participant IDs are in a column named &quot;participant_id&quot; # and row numbers are in a column named &quot;row_number&quot; # Replace &quot;your_data&quot; with the name of your dataset # Calculate the maximum row number for each participant max_rows &lt;- aggregate(trial_index ~ src_subject_id, data_p2, max) # Check if any participant&#39;s maximum row number is not 72 max_rows[max_rows$trial_index &gt; 72,]$src_subject_id ## [1] 98 p2 :: identify erroneous participant Code # participants who have more than 72 trials will be flagged # excluded for now # TODO: resolve subject 98 count_trial &lt;- df_clean %&gt;% count(&quot;src_subject_id&quot;) count_trial[count_trial$freq &gt; 72,] ## [1] src_subject_id freq ## &lt;0 rows&gt; (or 0-length row.names) Code count_trial[count_trial$freq &gt; 60,]$src_subject_id ## [1] 18 29 31 33 34 36 37 38 39 43 44 46 50 51 52 53 55 57 58 ## [20] 60 61 62 65 73 74 78 80 86 87 88 90 91 93 94 95 99 100 101 ## [39] 105 106 109 111 115 116 122 124 126 127 128 130 132 133 p2 :: convert to long form Code df_clean &lt;- df_clean[df_clean$src_subject_id != 98, ] data_p2_long = df_clean %&gt;% pivot_longer(cols = c(&#39;event02_expect_angle&#39;, &#39;event04_actual_angle&#39;), names_to = &quot;rating_type&quot;, values_to = &quot;rating_value&quot;) p2 :: plot data I’m plotting all of the trials per participant. In this case, there is no trialwise variability, because we’re plotting all 72 trials. Averaging across participants will be the only source of variability, reflected in the error bars Code # PLOT # I&#39;m plotting data_p2_long$cue_name[data_p2_long$param_cue_type == &quot;high_cue&quot;] &lt;- &quot;high cue&quot; ## Warning: Unknown or uninitialised column: `cue_name`. Code data_p2_long$cue_name[data_p2_long$param_cue_type == &quot;low_cue&quot;] &lt;- &quot;low cue&quot; data_p2_long$stim_name[data_p2_long$param_stimulus_type == &quot;high_stim&quot;] &lt;- &quot;high&quot; ## Warning: Unknown or uninitialised column: `stim_name`. Code data_p2_long$stim_name[data_p2_long$param_stimulus_type == &quot;med_stim&quot;] &lt;- &quot;med&quot; data_p2_long$stim_name[data_p2_long$param_stimulus_type == &quot;low_stim&quot;] &lt;- &quot;low&quot; data_p2_long$stim_ordered &lt;- factor( data_p2_long$stim_name, levels = c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;) ) data_p2_long$cue_ordered &lt;- factor( data_p2_long$cue_name, levels = c(&quot;low cue&quot;, &quot;high cue&quot;) ) model_iv1 &lt;- &quot;stim_ordered&quot; model_iv2 &lt;- &quot;cue_ordered&quot; rating &lt;- &quot;rating_type&quot; dv &lt;- &quot;rating_value&quot; trialorder_subjectwise_p2 &lt;- meanSummary( data_p2_long, c( model_iv2, rating, &quot;trial_index&quot;), dv ) subjectwise_naomit_p2 &lt;- na.omit(trialorder_subjectwise_p2) trialorder_groupwise_p2 &lt;- summarySEwithin( data = subjectwise_naomit_p2, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;cue_ordered&quot;, &quot;rating_type&quot;, &quot;trial_index&quot;), idvar = subject ) ## Automatically converting the following non-factors to factors: rating_type, trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced Code trialorder_groupwise_p2$rating_type_key &lt;- mapvalues(trialorder_groupwise_p2$rating_type, from = c(&quot;event02_expect_angle&quot;, &quot;event04_actual_angle&quot;), to = c(&quot;expect&quot;, &quot;actual&quot;)) ggplot ## plot data version 2 ## Warning: Removed 1 rows containing missing values (`geom_point()`). ## subset of participants ## # A tibble: 5,851 × 60 ## # Groups: src_subject_id [110] ## src_subject_id session_id param_task_name param_run_num ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2 1 pain 1 ## 2 2 1 pain 1 ## 3 2 1 pain 1 ## 4 2 1 pain 1 ## 5 2 1 pain 1 ## 6 2 1 pain 1 ## 7 2 1 pain 1 ## 8 2 1 pain 1 ## 9 2 1 pain 1 ## 10 2 1 pain 1 ## # ℹ 5,841 more rows ## # ℹ 56 more variables: param_counterbalance_ver &lt;int&gt;, ## # param_counterbalance_block_num &lt;int&gt;, param_cue_type &lt;chr&gt;, ## # param_stimulus_type &lt;chr&gt;, param_cond_type &lt;int&gt;, ## # param_trigger_onset &lt;dbl&gt;, param_start_biopac &lt;dbl&gt;, ITI_onset &lt;dbl&gt;, ## # ITI_biopac &lt;dbl&gt;, ITI_duration &lt;dbl&gt;, event01_cue_onset &lt;dbl&gt;, ## # event01_cue_biopac &lt;dbl&gt;, event01_cue_type &lt;chr&gt;, … ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: rating_type, trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning: Removed 1 rows containing missing values (`geom_point()`). 7.4 vicarious ## # A tibble: 5,851 × 60 ## # Groups: src_subject_id [110] ## src_subject_id session_id param_task_name param_run_num ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2 1 pain 1 ## 2 2 1 pain 1 ## 3 2 1 pain 1 ## 4 2 1 pain 1 ## 5 2 1 pain 1 ## 6 2 1 pain 1 ## 7 2 1 pain 1 ## 8 2 1 pain 1 ## 9 2 1 pain 1 ## 10 2 1 pain 1 ## # ℹ 5,841 more rows ## # ℹ 56 more variables: param_counterbalance_ver &lt;int&gt;, ## # param_counterbalance_block_num &lt;int&gt;, param_cue_type &lt;chr&gt;, ## # param_stimulus_type &lt;chr&gt;, param_cond_type &lt;int&gt;, ## # param_trigger_onset &lt;dbl&gt;, param_start_biopac &lt;dbl&gt;, ITI_onset &lt;dbl&gt;, ## # ITI_biopac &lt;dbl&gt;, ITI_duration &lt;dbl&gt;, event01_cue_onset &lt;dbl&gt;, ## # event01_cue_biopac &lt;dbl&gt;, event01_cue_type &lt;chr&gt;, … ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: rating_type, trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning: Removed 1 rows containing missing values (`geom_point()`). 7.5 cognitive ## # A tibble: 5,851 × 60 ## # Groups: src_subject_id [110] ## src_subject_id session_id param_task_name param_run_num ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2 1 pain 1 ## 2 2 1 pain 1 ## 3 2 1 pain 1 ## 4 2 1 pain 1 ## 5 2 1 pain 1 ## 6 2 1 pain 1 ## 7 2 1 pain 1 ## 8 2 1 pain 1 ## 9 2 1 pain 1 ## 10 2 1 pain 1 ## # ℹ 5,841 more rows ## # ℹ 56 more variables: param_counterbalance_ver &lt;int&gt;, ## # param_counterbalance_block_num &lt;int&gt;, param_cue_type &lt;chr&gt;, ## # param_stimulus_type &lt;chr&gt;, param_cond_type &lt;int&gt;, ## # param_trigger_onset &lt;dbl&gt;, param_start_biopac &lt;dbl&gt;, ITI_onset &lt;dbl&gt;, ## # ITI_biopac &lt;dbl&gt;, ITI_duration &lt;dbl&gt;, event01_cue_onset &lt;dbl&gt;, ## # event01_cue_biopac &lt;dbl&gt;, event01_cue_type &lt;chr&gt;, … ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: rating_type, trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning: Removed 1 rows containing missing values (`geom_point()`). 7.6 within subject vicarious ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: trial_index, rating_type ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in geom_point(data = trialorder_groupwise_p2, aes(shape = ## as.character(rating_type), : Ignoring unknown aesthetics: linetype ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.7 Tor request – only outcome ratings. 3 tasks 7.7.1 pain ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.7.2 vicarious ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.7.3 cognitive ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.8 Tor request – only expect ratings. 3 tasks 7.8.1 pain ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## event04_actual_angle ~ trial_index * param_cue_type + (param_cue_type | ## src_subject_id) ## Data: df_clean ## ## REML criterion at convergence: 51815.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.1391 -0.6157 0.0088 0.6196 4.0913 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 860.95 29.342 ## param_cue_typelow_cue 41.77 6.463 -0.11 ## Residual 582.58 24.137 ## Number of obs: 5571, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 74.58379 2.94621 125.10816 25.315 ## trial_index -0.14818 0.02397 5358.25599 -6.183 ## param_cue_typelow_cue -10.07630 1.42124 568.67767 -7.090 ## trial_index:param_cue_typelow_cue 0.05386 0.03307 4679.39277 1.629 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## trial_index 6.76e-10 *** ## param_cue_typelow_cue 4.00e-12 *** ## trial_index:param_cue_typelow_cue 0.103 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) trl_nd prm___ ## trial_index -0.258 ## prm_c_typl_ -0.241 0.524 ## trl_ndx:___ 0.184 -0.688 -0.765 ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## trial_index 28321.1 28321.1 1 5407.0 48.6134 3.490e-12 *** ## param_cue_type 29283.6 29283.6 1 568.7 50.2655 3.999e-12 *** ## trial_index:param_cue_type 1545.4 1545.4 1 4679.4 2.6527 0.1034 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.8.2 vicarious ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## event04_actual_angle ~ trial_index * param_cue_type + (param_cue_type | ## src_subject_id) ## Data: df_clean ## ## REML criterion at convergence: 54872.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.8647 -0.6683 -0.2164 0.4658 5.5008 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 132.485 11.510 ## param_cue_typelow_cue 4.966 2.229 -0.89 ## Residual 578.849 24.059 ## Number of obs: 5936, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 26.04180 1.42259 222.10012 18.306 ## trial_index 0.14033 0.02205 5791.42518 6.363 ## param_cue_typelow_cue -10.93655 1.27427 1283.22709 -8.583 ## trial_index:param_cue_typelow_cue 0.08509 0.03037 5513.06146 2.802 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## trial_index 2.13e-10 *** ## param_cue_typelow_cue &lt; 2e-16 *** ## trial_index:param_cue_typelow_cue 0.00509 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) trl_nd prm___ ## trial_index -0.543 ## prm_c_typl_ -0.556 0.594 ## trl_ndx:___ 0.387 -0.697 -0.854 ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## trial_index 77430 77430 1 5869.4 133.7658 &lt; 2.2e-16 *** ## param_cue_type 42638 42638 1 1283.2 73.6606 &lt; 2.2e-16 *** ## trial_index:param_cue_type 4546 4546 1 5513.1 7.8532 0.005091 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.8.3 cognitive ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## event04_actual_angle ~ trial_index * param_cue_type + (param_cue_type | ## src_subject_id) ## Data: df_clean ## ## REML criterion at convergence: 52049.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.1640 -0.6201 -0.1508 0.4678 6.3870 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 197.53 14.054 ## param_cue_typelow_cue 26.61 5.158 -0.54 ## Residual 369.48 19.222 ## Number of obs: 5901, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 34.74794 1.52822 156.81597 22.737 ## trial_index -0.06251 0.01795 5781.38971 -3.482 ## param_cue_typelow_cue -10.41305 1.13179 632.34807 -9.201 ## trial_index:param_cue_typelow_cue 0.06544 0.02468 5538.06618 2.651 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## trial_index 0.000502 *** ## param_cue_typelow_cue &lt; 2e-16 *** ## trial_index:param_cue_typelow_cue 0.008037 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) trl_nd prm___ ## trial_index -0.411 ## prm_c_typl_ -0.514 0.547 ## trl_ndx:___ 0.296 -0.705 -0.776 ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## trial_index 2020.5 2020.5 1 5783.0 5.4686 0.019395 * ## param_cue_type 31276.3 31276.3 1 632.3 84.6500 &lt; 2.2e-16 *** ## trial_index:param_cue_type 2597.6 2597.6 1 5538.1 7.0304 0.008037 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Warning: Unknown or uninitialised column: `cue_name`. ## Warning: Unknown or uninitialised column: `stim_name`. ## Automatically converting the following non-factors to factors: trial_index ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## Warning in qt(conf.interval/2 + 0.5, datac$N - 1): NaNs produced ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.9 lmer ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## event04_actual_angle ~ trial_index * param_cue_type + (param_cue_type | ## src_subject_id) ## Data: df_clean ## ## REML criterion at convergence: 52049.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.1640 -0.6201 -0.1508 0.4678 6.3870 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 197.53 14.054 ## param_cue_typelow_cue 26.61 5.158 -0.54 ## Residual 369.48 19.222 ## Number of obs: 5901, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 34.74794 1.52822 156.81597 22.737 ## trial_index -0.06251 0.01795 5781.38971 -3.482 ## param_cue_typelow_cue -10.41305 1.13179 632.34807 -9.201 ## trial_index:param_cue_typelow_cue 0.06544 0.02468 5538.06618 2.651 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## trial_index 0.000502 *** ## param_cue_typelow_cue &lt; 2e-16 *** ## trial_index:param_cue_typelow_cue 0.008037 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) trl_nd prm___ ## trial_index -0.411 ## prm_c_typl_ -0.514 0.547 ## trl_ndx:___ 0.296 -0.705 -0.776 ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## trial_index 2020.5 2020.5 1 5783.0 5.4686 0.019395 * ## param_cue_type 31276.3 31276.3 1 632.3 84.6500 &lt; 2.2e-16 *** ## trial_index:param_cue_type 2597.6 2597.6 1 5538.1 7.0304 0.008037 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.9.1 lmer histogram Code hist(random_slopes) Code df &lt;- data.frame(sub = group_ids, cue_randomslope = random_slopes) write.csv(df, file.path(main_dir,&quot;data&quot;,&quot;RL&quot;, &quot;cue_trial_ranef_{taskname}.csv&quot;), row.names = FALSE) # TODO: create a json file that also keeps track of which participants are include hree, using what model # comment 7.10 Do current expectation ratings predict outcome ratings? Additional analyse 01/18/2023 see if current expectation ratings predict outcome ratings see if prior stimulus experience (N-1) predicts current expectation ratings see if current expectation ratings are explained as a function of prior outcome rating and current expectation rating when loading the dataset, I need to add in trial index per dataframe. Then, for the shift the rating? Code data_a3 &lt;- data_p2 %&gt;% group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate(trial_index = row_number(param_run_num)) data_a3lag &lt;- data_a3 %&gt;% group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate(lag.04outcomeangle = dplyr::lag(event04_actual_angle, n = 1, default = NA)) data_a3lag_omit &lt;- data_a3lag[complete.cases(data_a3lag$lag.04outcomeangle),] Code trialorder_subjectwise_lagoutcome &lt;- meanSummary( data_a3lag_omit, c(&quot;src_subject_id&quot;, &quot;session_id&quot;, &quot;param_run_num&quot;), &quot;lag.04outcomeangle&quot; ) trialorder_subjectwise_lagoutcome &lt;- meanSummary( data_a3lag_omit, c(&quot;src_subject_id&quot;, &quot;session_id&quot;, &quot;param_run_num&quot;), &quot;lag.04outcomeangle&quot; ) # subjectwise_naomit &lt;- na.omit(trialorder_subjectwise) # trialorder_groupwise &lt;- summarySEwithin( # data = subjectwise_naomit, # measurevar = &quot;mean_per_sub&quot;, # withinvars = c(&quot;cue_ordered&quot;, &quot;rating_type&quot;, &quot;trial_index&quot;), idvar = subject # ) Code model.lagoutcome = lmer(event02_expect_angle ~ lag.04outcomeangle + (1 | src_subject_id) + (1|session_id) , data = data_a3lag_omit) summary(model.lagoutcome) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: event02_expect_angle ~ lag.04outcomeangle + (1 | src_subject_id) + ## (1 | session_id) ## Data: data_a3lag_omit ## ## REML criterion at convergence: 49728.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.8551 -0.7402 -0.1322 0.6351 6.3909 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 121.4858 11.0221 ## session_id (Intercept) 0.2696 0.5192 ## Residual 531.1376 23.0464 ## Number of obs: 5427, groups: src_subject_id, 110; session_id, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.873e+01 1.230e+00 6.852e+01 23.36 &lt;2e-16 *** ## lag.04outcomeangle 1.780e-01 1.562e-02 5.374e+03 11.40 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## lg.04tcmngl -0.365 Code meanSummary_2dv &lt;- function(DATA, GROUP, DV1, DV2) { z &lt;- ddply(DATA, GROUP, .fun = function(xx) { c( DV1_mean_per_sub = mean(xx[, DV1], na.rm = TRUE), DV1_sd = sd(xx[, DV1], na.rm = TRUE), DV2_mean_per_sub = mean(xx[, DV2], na.rm = TRUE), DV2_sd = sd(xx[, DV1], na.rm = TRUE) ) }) return(z) } Code subjectwise_2dv = meanSummary_2dv(data_a3lag_omit, c(&quot;src_subject_id&quot;, &quot;trial_index&quot;), &quot;lag.04outcomeangle&quot;, &quot;event02_expect_angle&quot;) Code subjectwise_naomit_2dv &lt;- na.omit(subjectwise_2dv) Code sp &lt;- ggplot(data=subjectwise_naomit_2dv, aes(x=DV1_mean_per_sub, y=DV2_mean_per_sub)) + geom_point() + geom_abline(intercept = 0, slope = 1, color=&quot;green&quot;, linetype=&quot;dashed&quot;, size=0.5) + theme(aspect.ratio=1) + xlab(&quot;n-1 outcome rating&quot;) + ylab(&quot;n expectation rating&quot;) sp Code # plot(subjectwise_naomit_2dv$DV1_mean_per_sub, subjectwise_naomit_2dv$DV2_mean_per_sub) + lines(x = c(0,200), y = c(0,200)) Code trialorder_groupwise &lt;- summarySEwithin( data = subjectwise_naomit_2dv, measurevar = &quot;DV1_mean_per_sub&quot;, # betweenvars = &quot;src_subject_id&quot;, withinvars = factor( &quot;trial_index&quot;), idvar = &quot;src_subject_id&quot; ) ## Automatically converting the following non-factors to factors: src_subject_id Code trialorder_groupwise &lt;- summarySEwithin( data = subset(subjectwise_naomit_2dv, select = -c(src_subject_id)), measurevar = &quot;DV1_mean_per_sub&quot;, # betweenvars = &quot;src_subject_id&quot;, withinvars = as.factor( &quot;trial_index&quot;) #idvar = &quot;trial_index&quot; ) ## Automatically converting the following non-factors to factors: trial_index Code data_a3lag_omit$src_subject_id &lt;- as.factor(data_a3lag_omit$src_subject_id) lag.raw &lt;- ggplot(aes(x=lag.04outcomeangle, y=event02_expect_angle), data=data_a3lag_omit) + geom_smooth(method=&#39;lm&#39;, se=F, size=0.75) + geom_point(size=0.1) + geom_abline(intercept = 0, slope = 1, color=&quot;green&quot;, linetype=&quot;dashed&quot;, size=0.5) + facet_wrap(~src_subject_id) + theme(legend.position=&#39;none&#39;) + xlim(0,180) + ylim(0,180) + xlab(&quot;raw data from each participant: n-1 lagged outcome angle&quot;) + ylab(&quot;n current expectation rating&quot;) lag.raw + labs(title = paste(taskname, &quot;- Is there a linear relationship between current expectation ratings and the previous outcome ratings?&quot;), subtitle = &quot;Plotting the raw data - with all of the datapoints ignoring run differences&quot;, caption = &quot;Blue = fitted linear slope per participant; Green: 1:1 slope&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 276 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 276 rows containing missing values (`geom_point()`). Code subjectwise_naomit_2dv$src_subject_id &lt;- as.factor(subjectwise_naomit_2dv$src_subject_id) lag.avg &lt;- ggplot(aes(x=DV1_mean_per_sub, y=DV2_mean_per_sub), data=subjectwise_naomit_2dv) + geom_smooth(method=&#39;lm&#39;, se=F, size=0.75) + geom_point(size=0.1) + geom_abline(intercept = 0, slope = 1, color=&quot;green&quot;, linetype=&quot;dashed&quot;, size=0.5) + facet_wrap(~src_subject_id) + theme(legend.position=&#39;none&#39;) + xlim(0,180) + ylim(0,180) + xlab(&quot;raw data from each participant: n-1 lagged outcome angle&quot;) + ylab(&quot;n current expectation rating&quot;) lag.avg + labs(title = paste(taskname, &quot;- Is there a linear relationship between current expectation ratings and the previous outcome ratings?&quot;), subtitle = &quot;Observation notes: 1) The relationship is more of an attenuated one, where the higher outcome ratings lead to a slightly lower expectation rating, and a low outcome leads to a higher expectation rating, when considering a 1:1 relationship. This pattern could be explained by regression to the mean type mechanism, where participants are accounting for the fact that their previous experience was extreme on either ends and that this current trial will be under/over estimated. It probably will make sense to also see the relationship between current expectation ratings influencing current outcome ratings. &quot;, caption = &quot;Blue = fitted linear slope per participant; Green: 1:1 slope&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Code # https://gist.github.com/even4void/5074855 Code ggplot(data_a3lag_omit, aes(y = event02_expect_angle, x = lag.04outcomeangle, colour = subject), size = .3, color = &#39;gray&#39;) + geom_point(size = .1) + geom_smooth(method = &#39;lm&#39;, formula= y ~ x, se = FALSE, size = .3) + theme_bw() ## Warning: Removed 276 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 276 rows containing missing values (`geom_point()`). 7.11 Additional analysis 01/23/2023 Code model.lag_cue = lmer(event02_expect_angle ~ lag.04outcomeangle*param_cue_type + (1 | src_subject_id) + (1|session_id) , data = data_a3lag_omit) summary(model.lag_cue) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: event02_expect_angle ~ lag.04outcomeangle * param_cue_type + ## (1 | src_subject_id) + (1 | session_id) ## Data: data_a3lag_omit ## ## REML criterion at convergence: 46175.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3458 -0.6383 -0.0833 0.5246 9.8300 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 128.73691 11.3462 ## session_id (Intercept) 0.03408 0.1846 ## Residual 272.07427 16.4947 ## Number of obs: 5427, groups: src_subject_id, 110; session_id, 3 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) 42.76857 1.21398 129.42606 ## lag.04outcomeangle 0.23730 0.01465 5394.92176 ## param_cue_typelow_cue -27.33964 0.71898 5316.67781 ## lag.04outcomeangle:param_cue_typelow_cue -0.15072 0.01922 5309.65154 ## t value Pr(&gt;|t|) ## (Intercept) 35.230 &lt; 2e-16 *** ## lag.04outcomeangle 16.197 &lt; 2e-16 *** ## param_cue_typelow_cue -38.026 &lt; 2e-16 *** ## lag.04outcomeangle:param_cue_typelow_cue -7.842 5.3e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) lg.04t prm___ ## lg.04tcmngl -0.350 ## prm_c_typl_ -0.293 0.500 ## lg.04tc:___ 0.227 -0.639 -0.781 Code meanSummary_2dv &lt;- function(DATA, GROUP, DV1, DV2) { z &lt;- ddply(DATA, GROUP, .fun = function(xx) { c( DV1_mean_per_sub = mean(xx[, DV1], na.rm = TRUE), DV1_sd = sd(xx[, DV1], na.rm = TRUE), DV2_mean_per_sub = mean(xx[, DV2], na.rm = TRUE), DV2_sd = sd(xx[, DV1], na.rm = TRUE) ) }) return(z) } Code subjectwise_cuetype = meanSummary_2dv(data_a3lag_omit, c(&quot;src_subject_id&quot;, &quot;trial_index&quot;, &quot;param_cue_type&quot;), &quot;lag.04outcomeangle&quot;, &quot;event02_expect_angle&quot;) Code # subjectwise_cuetype_2dv &lt;- na.omit(subjectwise_cuetype) Code subjectwise_cuetype$param_cue_type &lt;- as.factor(subjectwise_cuetype$param_cue_type) sp &lt;- ggplot(data=subjectwise_cuetype, aes(x=DV1_mean_per_sub, y=DV2_mean_per_sub, color = param_cue_type)) + geom_point() + geom_abline(intercept = 0, slope = 1, color=&quot;green&quot;, linetype=&quot;dashed&quot;, size=0.5) + geom_smooth(method = &#39;lm&#39;) + theme(aspect.ratio=1) + xlab(&quot;n-1 outcome rating&quot;) + ylab(&quot;n expectation rating&quot;) sp + labs(title = paste(taskname, &quot;- Does the linear relationship between current expectation ratings and the previous outcome ratings differ as a function of cue?&quot;), subtitle = &quot;Plotting the raw data - with all of the datapoints averaged across runs per 12 trials&quot;, caption = &quot;high cue vs low cue. The slope is significant, theree is not interaction; Green: 1:1 slope&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 49 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 49 rows containing missing values (`geom_point()`). Code # plot(subjectwise_naomit_2dv$DV1_mean_per_sub, subjectwise_naomit_2dv$DV2_mean_per_sub) + lines(x = c(0,200), y = c(0,200)) "],["ch15_demean_per_sub.html", "Chapter 8 [ beh ] outcome_demean ~ cue * stim * expectrating * n-1outcomerating What is the purpose of this notebook? 8.1 linear model 0508 8.2 linear model 8.3 Q. Are those overestimating for high cues also underestimators for low cues? 8.4 TODO: Can you test if the slopes are the same? That might tell us something about whether, expectancies translate into outcomes with the same efficacy across all three tasks. 8.5 pain run, collapsed across stimulus intensity 8.6 vicarious 8.7 cognitive 8.8 linear model 0508 8.9 across tasks (PVC), is the slope for (highvslow cue) the same?Tor question 8.10 Alireza congruent incongruent (PE) 8.11 Alireza congruent incongruent (incorrect)", " Chapter 8 [ beh ] outcome_demean ~ cue * stim * expectrating * n-1outcomerating What is the purpose of this notebook? Here, I model the outcome ratings as a function of cue, stimulus intensity, expectation ratings, N-1 outcome rating. * As opposed to notebook 14, I demean the ratings within participants * In other words, calculate the average within subjects and subtract ratings * Main model: lmer(outcome_rating ~ cue * stim * expectation rating + N-1 outcomerating) * Main question: What constitutes a reported outcome rating? * Sub questions: - If there is a linear relationship between expectation rating and outcome rating, does this differ as a function of cue? - How does a N-1 outcome rating affect current expectation ratings? - Later, is this effect different across tasks or are they similar? IV: stim (high / med / low) cue (high / low) expectation rating (continuous) N-1 outcome rating (continuous) DV: outcome rating Some thoughts, TODOs Standardized coefficients Slope difference? Intercept difference? ( cue and expectation rating) Correct for the range (within participant) hypothesis: Larger expectation leads to prediction error Individual differences in ratings Outcome experience, based on behavioral experience What are the brain maps associated with each component. load data and combine participant data ## event02_expect_RT event04_actual_RT event02_expect_angle event04_actual_angle ## Min. :0.6504 Min. :0.0168 Min. : 0.00 Min. : 0.00 ## 1st Qu.:1.6341 1st Qu.:1.9197 1st Qu.: 30.18 1st Qu.: 38.80 ## Median :2.0517 Median :2.3510 Median : 58.56 Median : 60.77 ## Mean :2.1397 Mean :2.4005 Mean : 62.94 Mean : 66.33 ## 3rd Qu.:2.5678 3rd Qu.:2.8512 3rd Qu.: 90.00 3rd Qu.: 88.38 ## Max. :3.9912 Max. :3.9930 Max. :180.00 Max. :180.00 ## NA&#39;s :661 NA&#39;s :638 NA&#39;s :661 NA&#39;s :641 8.0.1 groupby subject and average 8.1 linear model 0508 Code # model.factorize_demean = lmer(demean_outcome~ CUE_high_gt_low*stim_factor*demean_expect +EXPECT_cmc+ lag.demean_outcome+(1|src_subject_id), data = pvc) # summary(model.factorize_demean) model.factorize_demean = lmer(demean_outcome~ CUE_high_gt_low*stim_con_linear*demean_expect + CUE_high_gt_low*stim_con_quad*demean_expect + EXPECT_cmc + (1|src_subject_id), data = pvc) ## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient ## boundary (singular) fit: see help(&#39;isSingular&#39;) Code summary(model.factorize_demean) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: demean_outcome ~ CUE_high_gt_low * stim_con_linear * demean_expect + ## CUE_high_gt_low * stim_con_quad * demean_expect + EXPECT_cmc + ## (1 | src_subject_id) ## Data: pvc ## ## REML criterion at convergence: 43246.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.3122 -0.5823 -0.0037 0.5936 5.1434 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 0.0 0.00 ## Residual 370.8 19.26 ## Number of obs: 4939, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) -7.897e-01 3.461e-01 4.927e+03 ## CUE_high_gt_low -3.686e+00 6.922e-01 4.927e+03 ## stim_con_linear 3.010e+01 8.554e-01 4.927e+03 ## demean_expect 3.306e-01 1.219e-02 4.927e+03 ## stim_con_quad 2.020e+00 7.350e-01 4.927e+03 ## CUE_high_gt_low:stim_con_linear 1.066e+00 1.711e+00 4.927e+03 ## CUE_high_gt_low:demean_expect -4.084e-02 2.438e-02 4.927e+03 ## stim_con_linear:demean_expect 1.195e-02 2.993e-02 4.927e+03 ## CUE_high_gt_low:stim_con_quad -3.891e+00 1.470e+00 4.927e+03 ## demean_expect:stim_con_quad 3.232e-03 2.606e-02 4.927e+03 ## CUE_high_gt_low:stim_con_linear:demean_expect 2.105e-01 5.986e-02 4.927e+03 ## CUE_high_gt_low:demean_expect:stim_con_quad -3.814e-02 5.212e-02 4.927e+03 ## t value Pr(&gt;|t|) ## (Intercept) -2.281 0.022563 * ## CUE_high_gt_low -5.324 1.06e-07 *** ## stim_con_linear 35.190 &lt; 2e-16 *** ## demean_expect 27.118 &lt; 2e-16 *** ## stim_con_quad 2.748 0.006013 ** ## CUE_high_gt_low:stim_con_linear 0.623 0.533311 ## CUE_high_gt_low:demean_expect -1.675 0.094003 . ## stim_con_linear:demean_expect 0.399 0.689707 ## CUE_high_gt_low:stim_con_quad -2.647 0.008141 ** ## demean_expect:stim_con_quad 0.124 0.901304 ## CUE_high_gt_low:stim_con_linear:demean_expect 3.516 0.000442 *** ## CUE_high_gt_low:demean_expect:stim_con_quad -0.732 0.464328 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) CUE_h__ stm_cn_l dmn_xp stm_cn_q ## CUE_hgh_gt_ 0.092 ## stim_cn_lnr -0.030 -0.028 ## demean_xpct -0.100 -0.609 0.016 ## stim_con_qd -0.026 -0.028 0.022 0.015 ## CUE_hgh_gt_lw:stm_cn_l -0.028 -0.030 0.110 0.023 0.020 ## CUE_hgh__:_ -0.609 -0.100 0.023 0.102 0.019 ## stm_cn_ln:_ 0.016 0.024 -0.109 -0.040 -0.012 ## CUE_hgh_gt_lw:stm_cn_q -0.028 -0.026 0.020 0.019 0.074 ## dmn_xpct:__ 0.014 0.018 -0.011 -0.007 -0.091 ## CUE___:__:_ 0.024 0.016 -0.615 -0.001 -0.017 ## CUE___:_:__ 0.018 0.014 -0.017 0.001 -0.603 ## CUE_hgh_gt_lw:stm_cn_l CUE_h__:_ st__:_ ## CUE_hgh_gt_ ## stim_cn_lnr ## demean_xpct ## stim_con_qd ## CUE_hgh_gt_lw:stm_cn_l ## CUE_hgh__:_ 0.016 ## stm_cn_ln:_ -0.615 -0.001 ## CUE_hgh_gt_lw:stm_cn_q 0.022 0.015 -0.017 ## dmn_xpct:__ -0.017 0.001 0.028 ## CUE___:__:_ -0.109 -0.040 0.101 ## CUE___:_:__ -0.011 -0.007 0.001 ## CUE_hgh_gt_lw:stm_cn_q dm_:__ CUE___:__: ## CUE_hgh_gt_ ## stim_cn_lnr ## demean_xpct ## stim_con_qd ## CUE_hgh_gt_lw:stm_cn_l ## CUE_hgh__:_ ## stm_cn_ln:_ ## CUE_hgh_gt_lw:stm_cn_q ## dmn_xpct:__ -0.603 ## CUE___:__:_ -0.012 0.001 ## CUE___:_:__ -0.091 0.104 0.028 ## fit warnings: ## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 8.2 linear model ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: demean_outcome ~ CUE_high_gt_low * stim_factor * demean_expect + ## lag.demean_outcome + (1 | src_subject_id) ## Data: pvc ## ## REML criterion at convergence: 42884.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.6746 -0.5825 0.0064 0.5976 5.5568 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 0.0 0.00 ## Residual 344.1 18.55 ## Number of obs: 4939, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error ## (Intercept) 1.359e+01 5.718e-01 ## CUE_high_gt_low 2.734e-01 1.149e+00 ## stim_factorlow_stim -3.049e+01 8.242e-01 ## stim_factormed_stim -1.279e+01 8.054e-01 ## demean_expect 2.713e-01 2.015e-02 ## lag.demean_outcome 2.177e-01 1.111e-02 ## CUE_high_gt_low:stim_factorlow_stim -4.737e-01 1.648e+00 ## CUE_high_gt_low:stim_factormed_stim -5.026e+00 1.611e+00 ## CUE_high_gt_low:demean_expect 1.048e-01 3.980e-02 ## stim_factorlow_stim:demean_expect -1.928e-02 2.883e-02 ## stim_factormed_stim:demean_expect -1.078e-03 2.838e-02 ## CUE_high_gt_low:stim_factorlow_stim:demean_expect -2.491e-01 5.770e-02 ## CUE_high_gt_low:stim_factormed_stim:demean_expect -1.639e-01 5.677e-02 ## df t value Pr(&gt;|t|) ## (Intercept) 4.926e+03 23.761 &lt; 2e-16 ## CUE_high_gt_low 4.926e+03 0.238 0.81188 ## stim_factorlow_stim 4.926e+03 -36.993 &lt; 2e-16 ## stim_factormed_stim 4.926e+03 -15.879 &lt; 2e-16 ## demean_expect 4.926e+03 13.461 &lt; 2e-16 ## lag.demean_outcome 4.926e+03 19.585 &lt; 2e-16 ## CUE_high_gt_low:stim_factorlow_stim 4.926e+03 -0.287 0.77384 ## CUE_high_gt_low:stim_factormed_stim 4.926e+03 -3.120 0.00182 ## CUE_high_gt_low:demean_expect 4.926e+03 2.633 0.00849 ## stim_factorlow_stim:demean_expect 4.926e+03 -0.669 0.50373 ## stim_factormed_stim:demean_expect 4.926e+03 -0.038 0.96969 ## CUE_high_gt_low:stim_factorlow_stim:demean_expect 4.926e+03 -4.317 1.61e-05 ## CUE_high_gt_low:stim_factormed_stim:demean_expect 4.926e+03 -2.888 0.00390 ## ## (Intercept) *** ## CUE_high_gt_low ## stim_factorlow_stim *** ## stim_factormed_stim *** ## demean_expect *** ## lag.demean_outcome *** ## CUE_high_gt_low:stim_factorlow_stim ## CUE_high_gt_low:stim_factormed_stim ** ## CUE_high_gt_low:demean_expect ** ## stim_factorlow_stim:demean_expect ## stim_factormed_stim:demean_expect ## CUE_high_gt_low:stim_factorlow_stim:demean_expect *** ## CUE_high_gt_low:stim_factormed_stim:demean_expect ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 8.3 Q. Are those overestimating for high cues also underestimators for low cues? y axis: outcome rating x axis: high cue distance from 1:1 line Using ODR, we can test whether different cues lead to different distances from the identity line ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## as.formula(reformulate(c(iv, sprintf(&quot;(%s|%s)&quot;, iv, subject_keyword)), ## response = dv)) ## Data: df ## ## REML criterion at convergence: 42138.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0513 -0.6728 -0.1352 0.5460 6.0466 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 66.398 8.148 ## cue_namelow 2.915 1.707 1.00 ## Residual 280.906 16.760 ## Number of obs: 4939, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 23.1557 0.8577 108.8675 26.998 &lt;2e-16 *** ## cue_namelow 0.7289 0.5052 478.1364 1.443 0.15 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## cue_namelow 0.032 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 8.4 TODO: Can you test if the slopes are the same? That might tell us something about whether, expectancies translate into outcomes with the same efficacy across all three tasks. ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## as.formula(reformulate(c(iv, sprintf(&quot;(%s|%s)&quot;, iv, subject_keyword)), ## response = dv)) ## Data: df ## ## REML criterion at convergence: 42138.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0513 -0.6728 -0.1352 0.5460 6.0466 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 66.398 8.148 ## cue_namelow 2.915 1.707 1.00 ## Residual 280.906 16.760 ## Number of obs: 4939, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 23.1557 0.8577 108.8675 26.998 &lt;2e-16 *** ## cue_namelow 0.7289 0.5052 478.1364 1.443 0.15 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## cue_namelow 0.032 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill ## Warning: Removed 1 rows containing non-finite values (`stat_half_ydensity()`). ## Warning: Removed 1 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 1 row containing missing values (`geom_line()`). ## Warning: Removed 1 rows containing missing values (`geom_point()`). ## Warning: Removed 1 rows containing non-finite values (`stat_half_ydensity()`). ## Warning: Removed 1 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 1 row containing missing values (`geom_line()`). ## Warning: Removed 1 rows containing missing values (`geom_point()`). 8.5 pain run, collapsed across stimulus intensity 8.6 vicarious Code model.factorize_demean = lmer(demean_outcome~ CUE_high_gt_low*stim_con_linear*demean_expect + CUE_high_gt_low*stim_con_quad*demean_expect + EXPECT_cmc + (1|src_subject_id), data = pvc) ## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient ## boundary (singular) fit: see help(&#39;isSingular&#39;) Code summary(model.factorize_demean) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: demean_outcome ~ CUE_high_gt_low * stim_con_linear * demean_expect + ## CUE_high_gt_low * stim_con_quad * demean_expect + EXPECT_cmc + ## (1 | src_subject_id) ## Data: pvc ## ## REML criterion at convergence: 45954.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.475 -0.602 -0.138 0.446 5.630 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 0.0 0.00 ## Residual 469.7 21.67 ## Number of obs: 5111, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) 0.21603 0.44918 5099.00000 ## CUE_high_gt_low 1.24687 0.89837 5099.00000 ## stim_con_linear 23.90084 1.10386 5099.00000 ## demean_expect 0.19711 0.01918 5099.00000 ## stim_con_quad -4.72799 0.95934 5099.00000 ## CUE_high_gt_low:stim_con_linear 6.38339 2.20771 5099.00000 ## CUE_high_gt_low:demean_expect -0.02451 0.03835 5099.00000 ## stim_con_linear:demean_expect 0.03147 0.04753 5099.00000 ## CUE_high_gt_low:stim_con_quad -3.13553 1.91869 5099.00000 ## demean_expect:stim_con_quad 0.01270 0.04060 5099.00000 ## CUE_high_gt_low:stim_con_linear:demean_expect 0.37387 0.09506 5099.00000 ## CUE_high_gt_low:demean_expect:stim_con_quad -0.07222 0.08120 5099.00000 ## t value Pr(&gt;|t|) ## (Intercept) 0.481 0.63058 ## CUE_high_gt_low 1.388 0.16522 ## stim_con_linear 21.652 &lt; 2e-16 *** ## demean_expect 10.278 &lt; 2e-16 *** ## stim_con_quad -4.928 8.55e-07 *** ## CUE_high_gt_low:stim_con_linear 2.891 0.00385 ** ## CUE_high_gt_low:demean_expect -0.639 0.52280 ## stim_con_linear:demean_expect 0.662 0.50793 ## CUE_high_gt_low:stim_con_quad -1.634 0.10228 ## demean_expect:stim_con_quad 0.313 0.75449 ## CUE_high_gt_low:stim_con_linear:demean_expect 3.933 8.50e-05 *** ## CUE_high_gt_low:demean_expect:stim_con_quad -0.889 0.37386 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) CUE_h__ stm_cn_l dmn_xp stm_cn_q ## CUE_hgh_gt_ -0.193 ## stim_cn_lnr 0.007 -0.063 ## demean_xpct 0.258 -0.736 0.052 ## stim_con_qd -0.009 0.026 -0.005 -0.009 ## CUE_hgh_gt_lw:stm_cn_l -0.063 0.007 -0.210 -0.015 0.045 ## CUE_hgh__:_ -0.736 0.258 -0.015 -0.336 0.019 ## stm_cn_ln:_ 0.052 -0.015 0.261 0.019 -0.037 ## CUE_hgh_gt_lw:stm_cn_q 0.026 -0.009 0.045 0.019 -0.176 ## dmn_xpct:__ -0.010 0.019 -0.037 -0.034 0.255 ## CUE___:__:_ -0.015 0.052 -0.738 -0.023 0.011 ## CUE___:_:__ 0.019 -0.010 0.011 -0.034 -0.734 ## CUE_hgh_gt_lw:stm_cn_l CUE_h__:_ st__:_ ## CUE_hgh_gt_ ## stim_cn_lnr ## demean_xpct ## stim_con_qd ## CUE_hgh_gt_lw:stm_cn_l ## CUE_hgh__:_ 0.052 ## stm_cn_ln:_ -0.738 -0.023 ## CUE_hgh_gt_lw:stm_cn_q -0.005 -0.009 0.011 ## dmn_xpct:__ 0.011 -0.034 -0.014 ## CUE___:__:_ 0.261 0.019 -0.305 ## CUE___:_:__ -0.037 -0.034 0.017 ## CUE_hgh_gt_lw:stm_cn_q dm_:__ CUE___:__: ## CUE_hgh_gt_ ## stim_cn_lnr ## demean_xpct ## stim_con_qd ## CUE_hgh_gt_lw:stm_cn_l ## CUE_hgh__:_ ## stm_cn_ln:_ ## CUE_hgh_gt_lw:stm_cn_q ## dmn_xpct:__ -0.734 ## CUE___:__:_ -0.037 0.017 ## CUE___:_:__ 0.255 -0.369 -0.014 ## fit warnings: ## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## as.formula(reformulate(c(iv, sprintf(&quot;(%s|%s)&quot;, iv, subject_keyword)), ## response = dv)) ## Data: df ## ## REML criterion at convergence: 42592.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.8941 -0.6915 -0.1167 0.5386 5.2943 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 4.520e+01 6.72295 ## cue_namelow 9.234e-03 0.09609 -1.00 ## Residual 2.325e+02 15.24653 ## Number of obs: 5111, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 20.3562 0.7177 111.0539 28.364 &lt;2e-16 *** ## cue_namelow -0.2391 0.4276 4783.4156 -0.559 0.576 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## cue_namelow -0.319 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill ## Warning: Removed 1 rows containing non-finite values (`stat_half_ydensity()`). ## Warning: Removed 1 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 1 row containing missing values (`geom_line()`). ## Warning: Removed 1 rows containing missing values (`geom_point()`). ## Warning: Removed 1 rows containing non-finite values (`stat_half_ydensity()`). ## Warning: Removed 1 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 1 row containing missing values (`geom_line()`). ## Warning: Removed 1 rows containing missing values (`geom_point()`). 8.7 cognitive 8.8 linear model 0508 Code # model.factorize_demean = lmer(demean_outcome~ CUE_high_gt_low*stim_factor*demean_expect +EXPECT_cmc+ lag.demean_outcome+(1|src_subject_id), data = pvc) # summary(model.factorize_demean) model.factorize_demean = lmer(demean_outcome~ CUE_high_gt_low*stim_con_linear*demean_expect + CUE_high_gt_low*stim_con_quad*demean_expect + EXPECT_cmc + (1|src_subject_id), data = pvc) ## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient ## boundary (singular) fit: see help(&#39;isSingular&#39;) Code summary(model.factorize_demean) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: demean_outcome ~ CUE_high_gt_low * stim_con_linear * demean_expect + ## CUE_high_gt_low * stim_con_quad * demean_expect + EXPECT_cmc + ## (1 | src_subject_id) ## Data: pvc ## ## REML criterion at convergence: 43807.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.1272 -0.6113 -0.1388 0.4529 7.1545 ## ## Random effects: ## Groups Name Variance Std.Dev. ## src_subject_id (Intercept) 0.0 0.00 ## Residual 347.4 18.64 ## Number of obs: 5041, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) 6.017e-02 3.729e-01 5.029e+03 ## CUE_high_gt_low -1.515e-02 7.458e-01 5.029e+03 ## stim_con_linear 8.066e+00 8.920e-01 5.029e+03 ## demean_expect 2.374e-01 1.662e-02 5.029e+03 ## stim_con_quad 3.181e+00 8.174e-01 5.029e+03 ## CUE_high_gt_low:stim_con_linear 2.562e+00 1.784e+00 5.029e+03 ## CUE_high_gt_low:demean_expect -1.247e-02 3.324e-02 5.029e+03 ## stim_con_linear:demean_expect 1.995e-02 3.894e-02 5.029e+03 ## CUE_high_gt_low:stim_con_quad -2.304e+00 1.635e+00 5.029e+03 ## demean_expect:stim_con_quad 1.089e-03 3.710e-02 5.029e+03 ## CUE_high_gt_low:stim_con_linear:demean_expect 1.105e-01 7.788e-02 5.029e+03 ## CUE_high_gt_low:demean_expect:stim_con_quad -9.891e-03 7.419e-02 5.029e+03 ## t value Pr(&gt;|t|) ## (Intercept) 0.161 0.871825 ## CUE_high_gt_low -0.020 0.983797 ## stim_con_linear 9.044 &lt; 2e-16 *** ## demean_expect 14.286 &lt; 2e-16 *** ## stim_con_quad 3.892 0.000101 *** ## CUE_high_gt_low:stim_con_linear 1.436 0.151056 ## CUE_high_gt_low:demean_expect -0.375 0.707610 ## stim_con_linear:demean_expect 0.512 0.608536 ## CUE_high_gt_low:stim_con_quad -1.410 0.158728 ## demean_expect:stim_con_quad 0.029 0.976584 ## CUE_high_gt_low:stim_con_linear:demean_expect 1.419 0.156016 ## CUE_high_gt_low:demean_expect:stim_con_quad -0.133 0.893945 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) CUE_h__ stm_cn_l dmn_xp stm_cn_q ## CUE_hgh_gt_ -0.165 ## stim_cn_lnr -0.006 -0.029 ## demean_xpct 0.225 -0.709 0.015 ## stim_con_qd 0.064 -0.036 0.004 0.050 ## CUE_hgh_gt_lw:stm_cn_l -0.029 -0.006 -0.146 -0.005 0.020 ## CUE_hgh__:_ -0.709 0.225 -0.005 -0.307 -0.079 ## stm_cn_ln:_ 0.015 -0.005 0.202 0.006 -0.011 ## CUE_hgh_gt_lw:stm_cn_q -0.036 0.064 0.020 -0.079 -0.183 ## dmn_xpct:__ 0.049 -0.077 -0.010 0.115 0.245 ## CUE___:__:_ -0.005 0.015 -0.698 0.014 0.003 ## CUE___:_:__ -0.077 0.049 0.003 -0.078 -0.719 ## CUE_hgh_gt_lw:stm_cn_l CUE_h__:_ st__:_ ## CUE_hgh_gt_ ## stim_cn_lnr ## demean_xpct ## stim_con_qd ## CUE_hgh_gt_lw:stm_cn_l ## CUE_hgh__:_ 0.015 ## stm_cn_ln:_ -0.698 0.014 ## CUE_hgh_gt_lw:stm_cn_q 0.004 0.050 0.003 ## dmn_xpct:__ 0.003 -0.078 -0.004 ## CUE___:__:_ 0.202 0.006 -0.273 ## CUE___:_:__ -0.010 0.115 -0.009 ## CUE_hgh_gt_lw:stm_cn_q dm_:__ CUE___:__: ## CUE_hgh_gt_ ## stim_cn_lnr ## demean_xpct ## stim_con_qd ## CUE_hgh_gt_lw:stm_cn_l ## CUE_hgh__:_ ## stm_cn_ln:_ ## CUE_hgh_gt_lw:stm_cn_q ## dmn_xpct:__ -0.719 ## CUE___:__:_ -0.011 -0.009 ## CUE___:_:__ 0.245 -0.336 -0.004 ## fit warnings: ## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Code # model.factorize_C= lmer(demean_outcome~ CUE_high_gt_low*stim_factor*demean_expect +EXPECT_cmc+ (1|src_subject_id), data = pvc) # summary(model.factorize_demean) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## as.formula(reformulate(c(iv, sprintf(&quot;(%s|%s)&quot;, iv, subject_keyword)), ## response = dv)) ## Data: df ## ## REML criterion at convergence: 40996.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4634 -0.6567 -0.1325 0.5028 6.6541 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## src_subject_id (Intercept) 43.94315 6.6290 ## cue_namelow 0.03079 0.1755 -1.00 ## Residual 189.43985 13.7637 ## Number of obs: 5041, groups: src_subject_id, 110 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 18.8604 0.6959 107.7330 27.101 &lt;2e-16 *** ## cue_namelow -0.5008 0.3891 4141.2072 -1.287 0.198 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## cue_namelow -0.319 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning in geom_line(data = subjectwise, aes(group = .data[[subject]], x = ## as.numeric(as.factor(.data[[iv]])) - : Ignoring unknown aesthetics: fill Code # library(plotly) # plot_ly(x=subjectwise_naomit_2dv$param_cue_type, y=subjectwise_naomit_2dv$DV1_mean_per_sub, z=subjectwise_naomit_2dv$DV2_mean_per_sub, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, color=subjectwise_naomit_2dv$param_cue_type) 8.9 across tasks (PVC), is the slope for (highvslow cue) the same?Tor question Adding “participant” as random effects leads to a singular boundary issue. The reason is because there is no random effects variance across participants. If I add task as a random effect, in other words, allowing for differences across tasks, I get the following results: expectancy-outcome relationship differs across tasks, taskname_lin:demean_expect, t(14130) = 4.317, p &lt; .001 expectancy-outcome relationship differs across cue and tasks, “taskname_lin:CUE_high_gt_low:demean_expect”, t(14130) = 5.758, p &lt; .001 taskname_lin:CUE_high_gt_low -3.790e+00 1.448e+00 1.413e+04 -2.618 0.00886 ** ++ taskname_lin:demean_expect 9.854e-02 2.283e-02 1.413e+04 4.317 1.59e-05 CUE_high_gt_low:demean_expect -9.077e-02 1.987e-02 1.413e+04 -4.569 4.95e-06 CUE_high_gt_low:taskname_quad 5.352e+00 1.334e+00 1.413e+04 4.012 6.04e-05 demean_expect:taskname_quad -1.596e-01 2.253e-02 1.413e+04 -7.084 1.47e-12 taskname_lin:CUE_high_gt_low:demean_expect 2.629e-01 4.565e-02 1.413e+04 5.758 8.67e-09 ** CUE_high_gt_low:demean_expect:taskname_quad -1.021e-01 4.505e-02 1.413e+04 -2.266 0.02348 If I add sub as random effect and ignore singular. Plus, if I remove the cue contrast… expectancy-outcome relationship differs across tasks, factor(param_task_name):demean_expect, F(2, 14136) = 54.765, p &lt; .001 Code p &lt;- load_task_social_df(datadir, taskname = &#39;pain&#39;, subject_varkey = subject_varkey, iv = iv, exclude = exclude) v &lt;- load_task_social_df(datadir, taskname = &#39;vicarious&#39;, subject_varkey = subject_varkey, iv = iv, exclude = exclude) c &lt;- load_task_social_df(datadir, taskname = &#39;cognitive&#39;, subject_varkey = subject_varkey, iv = iv, exclude = exclude) p_sub &lt;- p[, c(&quot;param_task_name&quot;, &quot;param_cue_type&quot;, &quot;src_subject_id&quot;,&quot;session_id&quot;, &quot;param_run_num&quot;, &quot;param_stimulus_type&quot;, &quot;event04_actual_angle&quot;, &quot;event02_expect_angle&quot;)] v_sub &lt;- v[, c(&quot;param_task_name&quot;, &quot;param_cue_type&quot;, &quot;src_subject_id&quot;,&quot;session_id&quot;, &quot;param_run_num&quot;, &quot;param_stimulus_type&quot;, &quot;event04_actual_angle&quot;, &quot;event02_expect_angle&quot;)] c_sub &lt;- c[, c(&quot;param_task_name&quot;, &quot;param_cue_type&quot;, &quot;src_subject_id&quot;, &quot;session_id&quot;, &quot;param_run_num&quot;,&quot;param_stimulus_type&quot;, &quot;event04_actual_angle&quot;, &quot;event02_expect_angle&quot;)] pvc_sub &lt;- do.call(&quot;rbind&quot;, list(p_sub, v_sub, c_sub)) Code maindata &lt;- pvc_sub %&gt;% group_by(src_subject_id) %&gt;% mutate(event04_actual_angle = as.numeric(event04_actual_angle)) %&gt;% mutate(event02_expect_angle = as.numeric(event02_expect_angle)) %&gt;% mutate(avg_outcome = mean(event04_actual_angle, na.rm = TRUE)) %&gt;% mutate(demean_outcome = event04_actual_angle - avg_outcome) %&gt;% mutate(avg_expect = mean(event02_expect_angle, na.rm = TRUE)) %&gt;% mutate(demean_expect = event02_expect_angle - avg_expect) data_p2= maindata %&gt;% arrange(src_subject_id ) %&gt;% group_by(src_subject_id) %&gt;% mutate(trial_index = row_number()) data_a3 &lt;- data_p2 %&gt;% group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate(trial_index = row_number(param_run_num)) data_a3lag &lt;- data_a3 %&gt;% group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate(lag.demean_outcome = dplyr::lag(demean_outcome, n = 1, default = NA)) data_a3lag_omit &lt;- data_a3lag[complete.cases(data_a3lag$lag.demean_outcome),] df &lt;- data_a3lag_omit pvc_sub &lt;- simple_contrasts_beh(df) ## Warning: Unknown or uninitialised column: `stim_con_linear`. ## Warning: Unknown or uninitialised column: `stim_con_quad`. ## Warning: Unknown or uninitialised column: `CUE_high_gt_low`. ## Warning: Unknown or uninitialised column: `cue_name`. Code # contrast code 1 linear pvc_sub$taskname_lin[pvc_sub$param_task_name == &quot;pain&quot;] &lt;- 0.5 ## Warning: Unknown or uninitialised column: `taskname_lin`. Code pvc_sub$taskname_lin[pvc_sub$param_task_name == &quot;vicarious&quot;] &lt;- 0 pvc_sub$taskname_lin[pvc_sub$param_task_name == &quot;cognitive&quot;] &lt;- -0.5 # contrast code 2 quadratic pvc_sub$taskname_quad[pvc_sub$param_task_name == &quot;pain&quot;] &lt;- -0.33 ## Warning: Unknown or uninitialised column: `taskname_quad`. Code pvc_sub$taskname_quad[pvc_sub$param_task_name == &quot;vicarious&quot;] &lt;- 0.66 pvc_sub$taskname_quad[pvc_sub$param_task_name == &quot;cognitive&quot;] &lt;- -0.33 pvc_sub$sub = factor(pvc_sub$src_subject_id) # model_test = lm(pvc_sub$demean_outcome~ pvc_sub$demean_expect) model_task = lmer(demean_outcome~ taskname_lin*CUE_high_gt_low*demean_expect + taskname_quad*CUE_high_gt_low*demean_expect + (1 | sub), data = pvc_sub) model_wotask = lmer(demean_outcome~ CUE_high_gt_low*demean_expect +(1 | sub), data = pvc_sub) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Code summary(model_task) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: demean_outcome ~ taskname_lin * CUE_high_gt_low * demean_expect + ## taskname_quad * CUE_high_gt_low * demean_expect + (1 | sub) ## Data: pvc_sub ## ## REML criterion at convergence: 139399.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5727 -0.6342 -0.1226 0.5514 5.3674 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub (Intercept) 0.1232 0.351 ## Residual 600.5633 24.506 ## Number of obs: 15091, groups: sub, 111 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) -3.019e-01 2.951e-01 2.477e+02 ## taskname_lin 1.801e+01 6.935e-01 1.447e+04 ## CUE_high_gt_low -9.551e+00 5.853e-01 1.369e+04 ## demean_expect 4.696e-01 9.457e-03 1.156e+04 ## taskname_quad -1.082e+01 6.465e-01 1.482e+04 ## taskname_lin:CUE_high_gt_low -4.377e+00 1.388e+00 7.418e+03 ## taskname_lin:demean_expect 1.007e-01 2.163e-02 1.758e+03 ## CUE_high_gt_low:demean_expect -8.861e-02 1.894e-02 4.064e+03 ## CUE_high_gt_low:taskname_quad 5.276e+00 1.294e+00 1.247e+04 ## demean_expect:taskname_quad -1.730e-01 2.157e-02 8.290e+03 ## taskname_lin:CUE_high_gt_low:demean_expect 2.685e-01 4.314e-02 1.405e+04 ## CUE_high_gt_low:demean_expect:taskname_quad -1.063e-01 4.310e-02 1.484e+04 ## t value Pr(&gt;|t|) ## (Intercept) -1.023 0.30723 ## taskname_lin 25.973 &lt; 2e-16 *** ## CUE_high_gt_low -16.319 &lt; 2e-16 *** ## demean_expect 49.657 &lt; 2e-16 *** ## taskname_quad -16.741 &lt; 2e-16 *** ## taskname_lin:CUE_high_gt_low -3.153 0.00162 ** ## taskname_lin:demean_expect 4.655 3.48e-06 *** ## CUE_high_gt_low:demean_expect -4.679 2.98e-06 *** ## CUE_high_gt_low:taskname_quad 4.078 4.56e-05 *** ## demean_expect:taskname_quad -8.022 1.18e-15 *** ## taskname_lin:CUE_high_gt_low:demean_expect 6.224 4.98e-10 *** ## CUE_high_gt_low:demean_expect:taskname_quad -2.466 0.01368 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) tsknm_l CUE_h__ dmn_xp tsknm_q ts_:CUE___ tsk_:_ ## taskname_ln 0.005 ## CUE_hgh_gt_ -0.188 0.369 ## demean_xpct 0.224 -0.360 -0.620 ## taskname_qd 0.087 -0.004 -0.299 0.280 ## tsk_:CUE___ 0.368 0.032 0.004 0.141 -0.253 ## tsknm_ln:d_ -0.374 0.022 0.146 -0.281 0.256 -0.574 ## CUE_hgh_gt_lw:d_ -0.616 0.140 0.225 -0.197 -0.142 -0.362 0.151 ## CUE_hgh_gt_lw:t_ -0.298 -0.253 0.088 -0.143 -0.382 -0.005 -0.097 ## dmn_xpct:t_ 0.271 0.240 -0.139 0.177 0.391 -0.091 0.183 ## t_:CUE___:_ 0.145 -0.572 -0.375 0.150 -0.099 0.023 -0.120 ## CUE_h__:_:_ -0.137 -0.093 0.272 -0.123 -0.659 0.239 -0.099 ## CUE_hgh_gt_lw:d_ CUE_hgh_gt_lw:t_ dmn_:_ t_:CUE___: ## taskname_ln ## CUE_hgh_gt_ ## demean_xpct ## taskname_qd ## tsk_:CUE___ ## tsknm_ln:d_ ## CUE_hgh_gt_lw:d_ ## CUE_hgh_gt_lw:t_ 0.281 ## dmn_xpct:t_ -0.123 -0.660 ## t_:CUE___:_ -0.280 0.256 -0.099 ## CUE_h__:_:_ 0.176 0.391 -0.255 0.185 Code anova(model_task) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF ## taskname_lin 405142 405142 1 14470.1 ## CUE_high_gt_low 159941 159941 1 13687.7 ## demean_expect 1480894 1480894 1 11560.3 ## taskname_quad 168310 168310 1 14820.0 ## taskname_lin:CUE_high_gt_low 5970 5970 1 7418.2 ## taskname_lin:demean_expect 13016 13016 1 1757.8 ## CUE_high_gt_low:demean_expect 13146 13146 1 4063.7 ## CUE_high_gt_low:taskname_quad 9989 9989 1 12473.3 ## demean_expect:taskname_quad 38647 38647 1 8289.7 ## taskname_lin:CUE_high_gt_low:demean_expect 23266 23266 1 14053.9 ## CUE_high_gt_low:demean_expect:taskname_quad 3652 3652 1 14843.7 ## F value Pr(&gt;F) ## taskname_lin 674.6035 &lt; 2.2e-16 *** ## CUE_high_gt_low 266.3187 &lt; 2.2e-16 *** ## demean_expect 2465.8423 &lt; 2.2e-16 *** ## taskname_quad 280.2539 &lt; 2.2e-16 *** ## taskname_lin:CUE_high_gt_low 9.9409 0.001623 ** ## taskname_lin:demean_expect 21.6722 3.477e-06 *** ## CUE_high_gt_low:demean_expect 21.8899 2.981e-06 *** ## CUE_high_gt_low:taskname_quad 16.6325 4.565e-05 *** ## demean_expect:taskname_quad 64.3519 1.183e-15 *** ## taskname_lin:CUE_high_gt_low:demean_expect 38.7401 4.979e-10 *** ## CUE_high_gt_low:demean_expect:taskname_quad 6.0802 0.013681 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Code anova(model_wotask, model_task) ## refitting model(s) with ML (instead of REML) ## Data: pvc_sub ## Models: ## model_wotask: demean_outcome ~ CUE_high_gt_low * demean_expect + (1 | sub) ## model_task: demean_outcome ~ taskname_lin * CUE_high_gt_low * demean_expect + taskname_quad * CUE_high_gt_low * demean_expect + (1 | sub) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## model_wotask 6 141394 141440 -70691 141382 ## model_task 14 139396 139502 -69684 139368 2014.4 8 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Code # summary(model_test) Code model_task1 = lmer(demean_outcome~ factor(param_task_name)*demean_expect + (1 | sub), data = pvc_sub) model_wotask1 = lmer(demean_outcome~ demean_expect+ (1 | sub), data = pvc_sub) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Code summary(model_task1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: demean_outcome ~ factor(param_task_name) * demean_expect + (1 | ## sub) ## Data: pvc_sub ## ## REML criterion at convergence: 139725.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.2535 -0.6307 -0.1171 0.5506 5.2255 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub (Intercept) 0.03748 0.1936 ## Residual 613.93050 24.7776 ## Number of obs: 15091, groups: sub, 111 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) -8.081e+00 3.662e-01 8.076e+02 ## factor(param_task_name)pain 2.304e+01 5.519e-01 1.465e+04 ## factor(param_task_name)vicarious -1.434e+00 5.227e-01 1.508e+04 ## demean_expect 3.702e-01 1.369e-02 9.687e+03 ## factor(param_task_name)pain:demean_expect 1.136e-01 1.724e-02 3.523e+03 ## factor(param_task_name)vicarious:demean_expect -8.418e-02 1.912e-02 1.410e+04 ## t value Pr(&gt;|t|) ## (Intercept) -22.067 &lt; 2e-16 *** ## factor(param_task_name)pain 41.742 &lt; 2e-16 *** ## factor(param_task_name)vicarious -2.744 0.00607 ** ## demean_expect 27.033 &lt; 2e-16 *** ## factor(param_task_name)pain:demean_expect 6.589 5.07e-11 *** ## factor(param_task_name)vicarious:demean_expect -4.403 1.08e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fctr(prm_tsk_nm)p fctr(prm_tsk_nm)v dmn_xp ## fctr(prm_tsk_nm)p -0.662 ## fctr(prm_tsk_nm)v -0.699 0.464 ## demean_xpct 0.298 -0.198 -0.209 ## fctr(prm_tsk_nm)p:_ -0.237 -0.080 0.166 -0.794 ## fctr(prm_tsk_nm)v:_ -0.214 0.142 0.336 -0.716 ## fctr(prm_tsk_nm)p:_ ## fctr(prm_tsk_nm)p ## fctr(prm_tsk_nm)v ## demean_xpct ## fctr(prm_tsk_nm)p:_ ## fctr(prm_tsk_nm)v:_ 0.569 Code anova(model_task1) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value ## factor(param_task_name) 1451299 725650 2 14837.1 1181.974 ## demean_expect 1679513 1679513 1 14954.2 2735.674 ## factor(param_task_name):demean_expect 86935 43467 2 5101.8 70.802 ## Pr(&gt;F) ## factor(param_task_name) &lt; 2.2e-16 *** ## demean_expect &lt; 2.2e-16 *** ## factor(param_task_name):demean_expect &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Code anova(model_wotask1) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## demean_expect 4785248 4785248 1 15089 6564.5 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Code anova(model_wotask1, model_task1) ## refitting model(s) with ML (instead of REML) ## Data: pvc_sub ## Models: ## model_wotask1: demean_outcome ~ demean_expect + (1 | sub) ## model_task1: demean_outcome ~ factor(param_task_name) * demean_expect + (1 | sub) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## model_wotask1 4 142306 142337 -71149 142298 ## model_task1 8 139720 139781 -69852 139704 2594.7 4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Code library(emmeans) # emm1 = emmeans(model_task, specs = pairwise ~ stimintensity:task) # emm1 8.10 Alireza congruent incongruent (PE) categorize based on congruent, incongruent split dataframe Code # %% ------------------------------------------------------------------------- # load pain data # ---------------------------------------------------------------------------- main_dir = dirname(dirname(getwd())) datadir = file.path(main_dir, &#39;data&#39;, &#39;beh&#39;, &#39;beh02_preproc&#39;) subject_varkey &lt;- &quot;src_subject_id&quot; iv &lt;- &quot;param_cue_type&quot; xlab &lt;- &quot;&quot; taskname &lt;- &quot;pain&quot; ylab &lt;- &quot;ratings (degree)&quot; subject &lt;- &quot;subject&quot; exclude &lt;- &quot;sub-0001|sub-0003|sub-0004|sub-0005|sub-0025|sub-0999&quot; data = data.frame() data &lt;- load_task_social_df(datadir, taskname = taskname, subject_varkey = subject_varkey, iv = iv, exclude = exclude) data$event03_RT &lt;- data$event03_stimulusC_reseponseonset - data$event03_stimulus_displayonset dir.create(analysis_dir, showWarnings = FALSE, recursive = TRUE) maindata &lt;- data %&gt;% group_by(src_subject_id) %&gt;% mutate(event04_actual_angle = as.numeric(event04_actual_angle)) %&gt;% mutate(event02_expect_angle = as.numeric(event02_expect_angle)) %&gt;% mutate(avg_outcome = mean(event04_actual_angle, na.rm = TRUE)) %&gt;% mutate(demean_outcome = event04_actual_angle - avg_outcome) %&gt;% mutate(avg_expect = mean(event02_expect_angle, na.rm = TRUE)) %&gt;% mutate(demean_expect = event02_expect_angle - avg_expect) # %% ------------------------------------------------------------------------- # 0. argparse # ---------------------------------------------------------------------------- data_p2= maindata %&gt;% arrange(src_subject_id ) %&gt;% group_by(src_subject_id) %&gt;% mutate(trial_index = row_number()) data_a3 &lt;- data_p2 %&gt;% group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate(trial_index = row_number(param_run_num)) data_a3lag &lt;- data_a3 %&gt;% group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate(lag.demean_outcome = dplyr::lag(demean_outcome, n = 1, default = NA)) data_a3lag &lt;- data_a3lag %&gt;% mutate(EXPECT_cmc = avg_expect - mean(avg_expect)) data_a3lag_omit &lt;- data_a3lag[complete.cases(data_a3lag$lag.demean_outcome),] df &lt;- data_a3lag_omit pvc &lt;- simple_contrasts_beh(df) ## Warning: Unknown or uninitialised column: `stim_con_linear`. ## Warning: Unknown or uninitialised column: `stim_con_quad`. ## Warning: Unknown or uninitialised column: `CUE_high_gt_low`. ## Warning: Unknown or uninitialised column: `cue_name`. 8.10.1 archive this cell Code # previous_congruency &lt;- is_congruent(param_cue_type[previous_index], param_stimulus_type[previous_index]) is_congruent &lt;- function(cue, stim) { if (cue == &quot;low_cue&quot; &amp;&amp; stim == &quot;low_stim&quot;) { return(&quot;congruent&quot;) } else if (cue == &quot;low_cue&quot; &amp;&amp; stim == &quot;high_stim&quot;) { return(&quot;incongruent&quot;) } else if (cue == &quot;high_cue&quot; &amp;&amp; stim == &quot;low_stim&quot;) { return(&quot;incongruent&quot;) } else if (cue == &quot;high_cue&quot; &amp;&amp; stim == &quot;high_stim&quot;) { return(&quot;congruent&quot;) } else if (cue == &quot;high_cue&quot; &amp;&amp; stim == &quot;med_stim&quot;) { return(&quot;congruent&quot;) } else if (cue == &quot;low_cue&quot; &amp;&amp; stim == &quot;med_stim&quot;) { return(&quot;incongruent&quot;) } else { return(NA) # Handle other cases } } Code pvc.pain &lt;- pvc[pvc$param_task_name == &quot;pain&quot;,] pvc.pain$pe &lt;- pvc.pain$demean_outcome - pvc.pain$demean_expect # define congruency patterns pvc.pain$congruency[(pvc.pain$param_cue_type == &quot;low_cue&quot;) &amp; (pvc.pain$pe &lt; 0)] &lt;- &quot;congruent&quot; ## Warning: Unknown or uninitialised column: `congruency`. Code pvc.pain$congruency[(pvc.pain$param_cue_type == &quot;high_cue&quot;) &amp; (pvc.pain$pe &gt; 0)] &lt;- &quot;congruent&quot; pvc.pain$congruency[(pvc.pain$param_cue_type == &quot;low_cue&quot;) &amp; (pvc.pain$pe &gt; 0)] &lt;- &quot;incongruent&quot; pvc.pain$congruency[(pvc.pain$param_cue_type == &quot;high_cue&quot;) &amp; (pvc.pain$pe &lt; 0)] &lt;- &quot;incongruent&quot; library(dplyr) # definition: current cue level (high, low). # Find N-1 trial where cue level matches current trial # populate that as &quot;congruency_vector&quot; pvc.pain &lt;- pvc.pain %&gt;% # group by subject, session, run group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate( congruency_status = { congruency_vector &lt;- vector(&quot;character&quot;, n()) for (i in 1:n()) { # get the current cue. find the trial indices with the same cue level as current trial `matching_indices` current_cue &lt;- param_cue_type[i] # identifying all existing trials that matches the current cue level matching_indices &lt;- which(param_cue_type[1:(i - 1)] == current_cue) if (length(matching_indices) &gt; 0) { # from matching_indices, grab the largest number. # this maximum number would be the most recent N-1 cue trial, based on matching_indices previous_index &lt;- max(matching_indices) # populate congruency_vector with previous N-1 trial&#39;s congruency value. (NOTE: must match current cue level) congruency_vector[i] &lt;- congruency[previous_index] } else { congruency_vector[i] &lt;- NA } } congruency_vector } ) # ungroup() %&gt;% # mutate( # congruency_status = ifelse(is.na(congruency_status), pvc$congruency, congruency_status) # ) 8.11 Alireza congruent incongruent (incorrect) categorize based on congruent, incongruent split dataframe 8.11.1 archive this cell Code # previous_congruency &lt;- is_congruent(param_cue_type[previous_index], param_stimulus_type[previous_index]) is_congruent &lt;- function(cue, stim) { if (cue == &quot;low_cue&quot; &amp;&amp; stim == &quot;low_stim&quot;) { return(&quot;congruent&quot;) } else if (cue == &quot;low_cue&quot; &amp;&amp; stim == &quot;high_stim&quot;) { return(&quot;incongruent&quot;) } else if (cue == &quot;high_cue&quot; &amp;&amp; stim == &quot;low_stim&quot;) { return(&quot;incongruent&quot;) } else if (cue == &quot;high_cue&quot; &amp;&amp; stim == &quot;high_stim&quot;) { return(&quot;congruent&quot;) } else if (cue == &quot;high_cue&quot; &amp;&amp; stim == &quot;med_stim&quot;) { return(&quot;congruent&quot;) } else if (cue == &quot;low_cue&quot; &amp;&amp; stim == &quot;med_stim&quot;) { return(&quot;incongruent&quot;) } else { return(NA) # Handle other cases } } Code pvc &lt;- pvc[pvc$param_task_name == &quot;pain&quot;,] # define congruency patterns pvc.pain$congruency2[(pvc.pain$param_cue_type == &quot;low_cue&quot;) &amp; (pvc.pain$param_stimulus_type == &quot;low_stim&quot;)] &lt;- &quot;congruent&quot; ## Warning: Unknown or uninitialised column: `congruency2`. Code pvc.pain$congruency2[(pvc.pain$param_cue_type == &quot;high_cue&quot;) &amp; (pvc.pain$param_stimulus_type == &quot;low_stim&quot;)] &lt;- &quot;incongruent&quot; pvc.pain$congruency2[(pvc.pain$param_cue_type == &quot;low_cue&quot;) &amp; (pvc.pain$param_stimulus_type == &quot;med_stim&quot;)] &lt;- &quot;incongruent&quot; pvc.pain$congruency2[(pvc.pain$param_cue_type == &quot;high_cue&quot;) &amp; (pvc.pain$param_stimulus_type == &quot;med_stim&quot;)] &lt;- &quot;congruent&quot; pvc.pain$congruency2[(pvc.pain$param_cue_type == &quot;low_cue&quot;) &amp; (pvc.pain$param_stimulus_type == &quot;high_stim&quot;)] &lt;- &quot;incongruent&quot; pvc.pain$congruency2[(pvc.pain$param_cue_type == &quot;high_cue&quot;) &amp; (pvc.pain$param_stimulus_type == &quot;high_stim&quot;)] &lt;- &quot;congruent&quot; library(dplyr) # definition: current cue level (high, low). # Find N-1 trial where cue level matches current trial # populate that as &quot;congruency_vector&quot; pvc.pain &lt;- pvc.pain %&gt;% # group by subject, session, run group_by(src_subject_id, session_id, param_run_num) %&gt;% mutate( congruency_status2 = { congruency_vector &lt;- vector(&quot;character&quot;, n()) for (i in 1:n()) { # get the current cue. find the trial indices with the same cue level as current trial `matching_indices` current_cue &lt;- param_cue_type[i] # identifying all existing trials that matches the current cue level matching_indices &lt;- which(param_cue_type[1:(i - 1)] == current_cue) if (length(matching_indices) &gt; 0) { # from matching_indices, grab the largest number. # this maximum number would be the most recent N-1 cue trial, based on matching_indices previous_index &lt;- max(matching_indices) # populate congruency_vector with previous N-1 trial&#39;s congruency value. (NOTE: must match current cue level) congruency_vector[i] &lt;- congruency2[previous_index] } else { congruency_vector[i] &lt;- NA } } congruency_vector } ) # ungroup() %&gt;% # mutate( # congruency_status2 = ifelse(is.na(congruency_status2), pvc$congruency2, congruency_status2) # ) "],["ch17_mediation.html", "Chapter 9 [beh] Mediation outcome ~ cue * stim * expectrating * n-1outcomerating 9.1 mediation 9.2 mediation 2 9.3 mediation 3: Test same model using mediation() from MBESS 9.4 mediation 4: Test library mediation", " Chapter 9 [beh] Mediation outcome ~ cue * stim * expectrating * n-1outcomerating helpful resources https://nmmichalak.github.io/nicholas_michalak/blog_entries/2018/nrg01/nrg01.html ## What is the purpose of this notebook? {.unlisted .unnumbered} Here, I model the outcome ratings as a function of cue, stimulus intensity, expectation ratings, N-1 outcome rating. * As opposed to notebook 15, I want to check if the demeaning process should be for runs as opposed to subjects. * In other words, calculate the average within run and subtract ratings * Main model: lmer(outcome_rating ~ cue * stim * expectation rating + N-1 outcomerating) * Main question: What constitutes a reported outcome rating? * Sub questions: - If there is a linear relationship between expectation rating and outcome rating, does this differ as a function of cue? - How does a N-1 outcome rating affect current expectation ratings? - Later, is this effect different across tasks or are they similar? IV: stim (high / med / low) cue (high / low) expectation rating (continuous) N-1 outcome rating (continuous) DV: outcome rating Some thoughts, TODOs Standardized coefficients Slope difference? Intercept difference? ( cue and expectantion rating) Correct for the range (within participant) hypothesis: Larger expectation leads to prediction error Individual differences in ratings Outcome experience, based on behavioral experience What are the brain maps associated with each component. load data and combine participant data ## event02_expect_RT event04_actual_RT event02_expect_angle event04_actual_angle ## Min. :0.6504 Min. :0.0171 Min. : 0.00 Min. : 0.00 ## 1st Qu.:1.6200 1st Qu.:1.9188 1st Qu.: 29.55 1st Qu.: 37.83 ## Median :2.0511 Median :2.3511 Median : 57.58 Median : 60.49 ## Mean :2.1337 Mean :2.4011 Mean : 61.88 Mean : 65.47 ## 3rd Qu.:2.5589 3rd Qu.:2.8514 3rd Qu.: 88.61 3rd Qu.: 87.70 ## Max. :3.9912 Max. :3.9930 Max. :180.00 Max. :180.00 ## NA&#39;s :651 NA&#39;s :638 NA&#39;s :651 NA&#39;s :641 Covariance matrix: ratings and RT Covariance matrix: fixation durations (e.g. ISIs) 9.1 mediation Code psych::mediate(event04_actual_angle ~ CUE_high_gt_low*stim_con_linear+ event02_expect_angle + lag.04outcomeangle, data = pvc, n.iter = 1000) %&gt;% print(short = FALSE) ## ## Mediation/Moderation Analysis ## Call: psych::mediate(y = event04_actual_angle ~ CUE_high_gt_low * stim_con_linear + ## event02_expect_angle + lag.04outcomeangle, data = pvc, n.iter = 1000) ## ## The DV (Y) was event04_actual_angle . The IV (X) was CUE_high_gt_low stim_con_linear event02_expect_angle lag.04outcomeangle CUE_high_gt_low*stim_con_linear . The mediating variable(s) = .Call: psych::mediate(y = event04_actual_angle ~ CUE_high_gt_low * stim_con_linear + ## event02_expect_angle + lag.04outcomeangle, data = pvc, n.iter = 1000) ## ## No mediator specified leads to traditional regression ## event04_actual_angle se t df Prob ## Intercept 0.00 0.30 -0.01 5023 9.89e-01 ## CUE_high_gt_low -5.37 0.71 -7.54 5023 5.62e-14 ## stim_con_linear 34.42 0.75 46.13 5023 0.00e+00 ## event02_expect_angle 0.36 0.01 33.69 5023 1.84e-224 ## lag.04outcomeangle 0.48 0.01 46.08 5023 0.00e+00 ## CUE_high_gt_low*stim_con_linear 1.71 1.49 1.14 5023 2.53e-01 ## ## R = 0.83 R2 = 0.68 F = 2177.74 on 5 and 5023 DF p-value: 0 9.2 mediation 2 Code mod1 &lt;- &quot;# a path #thirst ~ a * room_temp event02_expect_angle ~ a * CUE_high_gt_low # b path #consume ~ b * thirst event04_actual_angle ~ b* event02_expect_angle # c prime path #consume ~ cp * room_temp event04_actual_angle ~ cp * CUE_high_gt_low # indirect and total effects ab := a * b total := cp + ab&quot; Code library(lavaan) ## This is lavaan 0.6-17 ## lavaan is FREE software! Please report any bugs. ## ## Attaching package: &#39;lavaan&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## cor2cov Code fsem1 &lt;- sem(mod1, data = pvc, se = &quot;bootstrap&quot;, bootstrap = 1000) ## Warning in lav_model_nvcov_bootstrap(lavmodel = lavmodel, lavsamplestats = ## lavsamplestats, : lavaan WARNING: 256 bootstrap runs failed or did not ## converge. Code summary(fsem1, standardized = TRUE) ## lavaan 0.6.17 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 4621 5029 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 744 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## event02_expect_angle ~ ## CUE_hgh__ (a) 34.622 1.059 32.695 0.000 34.622 0.429 ## event04_actual_angle ~ ## evnt02_x_ (b) 0.674 0.013 51.662 0.000 0.674 0.715 ## CUE_hgh__ (cp) -15.034 0.943 -15.936 0.000 -15.034 -0.198 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .evnt02_xpct_ng 1328.872 32.424 40.984 0.000 1328.872 0.816 ## .evnt04_ctl_ngl 825.688 21.676 38.092 0.000 825.688 0.571 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 23.321 0.854 27.311 0.000 23.321 0.307 ## total 8.287 1.153 7.188 0.000 8.287 0.109 Code parameterestimates(fsem1, boot.ci.type = &quot;bca.simple&quot;, standardized = TRUE) %&gt;% kable() lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox event02_expect_angle ~ CUE_high_gt_low a 34.6222421 1.0589323 32.695425 0 32.5381074 36.8315576 34.6222421 0.4289292 0.8579540 event04_actual_angle ~ event02_expect_angle b 0.6735862 0.0130382 51.662357 0 0.6389107 0.6943671 0.6735862 0.7148784 0.7148784 event04_actual_angle ~ CUE_high_gt_low cp -15.0337345 0.9433923 -15.935825 0 -16.8526515 -13.2346722 -15.0337345 -0.1976680 -0.3953800 event02_expect_angle ~~ event02_expect_angle 1328.8715236 32.4239609 40.984244 0 1258.5450975 1388.0485222 1328.8715236 0.8160197 0.8160197 event04_actual_angle ~~ event04_actual_angle 825.6875657 21.6759152 38.092397 0 784.8224538 874.9438625 825.6875657 0.5710990 0.5710990 CUE_high_gt_low ~~ CUE_high_gt_low 0.2499443 0.0000000 NA NA 0.2499443 0.2499443 0.2499443 1.0000000 0.2499443 ab := a*b ab 23.3210633 0.8539010 27.311202 0 21.5371219 25.0046717 23.3210633 0.3066322 0.6133328 total := cp+ab total 8.2873288 1.1529270 7.188078 0 6.0810341 10.5875605 8.2873288 0.1089642 0.2179528 9.3 mediation 3: Test same model using mediation() from MBESS ## Warning in resid.Y.on.X + resid.Y.on.M: longer object length is not a multiple ## of shorter object length ## Warning in resid.Y.on.X + resid.Y.on.M - resid.Y.on.X.and.M: longer object ## length is not a multiple of shorter object length ## Warning in standardized.resid.Y.on.X + standardized.resid.Y.on.M: longer object ## length is not a multiple of shorter object length ## Warning in standardized.resid.Y.on.X + standardized.resid.Y.on.M - ## standardized.resid.Y.on.X.and.M: longer object length is not a multiple of ## shorter object length ## Warning in abs(e.1M) + abs(e.1Y): longer object length is not a multiple of ## shorter object length ## Warning in abs(standardized.e.1M) + abs(standardized.e.1Y): longer object ## length is not a multiple of shorter object length ## [1] &quot;Bootstrap resampling has begun. This process may take a considerable amount of time if the number of replications is large, which is optimal for the bootstrap procedure.&quot; ## Estimate CI.Lower_BCa CI.Upper_BCa ## Indirect.Effect 23.32106331 NA NA ## Indirect.Effect.Partially.Standardized 0.61326642 NA NA ## Index.of.Mediation 0.30663221 NA NA ## R2_4.5 -0.02001083 NA NA ## R2_4.6 0.07764679 NA NA ## R2_4.7 0.18103664 NA NA ## Ratio.of.Indirect.to.Total.Effect 2.81406275 NA NA ## Ratio.of.Indirect.to.Direct.Effect -1.55124885 NA NA ## Success.of.Surrogate.Endpoint 0.23936430 NA NA ## Residual.Based_Gamma NA NA NA ## Residual.Based.Standardized_gamma NA NA NA ## SOS -1.68537707 NA NA 9.4 mediation 4: Test library mediation "],["ch18_simulation.html", "Chapter 10 [beh] RL simulation What is the purpose of this notebook? load data 10.1 function plot data", " Chapter 10 [beh] RL simulation What is the purpose of this notebook? Here, Aryan simulated behavioral outcome ratings using a reinforcement learning model. load data 10.1 function Code # summarize dataframe __________________________________________________________ plot_twovariable &lt;- function(df, iv1, iv2, group, subject, xmin, xmax, ymin,ymax, xlab, ylab, ggtitle, color_scheme, alpha, fit_lm, lm_method = NULL, identity_line=TRUE, size=NULL) { # x: iv1 e.g. expect rating # y: iv2 e.g. outcome rating # group: param_cue_type # subject: src_subject_id # xlab(&quot;expect rating&quot;) + # ylab(&quot;outcome rating&quot;) + # color_scheme = c(&quot;high_cue&quot; = &quot;#000000&quot;,low_cue&quot; = &quot;#BBBBBB&quot; ) library(ggplot2) df_dropna &lt;- df[!is.na(df[, iv1]) &amp; !is.na(df[, iv2]), ] subjectwise_naomit_2dv &lt;- meanSummary_2dv( df_dropna, c(subject, group), iv1, iv2 ) # subjectwise_naomit_2dv &lt;- na.omit(subjectwise_2dv) subjectwise_naomit_2dv[ , group] &lt;- as.factor(subjectwise_naomit_2dv[, group]) # plot _________________________________________________________________________ #nolint g &lt;- ggplot( data = subjectwise_naomit_2dv, aes( x = .data[[&quot;DV1_mean_per_sub&quot;]], y = .data[[&quot;DV2_mean_per_sub&quot;]], color = .data[[group]], size = size ) ) + geom_point( aes(shape = .data[[group]], color = .data[[group]]), size = 2, alpha = alpha) + theme(aspect.ratio = 1) + scale_color_manual(values = color_scheme) + scale_shape_manual(values = c(16, 3)) + xlab(xlab) + ylab(ylab) + ylim(ymin,ymax) + xlim(xmin,xmax) + ggtitle(ggtitle) + theme( axis.line = element_line(colour = &quot;grey50&quot;), panel.background = element_blank(), plot.subtitle = ggtext::element_textbox_simple(size = 11) ) if (isTRUE(fit_lm)) { g &lt;- g + # geom_ribbon(stat = &quot;smooth&quot;, method = lm_method, se = FALSE, alpha = 0.1, # aes(color = NULL, group = factor(group))) + geom_line(stat = &quot;smooth&quot;, method = lm_method, se = FALSE, alpha = 0.8, linewidth = 1.5) } else { g } if (isTRUE(identity_line)) { g &lt;- g + geom_abline( intercept = 0, slope = 1, color = &quot;#373737&quot;, # color = &quot;green&quot; linetype = &quot;dashed&quot;, linewidth = .5 ) } else { g } return(g) } plot data Code group = &quot;cue&quot; iv1 = &quot;demean_expect&quot; iv2 = &quot;demean_outcome&quot; subject = &quot;sub&quot; xmin=48; xmax=50; ymin=48; ymax=50 xlab = &quot;expectation rating&quot; ylab = &quot;outcome rating&quot; ggtitle = &quot;all stimulus intensity&quot; color_scheme = c(&quot;high_cue&quot; =&quot;#941100&quot;,&quot;low_cue&quot; = &quot;#5D5C5C&quot;) alpha = .8; fit_lm = TRUE; lm_method = &quot;lm&quot;; identity_line=TRUE; size=NULL g &lt;- ggplot( data = merged_df, aes( x = .data[[&quot;expectation&quot;]], y = .data[[&quot;outcome&quot;]], color = .data[[group]], size = size ) ) + geom_point( aes(shape = .data[[group]], color = .data[[group]]), size = 2, alpha = alpha) + theme(aspect.ratio = 1) + scale_color_manual(values = color_scheme) + scale_shape_manual(values = c(16, 3)) + xlab(xlab) + ylab(ylab) + ylim(ymin,ymax) + xlim(xmin,xmax) + ggtitle(ggtitle) + theme( axis.line = element_line(colour = &quot;grey50&quot;), panel.background = element_blank(), plot.subtitle = ggtext::element_textbox_simple(size = 11) ) if (isTRUE(fit_lm)) { g &lt;- g + # geom_ribbon(stat = &quot;smooth&quot;, method = lm_method, se = FALSE, alpha = 0.1, # aes(color = NULL, group = factor(group))) + geom_line(stat = &quot;smooth&quot;, method = lm_method, se = FALSE, alpha = 0.8, linewidth = 1.5) } else { g } if (isTRUE(identity_line)) { g &lt;- g + geom_abline( intercept = 0, slope = 1, color = &quot;#373737&quot;, # color = &quot;green&quot; linetype = &quot;dashed&quot;, linewidth = .5 ) } else { g } g ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.1.1 model 04 4-4 lineplot "],["ch18_simulation_aryan.html", "Chapter 11 [beh] RL simulation Aryan What is the purpose of this notebook? load data 11.1 Plot the relationship between expectation and outcome rating using model 4 simulations (Jepma) 11.2 Plot the relationship between expectation and outcome rating using model 2 simulations (Jepma) 11.3 correlation betweeen alpha_incongruent and cue trial slope (randome effects of cue) 11.4 correlation betweeen alpha_incongruent and NPS 11.5 correlation bettween NPS and PE", " Chapter 11 [beh] RL simulation Aryan What is the purpose of this notebook? Here, I model Aryans model fitted results, using the same scheme as my behavioral analysis (15*.Rmd) load data 11.1 Plot the relationship between expectation and outcome rating using model 4 simulations (Jepma) 11.2 Plot the relationship between expectation and outcome rating using model 2 simulations (Jepma) 11.2.1 model fits from model 2. expectation ratings (Jepma model) Code main_dir = dirname(dirname(getwd())) data &lt;- read.csv(file.path(main_dir, &#39;data/simulated/model_ver04_0508/table_pain_new.csv&#39;)) subjectwise_2dv &lt;- meanSummary_2dv(data, c(&quot;src_subject_id&quot;), &quot;event02_expect_angle&quot;, &quot;Exp_mdl2&quot; ) ggplot(data = subjectwise_2dv, aes(x = .data[[&quot;DV1_mean_per_sub&quot;]], y = .data[[&quot;DV2_mean_per_sub&quot;]], size = .5 )) + geom_point(size = 2, alpha = .5 ) + ylim(0,180) + xlim(0,180) + coord_fixed() + geom_abline(intercept = 0, slope = 1, color = &quot;#373737&quot;, linetype = &quot;dashed&quot;, linewidth = .5) + xlab(&quot;Observed\\nexpectation rating&quot;) + ylab(&quot;Model-fitted \\nexpectation rating&quot;)+ theme( axis.line = element_line(colour = &quot;grey50&quot;), panel.background = element_blank(), plot.subtitle = ggtext::element_textbox_simple(size = 1), axis.text.x = element_text(size = 10), axis.text.y = element_text(size = 10), axis.title.x = element_text(size = 15), axis.title.y = element_text(size = 15) ) 11.2.2 model fits from model 2. outcome ratings (Jepma model) Code subjectwise_2dv &lt;- meanSummary_2dv(data, c(&quot;src_subject_id&quot;), &quot;event04_actual_angle&quot;, &quot;Pain_mdl2&quot; ) ggplot(data = subjectwise_2dv, aes(x = .data[[&quot;DV1_mean_per_sub&quot;]], y = .data[[&quot;DV2_mean_per_sub&quot;]], size = .5 )) + geom_point(size = 2, alpha = .5 ) + ylim(0,180) + xlim(0,180) + coord_fixed() + geom_abline(intercept = 0, slope = 1, color = &quot;#373737&quot;, linetype = &quot;dashed&quot;, linewidth = .5) + xlab(&quot;Observed\\noutcome rating&quot;) + ylab(&quot;Model-fitted \\noutcome rating&quot;)+ theme( axis.line = element_line(colour = &quot;grey50&quot;), panel.background = element_blank(), plot.subtitle = ggtext::element_textbox_simple(size = 1), axis.text.x = element_text(size = 15), axis.text.y = element_text(size = 15), axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20) ) 11.3 correlation betweeen alpha_incongruent and cue trial slope (randome effects of cue) ## Warning: Removed 3 rows containing missing values (`geom_point()`). ## Warning: Removed 2 rows containing missing values (`geom_point()`). 11.4 correlation betweeen alpha_incongruent and NPS Code # load dataframe NPS &lt;- data.frame(read.csv(file.path(main_dir, &#39;data/NPS_curated.csv&#39;))) NPS &lt;- NPS %&gt;% mutate(congruency = case_when( cuetype == &quot;cuetype-low&quot; &amp; stimintensity == &quot;low&quot; ~ &quot;congruent&quot;, cuetype == &quot;cuetype-high&quot; &amp; stimintensity == &quot;high&quot; ~ &quot;congruent&quot;, cuetype == &quot;cuetype-low&quot; &amp; stimintensity == &quot;high&quot; ~ &quot;incongruent&quot;, cuetype == &quot;cuetype-high&quot; &amp; stimintensity == &quot;low&quot; ~ &quot;incongruent&quot;, TRUE ~ &quot;other&quot; )) NPS_congru &lt;- NPS %&gt;% group_by(sub) %&gt;% summarise(avg_diff = mean(NPSpos[congruency == &quot;congruent&quot;]) - mean(NPSpos[congruency == &quot;incongruent&quot;])) # grab alpha_incongruent model_param &lt;- data.frame(read.csv(file.path(main_dir, &quot;data/RL/modelfit_jepma_0525/par_mdl2_pain.csv&quot;))) model_param &lt;- model_param %&gt;% mutate(sub = sprintf(&quot;sub-%04d&quot;, subj_num_new_pain)) # Merge the two dataframes based on the &quot;sub&quot; column merged_NPS &lt;- merge(NPS_congru, model_param, by = &quot;sub&quot;) merged_NPS$alpha_c_gt_i &lt;- merged_NPS$alpha_c - merged_NPS$alpha_i # grab cue slope # grab intersection of subject ids # plot ggplot ggplot(data = merged_NPS, aes(x = .data[[&quot;avg_diff&quot;]], y = .data[[&quot;alpha_c_gt_i&quot;]], size = .5 )) + geom_point(size = 2, alpha = .5 ) + ylim(-10,10) + xlim(-10,10) + coord_fixed() + geom_abline(intercept = 0, slope = 1, color = &quot;#373737&quot;, linetype = &quot;dashed&quot;, linewidth = .5) + xlab(&quot;NPS \\n(congruent &gt; incongruent)&quot;) + ylab(&quot;Alpha \\n(congruent &gt; incongruent)&quot;)+ theme( axis.line = element_line(colour = &quot;grey50&quot;), panel.background = element_blank(), plot.subtitle = ggtext::element_textbox_simple(size = 1), axis.text.x = element_text(size = 15), axis.text.y = element_text(size = 15), axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20) ) Code # run lmer 11.5 correlation bettween NPS and PE 11.5.1 test similarity between NPS positive values and PE (11/06/2023) Code PEdf &lt;- read.csv(file.path(main_dir, &#39;data/RL/modelfit_jepma_0525/table_pain.csv&#39;)) NPS &lt;- data.frame(read.csv(file.path(main_dir, &#39;data/NPS_curated.csv&#39;))) PEdf &lt;- PEdf %&gt;% mutate(sub = sprintf(&quot;sub-%04d&quot;, src_subject_id), ses = sprintf(&quot;ses-%02d&quot;, session_id), run = sprintf(&quot;run-%02d&quot;, param_run_num), trial = sprintf(&quot;trial-%03d&quot;, trial_index_runwise-1) ) merged_NPSpe &lt;- merge(NPS, PEdf, by = c(&quot;sub&quot;, &quot;ses&quot;, &quot;run&quot;, &quot;trial&quot;)) subjectwise_2dv &lt;- meanSummary_2dv(merged_NPSpe, c(&quot;src_subject_id&quot;,&quot;stimintensity&quot;, &quot;cuetype&quot;), &quot;PE_mdl2&quot;, &quot;NPSpos&quot; ) ggplot(data = subjectwise_2dv, aes(x = .data[[&quot;DV1_mean_per_sub&quot;]], y = .data[[&quot;DV2_mean_per_sub&quot;]], color = .data[[&quot;cuetype&quot;]], # shape = .data[[&quot;stimintensity&quot;]], # size = .5 )) + geom_point(size = 2, alpha = .5 ) + ylim(-50,50) + xlim(-50,50) + coord_fixed() + scale_color_manual(values = c(&quot;cuetype-high&quot; =&quot;red&quot;,&quot;cuetype-low&quot; = &quot;#5D5C5C&quot;))+ geom_abline(intercept = 0, slope = 1, color = &quot;#373737&quot;, linetype = &quot;dashed&quot;, linewidth = .5) + xlab(&quot;PE&quot;) + ylab(&quot;NPSpos&quot;)+ theme( axis.line = element_line(colour = &quot;grey50&quot;), panel.background = element_blank(), plot.subtitle = ggtext::element_textbox_simple(size = 1), axis.text.x = element_text(size = 15), axis.text.y = element_text(size = 15), axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20) ) ## Warning: Removed 13 rows containing missing values (`geom_point()`). Code model.25 &lt;- lmer(merged_NPSpe$NPSpos ~ merged_NPSpe$PE_mdl2 + (1|sub), data = merged_NPSpe) summary(model.25) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: merged_NPSpe$NPSpos ~ merged_NPSpe$PE_mdl2 + (1 | sub) ## Data: merged_NPSpe ## ## REML criterion at convergence: 20826.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.4290 -0.5073 -0.0168 0.5202 5.5313 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub (Intercept) 28.52 5.340 ## Residual 68.76 8.292 ## Number of obs: 2922, groups: sub, 54 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 6.501e+00 7.457e-01 5.305e+01 8.718 8.06e-12 *** ## merged_NPSpe$PE_mdl2 3.876e-02 6.866e-03 2.896e+03 5.645 1.81e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## mr_NPS$PE_2 -0.052 11.5.2 test relationship between PE and cue type and stimintensity (06/16/2023) Code model.PENPS &lt;- lmer(NPSpos ~ PE_mdl2*cuetype*stimintensity + (1|sub), data = merged_NPSpe) summary(model.PENPS) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: NPSpos ~ PE_mdl2 * cuetype * stimintensity + (1 | sub) ## Data: merged_NPSpe ## ## REML criterion at convergence: 20816 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.5455 -0.5260 -0.0153 0.5196 5.6086 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sub (Intercept) 28.32 5.321 ## Residual 68.21 8.259 ## Number of obs: 2922, groups: sub, 54 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) 7.590e+00 8.362e-01 8.488e+01 ## PE_mdl2 -1.962e-02 3.555e-02 2.887e+03 ## cuetypecuetype-low 1.515e+00 9.025e-01 2.890e+03 ## stimintensitylow -1.753e+00 8.436e-01 2.883e+03 ## stimintensitymed -2.107e+00 6.415e-01 2.873e+03 ## PE_mdl2:cuetypecuetype-low 2.818e-03 4.109e-02 2.887e+03 ## PE_mdl2:stimintensitylow 4.253e-02 4.452e-02 2.883e+03 ## PE_mdl2:stimintensitymed 1.800e-03 4.896e-02 2.870e+03 ## cuetypecuetype-low:stimintensitylow -1.618e+00 1.252e+00 2.893e+03 ## cuetypecuetype-low:stimintensitymed 4.626e-01 1.160e+00 2.868e+03 ## PE_mdl2:cuetypecuetype-low:stimintensitylow -2.715e-02 5.295e-02 2.867e+03 ## PE_mdl2:cuetypecuetype-low:stimintensitymed 2.309e-02 5.654e-02 2.863e+03 ## t value Pr(&gt;|t|) ## (Intercept) 9.077 3.81e-14 *** ## PE_mdl2 -0.552 0.58111 ## cuetypecuetype-low 1.678 0.09337 . ## stimintensitylow -2.077 0.03786 * ## stimintensitymed -3.284 0.00104 ** ## PE_mdl2:cuetypecuetype-low 0.069 0.94533 ## PE_mdl2:stimintensitylow 0.955 0.33953 ## PE_mdl2:stimintensitymed 0.037 0.97068 ## cuetypecuetype-low:stimintensitylow -1.293 0.19626 ## cuetypecuetype-low:stimintensitymed 0.399 0.69017 ## PE_mdl2:cuetypecuetype-low:stimintensitylow -0.513 0.60816 ## PE_mdl2:cuetypecuetype-low:stimintensitymed 0.408 0.68304 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) PE_md2 ctypc- stmntnstyl stmntnstym ## PE_mdl2 -0.215 ## ctypctyp-lw -0.224 0.174 ## stmntnstylw -0.252 0.234 0.273 ## stmntnstymd -0.329 0.314 0.308 0.365 ## PE_mdl2:ct- 0.184 -0.850 -0.565 -0.232 -0.276 ## PE_mdl2:stmntnstyl 0.164 -0.780 -0.096 0.285 -0.217 ## PE_mdl2:stmntnstym 0.141 -0.668 -0.096 -0.102 0.138 ## ctypctyp-lw:stmntnstyl 0.168 -0.138 -0.747 -0.704 -0.249 ## ctypctyp-lw:stmntnstym 0.180 -0.159 -0.741 -0.221 -0.557 ## PE_mdl2:ctypctyp-lw:stmntnstyl -0.136 0.636 0.359 -0.222 0.181 ## PE_mdl2:ctypctyp-lw:stmntnstym -0.121 0.572 0.337 0.095 -0.117 ## PE_m2:- PE_mdl2:stmntnstyl PE_mdl2:stmntnstym ## PE_mdl2 ## ctypctyp-lw ## stmntnstylw ## stmntnstymd ## PE_mdl2:ct- ## PE_mdl2:stmntnstyl 0.635 ## PE_mdl2:stmntnstym 0.554 0.566 ## ctypctyp-lw:stmntnstyl 0.434 -0.235 0.042 ## ctypctyp-lw:stmntnstym 0.431 0.090 -0.099 ## PE_mdl2:ctypctyp-lw:stmntnstyl -0.705 -0.807 -0.454 ## PE_mdl2:ctypctyp-lw:stmntnstym -0.643 -0.478 -0.855 ## ctypctyp-lw:stmntnstyl ctypctyp-lw:stmntnstym ## PE_mdl2 ## ctypctyp-lw ## stmntnstylw ## stmntnstymd ## PE_mdl2:ct- ## PE_mdl2:stmntnstyl ## PE_mdl2:stmntnstym ## ctypctyp-lw:stmntnstyl ## ctypctyp-lw:stmntnstym 0.562 ## PE_mdl2:ctypctyp-lw:stmntnstyl -0.068 -0.295 ## PE_mdl2:ctypctyp-lw:stmntnstym -0.226 -0.264 ## PE_mdl2:ctypctyp-lw:stmntnstyl ## PE_mdl2 ## ctypctyp-lw ## stmntnstylw ## stmntnstymd ## PE_mdl2:ct- ## PE_mdl2:stmntnstyl ## PE_mdl2:stmntnstym ## ctypctyp-lw:stmntnstyl ## ctypctyp-lw:stmntnstym ## PE_mdl2:ctypctyp-lw:stmntnstyl ## PE_mdl2:ctypctyp-lw:stmntnstym 0.520 11.5.3 plot the relationship between PE and NPS as a function of cue Code ggplot(data = merged_NPSpe, aes(x = .data[[&quot;PE_mdl2&quot;]], y = .data[[&quot;NPSpos&quot;]], color = .data[[&quot;cuetype&quot;]], size = .5 )) + geom_point(size = 2, alpha = .5 ) + ylim(-150,150) + xlim(-150,150) + coord_fixed() + scale_color_manual(values = c(&quot;cuetype-high&quot; =&quot;red&quot;,&quot;cuetype-low&quot; = &quot;#5D5C5C&quot;))+ geom_abline(intercept = 0, slope = 1, color = &quot;#373737&quot;, linetype = &quot;dashed&quot;, linewidth = .5) + xlab(&quot;PE&quot;) + ylab(&quot;NPSpos&quot;)+ theme( axis.line = element_line(colour = &quot;grey50&quot;), panel.background = element_blank(), plot.subtitle = ggtext::element_textbox_simple(size = 1), axis.text.x = element_text(size = 15), axis.text.y = element_text(size = 15), axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20) ) 11.5.4 plot the relationship between PE and NPS as a function of cue and stimulus intensity "],["ch20_npssimulation.html", "Chapter 12 [model] NPSsimulation 12.1 simulation ** Lineplots Original", " Chapter 12 [model] NPSsimulation 12.0.1 Function 12.0.2 NPS data 12.0.3 behavioral data Q. Within pain task, Does stimulus intenisty level and cue level significantly predict NPS dotproducts? 12.0.4 get pain relationship, controlling for cue, cuetype, expect Code model.stim &lt;- lmer(event04_actual_angle ~ STIM_linear + CUE_high_gt_low + STIM_quadratic+ EXPECT_demean + EXPECT_cmc + ses + (1|sub), data = df ) # CUE_high_gt_low+STIM+EXPECT_demean sjPlot::tab_model(model.stim, title = &quot;Multilevel-modeling: \\nlmer(NPSpos ~ CUE + STIM + EXPECT_demean + SES + (1| sub), data = pvc)&quot;, CSS = list(css.table = &#39;+font-size: 12;&#39;)) Multilevel-modeling: lmer(NPSpos ~ CUE + STIM + EXPECT_demean + SES + (1| sub), data = pvc)   event04_actual_angle Predictors Estimates CI p (Intercept) 71.33 68.56 – 74.10 &lt;0.001 STIM linear 29.89 28.43 – 31.34 &lt;0.001 CUE high gt low -2.04 -3.55 – -0.53 0.008 STIM quadratic 1.34 0.07 – 2.62 0.039 EXPECT demean 0.30 0.28 – 0.33 &lt;0.001 EXPECT cmc 0.93 0.84 – 1.02 &lt;0.001 sesses-03 -7.22 -8.86 – -5.59 &lt;0.001 sesses-04 -7.45 -9.07 – -5.83 &lt;0.001 Random Effects σ2 366.84 τ00 sub 157.35 ICC 0.30 N sub 96 Observations 4004 Marginal R2 / Conditional R2 0.645 / 0.751 Code # re.beta &lt;- coef(model.stim)$unit[,&quot;x&quot;] fixEffect_expect &lt;-as.data.frame(fixef(model.stim)) randEffect_expect &lt;-as.data.frame(ranef(model.stim)) Code ntrials = 12 lowintens = 48; medintens = 49; highintens = 50; stim &lt;- 48:50 painmean = 30 # average pain; arbitrary, on a 0 - 100 scale painslope = fixEffect_expect[&#39;STIM_linear&#39;,1] # rise in pain per unit change in stim (per degree) painslope_stan = 0.33621048 stdCoef.merMod &lt;- function(object) { sdy &lt;- sd(getME(object,&quot;y&quot;)) sdx &lt;- apply(getME(object,&quot;X&quot;), 2, sd) sc &lt;- fixef(object)*sdx/sdy se.fixef &lt;- coef(summary(object))[,&quot;Std. Error&quot;] se &lt;- se.fixef*sdx/sdy return(data.frame(stdcoef=sc, stdse=se)) } stdCoef.merMod(model.stim) ## stdcoef stdse ## (Intercept) 0.00000000 0.000000000 ## STIM_linear 0.31648413 0.007848709 ## CUE_high_gt_low -0.02640086 0.009953683 ## STIM_quadratic 0.01621649 0.007849931 ## EXPECT_demean 0.22157672 0.009947333 ## EXPECT_cmc 0.69471095 0.034434185 ## sesses-03 -0.08777975 0.010153298 ## sesses-04 -0.09126342 0.010120143 Code # # library(limma) # S &lt;- rep(stim,times=ntrials) # stim # C &lt;- rep(rep(c(1,-1), each = 3), times = 6) #cue # E &lt;- painslope * (C + rnorm(length(C))) + painmean # pseudo nociception # Szscore &lt;- (S - mean(S)) / sd(S) df$S &lt;- as.numeric(mapvalues(df$stimintensity, from = c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;), c(48, 49, 50))) df$C &lt;- as.numeric(mapvalues(df$cuetype, from = c(&quot;cuetype-low&quot;, &quot;cuetype-high&quot;), c(-1, 1))) df$E &lt;- painslope * (df$C + rnorm(length(df$C))) + painmean df$Szscore &lt;- (df$S - mean(df$S, na.rm = TRUE)) / sd(df$S) df$Pcalib = df$Szscore * painslope + painmean + rnorm(length(df$C)) model.stim2pain &lt;- lmer(Pcalib ~ S + (1|sub), df) b_stim2pain = fixef(model.stim2pain)[2] #0.4126089 #36.5757 df$Sprime = df$Szscore * b_stim2pain + painmean # subjective pain experience, converted to a scale of 0-180, in order to match expectation ratings # df$Sprime = df$S * b_stim2pain df &lt;- df %&gt;% group_by(sub) %&gt;% mutate(E = as.numeric(E)) %&gt;% mutate(avg_E = mean(E, na.rm = TRUE)) %&gt;% mutate(E_demean = E - avg_E) %&gt;% mutate(E_cmc = avg_E - mean(avg_E)) 12.1 simulation ** Code w = 0.7 error = rnorm(length(df$C)) df$P.assim &lt;- w * df$Sprime + (1 - w) * df$E + error df$P.pe = df$Sprime - df$E + error df$P.adapt &lt;- 1 minimal.diff &lt;- (df$Sprime - df$E)/std(df$Sprime) &lt; b_stim2pain large.diff &lt;- (df$Sprime - df$E)/std(df$Sprime) &gt; b_stim2pain df$P.adapt[minimal.diff] &lt;- w * df$Sprime[minimal.diff] + (1 - w) * df$E[minimal.diff] + error[minimal.diff] df$P.adapt[large.diff] &lt;- w * df$Sprime[large.diff] + error[large.diff] Lineplots Original ## Automatically converting the following non-factors to factors: cue_name Lineplots P.assim ## Automatically converting the following non-factors to factors: cue_name ### P.assim ~ demeaned_expect * cue * stim Lineplots P.pe ## Automatically converting the following non-factors to factors: cue_name 12.1.1 P.pe ~ demeaned_expect * cue * stim Lineplots P.adapt ## Automatically converting the following non-factors to factors: cue_name 12.1.2 P.adapt ~ demeaned_expect * cue * stim "],["ch51_fir_glasserTPJttl2.html", "Chapter 13 [fMRI] FIR ~ task 13.1 parameters {TODO: ignore}", " Chapter 13 [fMRI] FIR ~ task TODO * load tsv * concatenate * per time column, calculate mean and variance * plot Code plot_timeseries_onefactor &lt;- function(df, iv1, mean, error, xlab, ylab, ggtitle, color) { n_points &lt;- 100 # Number of points for interpolation g &lt;- ggplot( data = df, aes( x = .data[[iv1]], y = .data[[mean]], group = 1, color = color ), cex.lab = 1.5, cex.axis = 2, cex.main = 1.5, cex.sub = 1.5 ) + geom_errorbar(aes( ymin = (.data[[mean]] - .data[[error]]), ymax = (.data[[mean]] + .data[[error]]), color = color ), width = .1, alpha=0.8) + geom_line() + geom_point(color=color) + ggtitle(ggtitle) + xlab(xlab) + ylab(ylab) + theme_classic() + theme(aspect.ratio = .6) + expand_limits(x = 3.25) + scale_color_manual(&quot;&quot;, values = color) + # scale_fill_manual(&quot;&quot;, # values = color) + theme( legend.position = c(.99, .99), legend.justification = c(&quot;right&quot;, &quot;top&quot;), legend.box.just = &quot;right&quot;, legend.margin = margin(6, 6, 6, 6) ) + theme(legend.key = element_rect(fill = &quot;white&quot;, colour = &quot;white&quot;)) + theme_bw() return(g) } 13.1 parameters {TODO: ignore} Code # parameters main_dir &lt;- dirname(dirname(getwd())) datadir &lt;- file.path(main_dir, &#39;analysis/fmri/nilearn/glm/fir&#39;) analysis_folder = paste0(&quot;model52_iv-6cond_dv-firglasserSPM_ttl2&quot;) analysis_dir &lt;- file.path(main_dir, &quot;analysis&quot;, &quot;mixedeffect&quot;, analysis_folder, as.character(Sys.Date())) # nolint dir.create(analysis_dir, showWarnings = FALSE, recursive = TRUE) save_dir &lt;- analysis_dir "],["taskwise-stim-effect.html", "Chapter 14 taskwise stim effect 14.1 DEP: epoch: stim, high cue vs low cue 14.2 taskwise cue effect 14.3 epoch: stim, rating 14.4 epoch: 6 cond", " Chapter 14 taskwise stim effect Code roi_list &lt;- c(&#39;rINS&#39;, &#39;TPJ&#39;, &#39;dACC&#39;, &#39;PHG&#39;, &#39;V1&#39;, &#39;SM&#39;, &#39;MT&#39;, &#39;RSC&#39;, &#39;LOC&#39;, &#39;FFC&#39;, &#39;PIT&#39;, &#39;pSTS&#39;, &#39;AIP&#39;, &#39;premotor&#39;) run_types &lt;- c(&quot;pain&quot;, &quot;vicarious&quot;, &quot;cognitive&quot;) plot_list &lt;- list() TR_length &lt;- 42 for (ROI in roi_list) { datadir = file.path(main_dir, &quot;analysis/fmri/spm/fir/ttl2par&quot;) taskname = &#39;pain&#39; exclude &lt;- &quot;sub-0001&quot; filename &lt;- paste0(&quot;sub-*&quot;, &quot;*roi-&quot;, ROI, &quot;_tr-42.csv&quot;) common_path &lt;- Sys.glob(file.path(datadir, &quot;sub-*&quot;, filename )) filter_path &lt;- common_path[!str_detect(common_path, pattern = exclude)] df &lt;- do.call(&quot;rbind.fill&quot;, lapply(filter_path, FUN = function(files) { read.table(files, header = TRUE, sep = &quot;,&quot;) })) for (run_type in run_types) { print(run_type) filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type), ] parsed_df &lt;- filtered_df %&gt;% separate(condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE) # --------------------- subset regions based on ROI ---------------------------- df_long &lt;- pivot_longer(parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot;) # ----------------------------- clean factor ----------------------------------- df_long$tr_ordered &lt;- factor( df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length)) ) df_long$stim_ordered &lt;- factor( df_long$stim, levels = c(&quot;stimH&quot;, &quot;stimM&quot;, &quot;stimL&quot;) ) # --------------------------- summary statistics ------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;,&quot;tr_ordered&quot;, &quot;stim_ordered&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c( &quot;stim_ordered&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- run_type # https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 # ... Rest of your data processing code ... # subset &lt;- groupwise[groupwise$runtype == run_type, ] LINEIV1 = &quot;tr_ordered&quot; LINEIV2 = &quot;stim_ordered&quot; MEAN = &quot;mean_per_sub_norm_mean&quot; ERROR = &quot;se&quot; dv_keyword = &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices, ] p1 &lt;- plot_timeseries_bar(groupwise_sorted, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;TRs&quot;, ylab = paste0(ROI, &quot; activation (A.U.)&quot;), ggtitle = paste0(ROI, &quot;: &quot;,run_type, &quot; (N = &quot;, length(unique(subjectwise$sub)),&quot;) time series, Epoch - stimulus&quot;), color = c(&quot;#5f0f40&quot;,&quot;#ae2012&quot;, &quot;#fcbf49&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) #p1 &lt;- p1 + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:(7 + TR_length)])) + theme_classic() plot_list[[run_type]] &lt;- p1 + theme_classic() } # --------------------------- plot three tasks ------------------------------- library(gridExtra) plot_list &lt;- lapply(plot_list, function(plot) { plot + theme(plot.margin = margin(5, 5, 5, 5)) # Adjust plot margins if needed }) combined_plot &lt;- ggpubr::ggarrange(plot_list[[&quot;pain&quot;]],plot_list[[&quot;vicarious&quot;]],plot_list[[&quot;cognitive&quot;]], common.legend = TRUE,legend = &quot;bottom&quot;, ncol = 3, nrow = 1, widths = c(3, 3, 3), heights = c(.5,.5,.5), align = &quot;v&quot;) combined_plot ggsave(file.path(save_dir, paste0(&quot;roi-&quot;, ROI,&quot;_epoch-stim_desc-highstimGTlowstim.png&quot;)), combined_plot, width = 12, height = 4) } ## [1] &quot;pain&quot; ## ## Attaching package: &#39;raincloudplots&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## GeomFlatViolin ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; ## [1] &quot;pain&quot; ## [1] &quot;vicarious&quot; ## [1] &quot;cognitive&quot; 14.0.1 PCA subjectwise Code # install.packages(&quot;ggplot2&quot;) # Install ggplot2 if you haven&#39;t already # install.packages(&quot;FactoMineR&quot;) # Install FactoMineR if you haven&#39;t already library(ggplot2) library(FactoMineR) run_types = c(&quot;pain&quot;) for (run_type in run_types) { print(run_type) filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type), ] parsed_df &lt;- filtered_df %&gt;% separate(condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE) # --------------------- subset regions based on ROI ---------------------------- df_long &lt;- pivot_longer(parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot;) # ----------------------------- clean factor ----------------------------------- df_long$tr_ordered &lt;- factor( df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length)) ) df_long$stim_ordered &lt;- factor( df_long$stim, levels = c(&quot;stimH&quot;, &quot;stimM&quot;, &quot;stimL&quot;) ) # --------------------------- summary statistics ------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;,&quot;tr_ordered&quot;, &quot;stim_ordered&quot;), &quot;tr_value&quot;) # Assuming your original dataframe is named &#39;df&#39; # Convert the dataframe to wide format df_wide &lt;- pivot_wider(subjectwise, id_cols = c(&quot;tr_ordered&quot;, &quot;stim_ordered&quot;), names_from = c(&quot;sub&quot;), values_from = &quot;mean_per_sub&quot;) # df_wide &lt;- pivot_wider(subjectwise, # id_cols = c(&quot;sub&quot;, &quot;ROIindex&quot;,&quot;stim_ordered&quot;), # names_from = &quot;tr_ordered&quot;, # values_from = &quot;mean_per_sub&quot;) stim_high.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimH&quot;,] stim_med.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimM&quot;,] stim_low.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimL&quot;,] # selected_columns &lt;- subset(stim_high.df, select = 2:(ncol(stim_high.df) - 1)) meanhighdf &lt;- data.frame(subset(stim_high.df, select = 3:(ncol(stim_high.df) - 1))) high.pca_result &lt;- prcomp(meanhighdf) high.pca_scores &lt;- as.data.frame(high.pca_result$x) # Access the proportion of variance explained by each principal component high.variance_explained &lt;- high.pca_result$sdev^2 / sum(high.pca_result$sdev^2) plot(high.variance_explained) # Access the standard deviations of each principal component high.stdev &lt;- high.pca_result$sdev meanmeddf &lt;- data.frame(subset(stim_med.df, select = 3:(ncol(stim_med.df) - 1))) med.pca &lt;- prcomp(meanmeddf) med.pca_scores &lt;- as.data.frame(med.pca$x) meanlowdf &lt;- data.frame(subset(stim_low.df, select = 3:(ncol(stim_low.df) - 1))) low.pca &lt;- prcomp(meanlowdf) low.pca_scores &lt;- as.data.frame(low.pca$x) library(plotly) # You can use plotly to create an interactive 3D plot # plot_ly(high.pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) # plot_ly(low.pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) combined_pca_scores &lt;- rbind(high.pca_scores, med.pca_scores, low.pca_scores) # Add a new column to indicate the stim_ordered category (high_stim or low_stim) combined_pca_scores$stim_ordered &lt;- c(rep(&quot;high_stim&quot;, nrow(high.pca_scores)), rep(&quot;med_stim&quot;, nrow(med.pca_scores)), rep(&quot;low_stim&quot;, nrow(low.pca_scores))) # Create the 3D PCA plot plot_ly(combined_pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = ~stim_ordered) # data_matrix &lt;- groupwise[groupwise$stim_ordered == &quot;high_stim&quot;,c(&quot;tr_ordered&quot;, &quot;mean_per_sub_norm_mean&quot;)] # sorted_indices &lt;- order(data_matrix$tr_ordered) # df_ordered &lt;- data_matrix[sorted_indices, ] # pca_result &lt;- PCA(data_matrix$mean_per_sub_norm_mean) # datapoints &lt;- df$datapoints } ## [1] &quot;pain&quot; ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following objects are masked from &#39;package:plyr&#39;: ## ## arrange, mutate, rename, summarise ## The following object is masked from &#39;package:reshape&#39;: ## ## rename ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout Code plot_ly(combined_pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = ~stim_ordered) 14.0.2 PCA groupwise Code # install.packages(&quot;ggplot2&quot;) # Install ggplot2 if you haven&#39;t already # install.packages(&quot;FactoMineR&quot;) # Install FactoMineR if you haven&#39;t already library(ggplot2) library(FactoMineR) # Assuming your original dataframe is named &#39;df&#39; # Convert the dataframe to wide format df_wide.group &lt;- pivot_wider(subjectwise, id_cols = c(&quot;tr_ordered&quot;, &quot;stim_ordered&quot;), names_from = &quot;sub&quot;, values_from = &quot;mean_per_sub&quot;) # ------ # data_matrix &lt;- groupwise[groupwise$stim_ordered == &quot;high_stim&quot;,c(&quot;tr_ordered&quot;, &quot;mean_per_sub_norm_mean&quot;)] # sorted_indices &lt;- order(data_matrix$tr_ordered) # df_ordered &lt;- data_matrix[sorted_indices, ] # datapoints &lt;- df_ordered$mean_per_sub_norm_mean # data_df &lt;- data.frame(Dim1 = datapoints, Dim2 = datapoints, Dim3 = datapoints) # pca &lt;- prcomp(data_df) # pca_scores &lt;- as.data.frame(pca$x) # plot_ly(pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) # ------- stim_high.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimH&quot;,] stim_low.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimL&quot;,] # selected_columns &lt;- subset(stim_high.df, select = 2:(ncol(stim_high.df) - 1)) meanhighdf &lt;- data.frame(subset(stim_high.df, select = 3:(ncol(stim_high.df) - 1))) high.pca &lt;- prcomp(meanhighdf) high.pca_scores &lt;- as.data.frame(high.pca$x) meanlowdf &lt;- data.frame(subset(stim_low.df, select = 3:(ncol(stim_low.df) - 1))) low.pca &lt;- prcomp(meanlowdf) low.pca_scores &lt;- as.data.frame(low.pca$x) library(plotly) # You can use plotly to create an interactive 3D plot # plot_ly(high.pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) # plot_ly(low.pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) combined_pca_scores &lt;- rbind(high.pca_scores, low.pca_scores) # Add a new column to indicate the stim_ordered category (high_stim or low_stim) combined_pca_scores$stim_ordered &lt;- c(rep(&quot;high_stim&quot;, nrow(high.pca_scores)), rep(&quot;low_stim&quot;, nrow(low.pca_scores))) # Create the 3D PCA plot plot_ly(combined_pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = ~stim_ordered) ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels Code # data_matrix &lt;- groupwise[groupwise$stim_ordered == &quot;high_stim&quot;,c(&quot;tr_ordered&quot;, &quot;mean_per_sub_norm_mean&quot;)] # sorted_indices &lt;- order(data_matrix$tr_ordered) # df_ordered &lt;- data_matrix[sorted_indices, ] # pca_result &lt;- PCA(data_matrix$mean_per_sub_norm_mean) # datapoints &lt;- df$datapoints # Assuming you have a dataframe named &#39;data&#39; containing the 20 data points, &#39;x&#39; and &#39;y&#39; values, and corresponding standard deviations &#39;sd&#39; # Load the ggplot2 library # install.packages(&quot;ggplot2&quot;) library(ggplot2) # Create the plot # y = &quot;mean_per_sub_mean&quot;z # combined_pca &lt;- combined_pca_scores %&gt;% # mutate(group_index = group_indices(., stim_ordered)) combined_pca &lt;- combined_pca_scores %&gt;% group_by(stim_ordered) %&gt;% mutate(group_index = row_number()) ggplot(combined_pca, aes(x=group_index,y=PC1, group = stim_ordered, colour=stim_ordered)) + stat_smooth(method=&quot;loess&quot;, span=0.25, se=TRUE, aes(color=stim_ordered), alpha=0.3) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Code # Assuming you have a dataframe named &#39;data&#39; containing the 20 data points, &#39;x&#39; and &#39;y&#39; values, and corresponding standard deviations &#39;sd&#39; # Load the ggplot2 library # install.packages(&quot;ggplot2&quot;) library(ggplot2) # Create the plot # y = &quot;mean_per_sub_mean&quot;z ggplot(groupwise, aes(x=tr_ordered,y=mean_per_sub_mean, group = stim_ordered, colour=stim_ordered)) + stat_smooth(method=&quot;loess&quot;, span=0.25, se=TRUE, aes(color=stim_ordered), alpha=0.3) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Code # ggplot(data=groupwise, aes(x=tr_ordered, y=mean_per_sub_mean, ymin=se, ymax=se, fill=stim_ordered, linetype=stim_ordered)) + # geom_line() + # geom_ribbon(alpha=0.5) # Assuming you have a dataframe named &#39;data&#39; containing the 20 mean data points and corresponding standard errors # &#39;x&#39; represents the x-values (e.g., time points) # &#39;mean_y&#39; represents the mean y-values # &#39;se_y&#39; represents the standard errors of the mean y-values # Load the ggplot2 library # install.packages(&quot;ggplot2&quot;) library(ggplot2) # groupwise$x &lt;- as.numeric(groupwise$x) # # # Sort the dataframe by the &#39;x&#39; variable (if it&#39;s not already sorted) # data &lt;- data[order(data$x), ] # Create the plot # Create the plot with custom span and smoothing method ggplot(groupwise, aes(x=tr_ordered,y=mean_per_sub_mean)) + geom_line() + # Plot the smooth line for the mean geom_ribbon(aes(ymin = mean_per_sub_mean - se, ymax = mean_per_sub_mean + se), alpha = 0.3) + # Add the ribbon for standard error geom_smooth(method = &quot;loess&quot;, span = 0.1, se = FALSE) + # Add the loess smoothing curve labs(x = &quot;X-axis Label&quot;, y = &quot;Y-axis Label&quot;, title = &quot;Smooth Line with Standard Error Ribbon&quot;) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 14.1 DEP: epoch: stim, high cue vs low cue Code # filtered_df &lt;- subset(df, condition != &quot;rating&quot;) filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot;), ] parsed_df &lt;- filtered_df %&gt;% separate(condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE) TR_length &lt;- 42 # --------------------- subset regions based on ROI ---------------------------- df_long &lt;- pivot_longer(parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot;) # ----------------------------- clean factor ----------------------------------- df_long$tr_ordered &lt;- factor( df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length)) ) df_long$cue_ordered &lt;- factor( df_long$cue, levels = c(&quot;cueH&quot;,&quot;cueL&quot;) ) # --------------------------- summary statistics ------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;, &quot;tr_ordered&quot;, &quot;cue_ordered&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;cue_ordered&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- taskname # https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 # --------------------------------- plot --------------------------------------- LINEIV1 = &quot;tr_ordered&quot; LINEIV2 = &quot;cue_ordered&quot; MEAN = &quot;mean_per_sub_norm_mean&quot; ERROR = &quot;se&quot; dv_keyword = &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices, ] p1 = plot_timeseries_bar(groupwise, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;Runs&quot; , ylab= &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle=&quot;time_series&quot;, color=c(&quot;red&quot;, &quot;blue&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) p1 + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:7+TR_length]))+ theme_classic() 14.2 taskwise cue effect Code roi_list &lt;- c(&#39;rINS&#39;, &#39;TPJ&#39;, &#39;dACC&#39;, &#39;PHG&#39;, &#39;V1&#39;, &#39;SM&#39;, &#39;MT&#39;, &#39;RSC&#39;, &#39;LOC&#39;, &#39;FFC&#39;, &#39;PIT&#39;, &#39;pSTS&#39;, &#39;AIP&#39;, &#39;premotor&#39;) for (ROI in roi_list) { datadir = file.path(main_dir, &quot;analysis/fmri/spm/fir/ttl2par&quot;) # taskname = &#39;pain&#39; exclude &lt;- &quot;sub-0001&quot; filename &lt;- paste0(&quot;sub-*&quot;, &quot;*roi-&quot;, ROI, &quot;_tr-42.csv&quot;) common_path &lt;- Sys.glob(file.path(datadir, &quot;sub-*&quot;, filename )) filter_path &lt;- common_path[!str_detect(common_path, pattern = exclude)] df &lt;- do.call(&quot;rbind.fill&quot;, lapply(filter_path, FUN = function(files) { read.table(files, header = TRUE, sep = &quot;,&quot;) })) run_types &lt;- c(&quot;pain&quot;, &quot;vicarious&quot;, &quot;cognitive&quot;) plot_list &lt;- list() TR_length &lt;- 42 for (run_type in run_types) { filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type), ] parsed_df &lt;- filtered_df %&gt;% separate(condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE) # --------------------- subset regions based on ROI ---------------------------- df_long &lt;- pivot_longer(parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot;) # ----------------------------- clean factor ----------------------------------- df_long$tr_ordered &lt;- factor( df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length)) ) df_long$cue_ordered &lt;- factor( df_long$cue, levels = c(&quot;cueH&quot;,&quot;cueL&quot;) ) # --------------------------- summary statistics ------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;,&quot;tr_ordered&quot;, &quot;cue_ordered&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c( &quot;cue_ordered&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- run_type # https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 # ... Rest of your data processing code ... # subset &lt;- groupwise[groupwise$runtype == run_type, ] LINEIV1 = &quot;tr_ordered&quot; LINEIV2 = &quot;cue_ordered&quot; MEAN = &quot;mean_per_sub_norm_mean&quot; ERROR = &quot;se&quot; dv_keyword = &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices, ] p1 &lt;- plot_timeseries_bar(groupwise_sorted, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;TRs&quot;, ylab = paste0(ROI, &quot; activation (A.U.)&quot;), ggtitle = paste0(run_type, &quot; time series, Epoch - stimulus&quot;), color =c(&quot;red&quot;, &quot;blue&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) #p1 &lt;- p1 + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:(7 + TR_length)])) + theme_classic() plot_list[[run_type]] &lt;- p1 + theme_classic() } # --------------------------- plot three tasks ------------------------------- library(gridExtra) plot_list &lt;- lapply(plot_list, function(plot) { plot + theme(plot.margin = margin(5, 5, 5, 5)) # Adjust plot margins if needed }) combined_plot &lt;- ggpubr::ggarrange(plot_list[[&quot;pain&quot;]],plot_list[[&quot;vicarious&quot;]],plot_list[[&quot;cognitive&quot;]], common.legend = TRUE,legend = &quot;bottom&quot;, ncol = 3, nrow = 1, widths = c(3, 3, 3), heights = c(.5,.5,.5), align = &quot;v&quot;) combined_plot ggsave(file.path(save_dir, paste0(&quot;roi-&quot;, ROI, &quot;_epoch-stim_desc-highcueGTlowcue.png&quot;)), combined_plot, width = 12, height = 4) } 14.3 epoch: stim, rating 14.4 epoch: 6 cond Code # ------------------------------------------------------------------------------ # epoch stim, high cue vs low cue # ------------------------------------------------------------------------------ # --------------------- subset regions based on ROI ---------------------------- # ----------------------------- clean factor ----------------------------------- df_long$tr_ordered &lt;- factor( df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length)) ) df_long$cue_ordered &lt;- factor( df_long$cue, levels = c(&quot;cueH&quot;, &quot;cueL&quot;) ) df_long$stim_ordered &lt;- factor( df_long$stim, levels = c(&quot;stimH&quot;, &quot;stimM&quot;, &quot;stimL&quot;) ) df_long$sixcond &lt;- factor( df_long$condition, levels = c(&quot;cueH_stimH&quot;, &quot;cueL_stimH&quot;, &quot;cueH_stimM&quot;, &quot;cueL_stimM&quot;, &quot;cueH_stimL&quot;, &quot;cueL_stimL&quot;) ) # --------------------------- summary statistics ------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;, &quot;tr_ordered&quot;, &quot;sixcond&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;sixcond&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- taskname # https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 # --------------------------------- plot --------------------------------------- LINEIV1 = &quot;tr_ordered&quot; LINEIV2 = &quot;sixcond&quot; MEAN = &quot;mean_per_sub_norm_mean&quot; ERROR = &quot;se&quot; dv_keyword = &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices, ] p3H = plot_timeseries_bar(groupwise, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;Runs&quot; , ylab= &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle=paste0(&quot;High intensity - Low cue vs. High cue (N = &quot;, unique(groupwise$N), &quot;)&quot; ), color=c(&quot;red&quot;,&quot;#5f0f40&quot;,&quot;gray&quot;, &quot;gray&quot;, &quot;gray&quot;, &quot;gray&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) p3H + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:7+TR_length]))+ theme_classic() Code p3H + theme_classic() Code p3M = plot_timeseries_bar(groupwise, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;Runs&quot; , ylab= &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle=paste0(&quot;Medium intensity - Low cue vs. High cue (N = &quot;, unique(groupwise$N), &quot;)&quot;), color=c(&quot;#d6d6d6&quot;,&quot;#d6d6d6&quot;,&quot;#bc3908&quot;, &quot;#f6aa1c&quot;, &quot;gray&quot;, &quot;gray&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) p3M + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:7+TR_length]))+ theme_classic() Code p3M + theme_classic() Code p3L = plot_timeseries_bar(groupwise, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;Runs&quot; , ylab= &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle=paste0(&quot;Low intensity - Low cue vs. High cue (N = &quot;, unique(groupwise$N), &quot;)&quot;), color=c(&quot;gray&quot;,&quot;gray&quot;,&quot;gray&quot;, &quot;gray&quot;, &quot;#2541b2&quot;, &quot;#00a6fb&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) p3L + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:7+TR_length]))+ theme_classic() Code p3L + theme_classic() "],["taskwise-6-cond-effect.html", "Chapter 15 taskwise 6 cond effect", " Chapter 15 taskwise 6 cond effect Code # ------------------------------------------------------------------------------ # epoch stim, high cue vs low cue # ------------------------------------------------------------------------------ # --------------------- subset regions based on ROI ---------------------------- run_types &lt;- c(&quot;pain&quot;, &quot;vicarious&quot;, &quot;cognitive&quot;) TR_length &lt;- 42 for (run_type in run_types) { filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type), ] plot_list &lt;- list() parsed_df &lt;- filtered_df %&gt;% separate(condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE) # --------------------- subset regions based on ROI ---------------------------- df_long &lt;- pivot_longer(parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot;) # ----------------------------- clean factor ----------------------------------- df_long$tr_ordered &lt;- factor( df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length)) ) df_long$cue_ordered &lt;- factor( df_long$cue, levels = c(&quot;cueH&quot;, &quot;cueL&quot;) ) df_long$stim_ordered &lt;- factor( df_long$stim, levels = c(&quot;stimH&quot;, &quot;stimM&quot;, &quot;stimL&quot;) ) df_long$sixcond &lt;- factor( df_long$condition, levels = c(&quot;cueH_stimH&quot;, &quot;cueL_stimH&quot;, &quot;cueH_stimM&quot;, &quot;cueL_stimM&quot;, &quot;cueH_stimL&quot;, &quot;cueL_stimL&quot;) ) # --------------------------- summary statistics ------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;, &quot;tr_ordered&quot;, &quot;sixcond&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;sixcond&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- taskname # https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 # --------------------------------- plot --------------------------------------- LINEIV1 = &quot;tr_ordered&quot; LINEIV2 = &quot;sixcond&quot; MEAN = &quot;mean_per_sub_norm_mean&quot; ERROR = &quot;se&quot; dv_keyword = &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices, ] p3H = plot_timeseries_bar(groupwise, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;Runs&quot; , ylab= &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle=paste0(&quot;High intensity - Low cue vs. High cue (N = &quot;, unique(groupwise$N), &quot;)&quot; ), color=c(&quot;red&quot;,&quot;#5f0f40&quot;,&quot;gray&quot;, &quot;gray&quot;, &quot;gray&quot;, &quot;gray&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) # p3H + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:7+TR_length]))+ theme_classic() p3H + theme_classic() plot_list[[&quot;H&quot;]] &lt;- p3H + theme_classic() p3M = plot_timeseries_bar(groupwise, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;Runs&quot; , ylab= &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle=paste0(&quot;Medium intensity - Low cue vs. High cue (N = &quot;, unique(groupwise$N), &quot;)&quot;), color=c(&quot;#d6d6d6&quot;,&quot;#d6d6d6&quot;,&quot;#bc3908&quot;, &quot;#f6aa1c&quot;, &quot;gray&quot;, &quot;gray&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) # p3M + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:7+TR_length]))+ theme_classic() plot_list[[&quot;M&quot;]] &lt;- p3M + theme_classic() p3L = plot_timeseries_bar(groupwise, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;Runs&quot; , ylab= &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle=paste0(&quot;Low intensity - Low cue vs. High cue (N = &quot;, unique(groupwise$N), &quot;)&quot;), color=c(&quot;gray&quot;,&quot;gray&quot;,&quot;gray&quot;, &quot;gray&quot;, &quot;#2541b2&quot;, &quot;#00a6fb&quot;)) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) # p3L + scale_x_discrete(labels = setNames(time_points, colnames(df_long)[7:7+TR_length]))+ theme_classic() plot_list[[&quot;L&quot;]] &lt;- p3L + theme_classic() # --------------------------- plot three tasks ------------------------------- library(gridExtra) plot_list &lt;- lapply(plot_list, function(plot) { plot + theme(plot.margin = margin(5, 5, 5, 5)) # Adjust plot margins if needed }) combined_plot &lt;- ggpubr::ggarrange(plot_list[[&quot;H&quot;]],plot_list[[&quot;M&quot;]],plot_list[[&quot;L&quot;]], common.legend = FALSE,legend = &quot;bottom&quot;, ncol = 3, nrow = 1, widths = c(3, 3, 3), heights = c(.5,.5,.5), align = &quot;v&quot;) combined_plot ggsave(file.path(save_dir, paste0(&quot;taskwise-&quot;,run_type, &quot;_epoch-stim_desc-stimcuecomparison.png&quot;)), combined_plot, width = 12, height = 4) } "],["ch52_timeseries.html", "Chapter 16 [fMRI] FIR ~ task TTL1 16.1 references 16.2 taskwise stim effect 16.3 taskwise cue effect 16.4 MAIN SANDBOX: 6 condition in three panels. per task. per ROI", " Chapter 16 [fMRI] FIR ~ task TTL1 The purpose of this notebook is to plot the BOLD timeseries from SPM FIR model. TODO * load tsv * concatenate * per time column, calculate mean and variance * plot 16.1 references https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 Code plot_timeseries_onefactor &lt;- function(df, iv1, mean, error, xlab, ylab, ggtitle, color) { n_points &lt;- 100 # Number of points for interpolation g &lt;- ggplot( data = df, aes( x = .data[[iv1]], y = .data[[mean]], group = 1, color = color ), cex.lab = 1.5, cex.axis = 2, cex.main = 1.5, cex.sub = 1.5 ) + geom_errorbar(aes( ymin = (.data[[mean]] - .data[[error]]), ymax = (.data[[mean]] + .data[[error]]), color = color ), width = .1, alpha = 0.8) + geom_line() + geom_point(color = color) + ggtitle(ggtitle) + xlab(xlab) + ylab(ylab) + theme_classic() + theme(aspect.ratio = .6) + expand_limits(x = 3.25) + scale_color_manual(&quot;&quot;, values = color) + # theme( # legend.position = c(.99, .99), # legend.justification = c(&quot;right&quot;, &quot;top&quot;), # legend.box.just = &quot;right&quot;, # legend.margin = margin(6, 6, 6, 6) # ) + # theme(legend.key = element_rect(fill = &quot;white&quot;, colour = &quot;white&quot;)) + theme_bw() return(g) } Code plot_timeseries_bar_SANDBOX &lt;- function(df, iv1, iv2, mean, error, xlab, ylab, ggtitle, color) { n_points &lt;- 100 # Number of points for interpolation ## Removing &quot;tr&quot; from the column values df[[iv1]] &lt;- as.numeric(sub(&quot;tr&quot;, &quot;&quot;, df[[iv1]])) g &lt;- ggplot( data = df, aes( x = .data[[iv1]], y = .data[[mean]], group = factor(.data[[iv2]]), color = factor(.data[[iv2]]) ), cex.lab = 1.5, cex.axis = 2, cex.main = 1.5, cex.sub = 1.5 ) + geom_errorbar(aes( ymin = (.data[[mean]] - .data[[error]]), ymax = (.data[[mean]] + .data[[error]]), fill = factor(.data[[iv2]]) ), width = .1, alpha = 0.8) + geom_line() + geom_point() + ggtitle(ggtitle) + xlab(xlab) + ylab(ylab) + theme_classic() + expand_limits(x = 3.25) + scale_color_manual(&quot;&quot;, values = color) + scale_fill_manual(&quot;&quot;, values = color) + theme( aspect.ratio = .6, text = element_text(size = 20), axis.title.x = element_text(size = 24), axis.title.y = element_text(size = 24), legend.position = c(.99, .99), legend.justification = c(&quot;right&quot;, &quot;top&quot;), legend.box.just = &quot;right&quot;, legend.margin = margin(6, 6, 6, 6) ) + theme(legend.key = element_rect(fill = &quot;white&quot;, colour = &quot;white&quot;)) + theme_bw() return(g) } parameters Code main_dir &lt;- dirname(dirname(getwd())) datadir &lt;- file.path(main_dir, &quot;analysis/fmri/nilearn/glm/fir&quot;) analysis_folder &lt;- paste0(&quot;model52_iv-6cond_dv-firglasserSPM_ttl1&quot;) analysis_dir &lt;- file.path(main_dir, &quot;analysis&quot;, &quot;mixedeffect&quot;, analysis_folder, as.character(Sys.Date())) dir.create(analysis_dir, showWarnings = FALSE, recursive = TRUE) save_dir &lt;- analysis_dir 16.2 taskwise stim effect Here, I have a list of ROIs. Per ROI, I have FIR values for pain, vicarious, cognitive tasks. We’ll aggregate data per ROI and plot the time series for the 3 tasks. Code roi_list &lt;- c(&quot;dACC&quot;, &quot;PHG&quot;, &quot;V1&quot;, &quot;SM&quot;, &quot;MT&quot;, &quot;RSC&quot;, &quot;LOC&quot;, &quot;FFC&quot;, &quot;PIT&quot;, &quot;pSTS&quot;, &quot;AIP&quot;, &quot;premotor&quot;) # &#39;rINS&#39;, &#39;TPJ&#39;, run_types &lt;- c(&quot;pain&quot;, &quot;vicarious&quot;, &quot;cognitive&quot;) plot_list &lt;- list() TR_length &lt;- 42 for (ROI in roi_list) { main_dir &lt;- dirname(dirname(getwd())) datadir &lt;- file.path(main_dir, &quot;analysis/fmri/spm/fir/ttl1par&quot;) taskname &lt;- &quot;pain&quot; exclude &lt;- &quot;sub-0001&quot; filename &lt;- paste0(&quot;sub-*&quot;, &quot;*roi-&quot;, ROI, &quot;_tr-42.csv&quot;) common_path &lt;- Sys.glob(file.path(datadir, &quot;sub-*&quot;, filename)) filter_path &lt;- common_path[!str_detect(common_path, pattern = exclude)] df &lt;- do.call(&quot;rbind.fill&quot;, lapply( filter_path, FUN = function(files) { read.table(files, header = TRUE, sep = &quot;,&quot;) } )) for (run_type in run_types) { print(run_type) filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type),] parsed_df &lt;- filtered_df %&gt;% separate( condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE ) # -------------------------------------------------------------------------- # 0) subset dataframe based on ROI # -------------------------------------------------------------------------- df_long &lt;- pivot_longer( parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot; ) # -------------------------------------------------------------------------- # 1) clean factor # -------------------------------------------------------------------------- df_long$tr_ordered &lt;- factor(df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length))) df_long$stim_ordered &lt;- factor(df_long$stim, levels = c(&quot;stimH&quot;, &quot;stimM&quot;, &quot;stimL&quot;)) # -------------------------------------------------------------------------- # 2) summary statistics # -------------------------------------------------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;, &quot;tr_ordered&quot;, &quot;stim_ordered&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;stim_ordered&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- run_type # https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 LINEIV1 &lt;- &quot;tr_ordered&quot; LINEIV2 &lt;- &quot;stim_ordered&quot; MEAN &lt;- &quot;mean_per_sub_norm_mean&quot; ERROR &lt;- &quot;se&quot; dv_keyword &lt;- &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices,] # -------------------------------------------------------------------------- # 3) plot per run # -------------------------------------------------------------------------- p1 &lt;- plot_timeseries_bar_SANDBOX( groupwise_sorted, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;TRs&quot;, ylab = paste0(ROI, &quot; activation (A.U.)&quot;), ggtitle = paste0( ROI, &quot;: &quot;, run_type, &quot; (N = &quot;, length(unique(subjectwise$sub)), &quot;) time series, Epoch - stimulus&quot; ), color = c(&quot;#5f0f40&quot;, &quot;#ae2012&quot;, &quot;#fcbf49&quot;) ) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) p1 &lt;- p1 + annotate( &quot;rect&quot;, xmin = 0, xmax = 20, ymin = min(df[[MEAN]], na.rm = TRUE) - 5, ymax = max(df[[MEAN]], na.rm = TRUE) + 5, fill = &quot;grey&quot;, alpha = 0.2 ) plot_list[[run_type]] &lt;- p1 + theme_classic() } # -------------------------------------------------------------------------- # 4) plot three tasks per ROI # -------------------------------------------------------------------------- library(gridExtra) plot_list &lt;- lapply(plot_list, function(plot) { plot + theme(plot.margin = margin(5, 5, 5, 5)) # Adjust plot margins if needed }) combined_plot &lt;- ggpubr::ggarrange( plot_list[[&quot;pain&quot;]], plot_list[[&quot;vicarious&quot;]], plot_list[[&quot;cognitive&quot;]], common.legend = TRUE, legend = &quot;bottom&quot;, ncol = 3, nrow = 1, widths = c(3, 3, 3), heights = c(.5, .5, .5), align = &quot;v&quot; ) print(combined_plot) ggsave(file.path( save_dir, paste0(&quot;roi-&quot;, ROI, &quot;_epoch-stim_desc-highstimGTlowstim.png&quot;) ), combined_plot, width = 12, height = 4) } ## [1] &quot;pain&quot; ## ## Attaching package: &#39;raincloudplots&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## GeomFlatViolin ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;pain&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;vicarious&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf ## [1] &quot;cognitive&quot; ## Warning in geom_errorbar(aes(ymin = (.data[[mean]] - .data[[error]]), ymax = ## (.data[[mean]] + : Ignoring unknown aesthetics: fill ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf Code p1 + annotate(&quot;rect&quot;, xmin = 0, xmax = 10, ymin = min(df[[MEAN]], na.rm = TRUE) - 5, ymax = max(df[[MEAN]], na.rm = TRUE) + 5, fill = &quot;grey&quot;, alpha = 0.2) ## Warning in min(df[[MEAN]], na.rm = TRUE): no non-missing arguments to min; ## returning Inf ## Warning in max(df[[MEAN]], na.rm = TRUE): no non-missing arguments to max; ## returning -Inf 16.2.1 PCA subjectwise Code run_types &lt;- c(&quot;pain&quot;) for (run_type in run_types) { print(run_type) filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type),] parsed_df &lt;- filtered_df %&gt;% separate( condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE ) # ------------------------------------------------------------------------------ # subset regions based on ROI # ------------------------------------------------------------------------------ df_long &lt;- pivot_longer( parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot; ) # ------------------------------------------------------------------------------ # clean factor # ------------------------------------------------------------------------------ df_long$tr_ordered &lt;- factor(df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length))) df_long$stim_ordered &lt;- factor(df_long$stim, levels = c(&quot;stimH&quot;, &quot;stimM&quot;, &quot;stimL&quot;)) # ------------------------------------------------------------------------------ # summary stats # ------------------------------------------------------------------------------ subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;, &quot;tr_ordered&quot;, &quot;stim_ordered&quot;), &quot;tr_value&quot;) # ------------------------------------------------------------------------------ # convert dataframe long to wide # ------------------------------------------------------------------------------ df_wide &lt;- pivot_wider( subjectwise, id_cols = c(&quot;tr_ordered&quot;, &quot;stim_ordered&quot;), names_from = c(&quot;sub&quot;), values_from = &quot;mean_per_sub&quot; ) stim_high.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimH&quot;,] stim_med.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimM&quot;,] stim_low.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimL&quot;,] meanhighdf &lt;- data.frame(subset(stim_high.df, select = 3:(ncol(stim_high.df) - 1))) high.pca_result &lt;- prcomp(meanhighdf) high.pca_scores &lt;- as.data.frame(high.pca_result$x) # Access the proportion of variance explained by each principal component high.variance_explained &lt;- high.pca_result$sdev ^ 2 / sum(high.pca_result$sdev ^ 2) plot(high.variance_explained) # Access the standard deviations of each principal component high.stdev &lt;- high.pca_result$sdev meanmeddf &lt;- data.frame(subset(stim_med.df, select = 3:(ncol(stim_med.df) - 1))) med.pca &lt;- prcomp(meanmeddf) med.pca_scores &lt;- as.data.frame(med.pca$x) meanlowdf &lt;- data.frame(subset(stim_low.df, select = 3:(ncol(stim_low.df) - 1))) low.pca &lt;- prcomp(meanlowdf) low.pca_scores &lt;- as.data.frame(low.pca$x) combined_pca_scores &lt;- rbind(high.pca_scores, med.pca_scores, low.pca_scores) # Add a new column to indicate the stim_ordered category (high_stim or low_stim) combined_pca_scores$stim_ordered &lt;- c(rep(&quot;high_stim&quot;, nrow(high.pca_scores)), rep(&quot;med_stim&quot;, nrow(med.pca_scores)), rep(&quot;low_stim&quot;, nrow(low.pca_scores))) # ------------------------------------------------------------------------------ # 3d PCA plot # ------------------------------------------------------------------------------ plot_ly( combined_pca_scores, x = ~ PC1, y = ~ PC2, z = ~ PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = ~ stim_ordered ) } ## [1] &quot;pain&quot; Code plot_ly( combined_pca_scores, x = ~ PC1, y = ~ PC2, z = ~ PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = ~ stim_ordered ) 16.2.2 PCA groupwise Code # ------------------------------------------------------------------------------ # data formatting # ------------------------------------------------------------------------------ # Convert the dataframe to wide format df_wide.group &lt;- pivot_wider( subjectwise, id_cols = c(&quot;tr_ordered&quot;, &quot;stim_ordered&quot;), names_from = &quot;sub&quot;, values_from = &quot;mean_per_sub&quot; ) # Split the data into two subsets based on the &#39;stim_ordered&#39; value # One for &#39;stimH&#39; and another for &#39;stimL&#39; stim_high.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimH&quot;,] stim_low.df &lt;- df_wide[df_wide$stim_ordered == &quot;stimL&quot;,] # Prepare data for PCA analysis by selecting relevant columns # Exclude the first two columns and the last column meanhighdf &lt;- data.frame(subset(stim_high.df, select = 3:(ncol(stim_high.df) - 1))) meanlowdf &lt;- data.frame(subset(stim_low.df, select = 3:(ncol(stim_low.df) - 1))) # ------------------------------------------------------------------------------ # Principal Component Analysis (PCA) # ------------------------------------------------------------------------------ high.pca &lt;- prcomp(meanhighdf) # Perform Principal Component Analysis (PCA) high.pca_scores &lt;- as.data.frame(high.pca$x) # Extract PCA scores # Repeat the process for the low stimulus data low.pca &lt;- prcomp(meanlowdf) low.pca_scores &lt;- as.data.frame(low.pca$x) combined_pca_scores &lt;- rbind(high.pca_scores, low.pca_scores) # Add a new column to indicate the &#39;stim_ordered&#39; category (high_stim or low_stim) # This helps in distinguishing the groups in the plot combined_pca_scores$stim_ordered &lt;- c(rep(&quot;high_stim&quot;, nrow(high.pca_scores)), rep(&quot;low_stim&quot;, nrow(low.pca_scores))) # ------------------------------------------------------------------------------ # plot 3D scatter plot of the PCA scores # ------------------------------------------------------------------------------ # The points are colored based on their stim_ordered category plot_ly( combined_pca_scores, x = ~ PC1, y = ~ PC2, z = ~ PC3, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = ~ stim_ordered ) ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels Code # ------------------------------------------------------------------------------ # plot 2D group plot # ------------------------------------------------------------------------------ # Create a 2D plot with smoothed lines for each stim_ordered group combined_pca &lt;- combined_pca_scores %&gt;% group_by(stim_ordered) %&gt;% mutate(group_index = row_number()) ggplot(combined_pca, aes( x = group_index, y = PC1, group = stim_ordered, colour = stim_ordered )) + stat_smooth( method = &quot;loess&quot;, span = 0.25, se = TRUE, aes(color = stim_ordered), alpha = 0.3 ) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Code # Create the plot ggplot( groupwise, aes( x = tr_ordered, y = mean_per_sub_mean, group = stim_ordered, colour = stim_ordered ) ) + stat_smooth( method = &quot;loess&quot;, span = 0.25, se = TRUE, aes(color = stim_ordered), alpha = 0.3 ) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Code # Create the plot # Create the plot with custom span and smoothing method ggplot(groupwise, aes(x = tr_ordered, y = mean_per_sub_mean)) + geom_line() + # Plot the smooth line for the mean geom_ribbon(aes(ymin = mean_per_sub_mean - se, ymax = mean_per_sub_mean + se), alpha = 0.3) + # Add the ribbon for standard error geom_smooth(method = &quot;loess&quot;, span = 0.1, se = FALSE) + # Add the loess smoothing curve labs(x = &quot;X-axis Label&quot;, y = &quot;Y-axis Label&quot;, title = &quot;Smooth Line with Standard Error Ribbon&quot;) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 16.3 taskwise cue effect Code roi_list &lt;- c(&quot;dACC&quot;, &quot;PHG&quot;, &quot;V1&quot;, &quot;SM&quot;, &quot;MT&quot;, &quot;RSC&quot;, &quot;LOC&quot;, &quot;FFC&quot;, &quot;PIT&quot;, &quot;pSTS&quot;, &quot;AIP&quot;, &quot;premotor&quot;) # &#39;rINS&#39;, &#39;TPJ&#39;, for (ROI in roi_list) { datadir &lt;- file.path(main_dir, &quot;analysis/fmri/spm/fir/ttl2par&quot;) # taskname = &#39;pain&#39; exclude &lt;- &quot;sub-0001&quot; filename &lt;- paste0(&quot;sub-*&quot;, &quot;*roi-&quot;, ROI, &quot;_tr-42.csv&quot;) common_path &lt;- Sys.glob(file.path(datadir, &quot;sub-*&quot;, filename)) filter_path &lt;- common_path[!str_detect(common_path, pattern = exclude)] df &lt;- do.call(&quot;rbind.fill&quot;, lapply( filter_path, FUN = function(files) { read.table(files, header = TRUE, sep = &quot;,&quot;) } )) run_types &lt;- c(&quot;pain&quot;, &quot;vicarious&quot;, &quot;cognitive&quot;) plot_list &lt;- list() TR_length &lt;- 42 for (run_type in run_types) { filtered_df &lt;- df[!(df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type),] parsed_df &lt;- filtered_df %&gt;% separate( condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE ) # -------------------------------------------------------------------------- # subset regions based on ROI # -------------------------------------------------------------------------- df_long &lt;- pivot_longer( parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot; ) # -------------------------------------------------------------------------- # clean factor # -------------------------------------------------------------------------- df_long$tr_ordered &lt;- factor(df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length))) df_long$cue_ordered &lt;- factor(df_long$cue, levels = c(&quot;cueH&quot;, &quot;cueL&quot;)) # -------------------------------------------------------------------------- # summary statistics # -------------------------------------------------------------------------- subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;, &quot;tr_ordered&quot;, &quot;cue_ordered&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;cue_ordered&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- run_type # https://stackoverflow.com/questions/29402528/append-data-frames-together-in-a-for-loop/29419402 LINEIV1 &lt;- &quot;tr_ordered&quot; LINEIV2 &lt;- &quot;cue_ordered&quot; MEAN &lt;- &quot;mean_per_sub_norm_mean&quot; ERROR &lt;- &quot;se&quot; dv_keyword &lt;- &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices,] p1 &lt;- plot_timeseries_bar( groupwise_sorted, LINEIV1, LINEIV2, MEAN, ERROR, xlab = &quot;TRs&quot;, ylab = paste0(ROI, &quot; activation (A.U.)&quot;), ggtitle = paste0(run_type, &quot; time series, Epoch - stimulus&quot;), color = c(&quot;red&quot;, &quot;blue&quot;) ) time_points &lt;- seq(1, 0.46 * TR_length, 0.46) plot_list[[run_type]] &lt;- p1 + theme_classic() } # ---------------------------------------------------------------------------- # plot three tasks # ---------------------------------------------------------------------------- library(gridExtra) plot_list &lt;- lapply(plot_list, function(plot) { plot + theme(plot.margin = margin(5, 5, 5, 5)) # Adjust plot margins if needed }) combined_plot &lt;- ggpubr::ggarrange( plot_list[[&quot;pain&quot;]], plot_list[[&quot;vicarious&quot;]], plot_list[[&quot;cognitive&quot;]], common.legend = TRUE, legend = &quot;bottom&quot;, ncol = 3, nrow = 1, widths = c(3, 3, 3), heights = c(.5, .5, .5), align = &quot;v&quot; ) print(combined_plot) ggsave(file.path( save_dir, paste0(&quot;roi-&quot;, ROI, &quot;_epoch-cue_desc-highcueGTlowcue.png&quot;) ), combined_plot, width = 12, height = 4) } 16.3.1 epoch: stim, rating 16.4 MAIN SANDBOX: 6 condition in three panels. per task. per ROI Code # A function to plot data plot_data &lt;- function(groupwise, iv1, iv2, mean, error, xlab, ylab, ggtitle, run_type, colors) { p &lt;- plot_timeseries_bar( groupwise, &quot;tr_ordered&quot;, &quot;sixcond&quot;, &quot;mean_per_sub_norm_mean&quot;, &quot;se&quot;, xlab = &quot;TRs&quot;, ylab = &quot;Epoch: stimulus, High cue vs. Low cue&quot;, ggtitle = paste0(run_type, &quot; intensity (N = &quot;, unique(groupwise$N), &quot;)&quot;), color_mapping = colors, show_legend = FALSE ) p + theme_classic() } Code #&#39; Calculate Point Size Proportionally #&#39; #&#39; This function calculates point size proportionally based on a base point size and figure dimensions #&#39; (width and height). It can be used to adjust the point size in plots to maintain proportionality #&#39; with varying figure sizes. #&#39; #&#39; @param point_size_base The base point size for `geom_points`. #&#39; @param figure_width The width of the figure in which the point size needs to be adjusted. #&#39; @param figure_height The height of the figure in which the point size needs to be adjusted. #&#39; #&#39; @return The calculated point size. #&#39; #&#39; @examples #&#39; # Define your point size base #&#39; point_size_base &lt;- 3 #&#39; #&#39; # Define your figure dimensions (width and height) #&#39; figure_width &lt;- 12 #&#39; figure_height &lt;- 8 #&#39; #&#39; # Calculate the point size using the function #&#39; POINT_SIZE &lt;- calculate_point_size(point_size_base, figure_width, figure_height) #&#39; #&#39; # Apply the point size to your plot elements #&#39; plot + geom_point(size = POINT_SIZE) #&#39; #&#39; @export calculate_point_size &lt;- function(figure_width, figure_height, point_size_base = 5) { scaling_factor &lt;- min(figure_width, figure_height) / point_size_base return(scaling_factor) } Code # ------------------------------------------------------------------------------ # epoch stim, high cue vs low cue # ------------------------------------------------------------------------------ run_types &lt;- c(&quot;pain&quot;, &quot;vicarious&quot;, &quot;cognitive&quot;) all_plots &lt;- list() TR_length &lt;- 42 for (roi in c(&quot;dACC&quot;, &quot;PHG&quot;)) { plot_list_per_roi &lt;- list() for (run_type in run_types) { filtered_df &lt;- df[!( df$condition == &quot;rating&quot; | df$condition == &quot;cue&quot; | df$runtype != run_type | df$ROI == roi ),] plot_list &lt;- list() parsed_df &lt;- filtered_df %&gt;% separate( condition, into = c(&quot;cue&quot;, &quot;stim&quot;), sep = &quot;_&quot;, remove = FALSE ) # --------------------- subset regions based on ROI ---------------------------- df_long &lt;- pivot_longer( parsed_df, cols = starts_with(&quot;tr&quot;), names_to = &quot;tr_num&quot;, values_to = &quot;tr_value&quot; ) # ---------------------------------------------------------------------------- # clean factor # ---------------------------------------------------------------------------- df_long$tr_ordered &lt;- factor(df_long$tr_num, levels = c(paste0(&quot;tr&quot;, 1:TR_length))) df_long$cue_ordered &lt;- factor(df_long$cue, levels = c(&quot;cueH&quot;, &quot;cueL&quot;)) df_long$stim_ordered &lt;- factor(df_long$stim, levels = c(&quot;stimH&quot;, &quot;stimM&quot;, &quot;stimL&quot;)) df_long$sixcond &lt;- factor( df_long$condition, levels = c( &quot;cueH_stimH&quot;, &quot;cueL_stimH&quot;, &quot;cueH_stimM&quot;, &quot;cueL_stimM&quot;, &quot;cueH_stimL&quot;, &quot;cueL_stimL&quot; ) ) # ------------------------------------------------------------------------------ # summary statistics # ------------------------------------------------------------------------------ subjectwise &lt;- meanSummary(df_long, c(&quot;sub&quot;, &quot;tr_ordered&quot;, &quot;sixcond&quot;), &quot;tr_value&quot;) groupwise &lt;- summarySEwithin( data = subjectwise, measurevar = &quot;mean_per_sub&quot;, withinvars = c(&quot;sixcond&quot;, &quot;tr_ordered&quot;), idvar = &quot;sub&quot; ) groupwise$task &lt;- taskname # ---------------------------------------------------------------------------- # plot parameters # ---------------------------------------------------------------------------- # convert TR orders to numeric values tr_numbers &lt;- as.numeric(sub(&quot;tr&quot;, &quot;&quot;, as.character(groupwise$tr_ordered))) tr_sequence &lt;- (tr_numbers - 1) * 0.46 groupwise$tr_sequence &lt;- tr_sequence LINEIV1 &lt;- &quot;tr_sequence&quot; LINEIV2 &lt;- &quot;sixcond&quot; MEAN &lt;- &quot;mean_per_sub_norm_mean&quot; ERROR &lt;- &quot;se&quot; dv_keyword &lt;- &quot;actual&quot; sorted_indices &lt;- order(groupwise$tr_ordered) groupwise_sorted &lt;- groupwise[sorted_indices,] XLAB &lt;- &quot;TRs&quot; YLAB &lt;- &quot;Stimulus Epoch High vs. Low cue&quot; HIGHSTIM_COLOR &lt;- c( &quot;cueH_stimH&quot; = &quot;red&quot;, &quot;cueL_stimH&quot; = &quot;#5f0f40&quot;, &quot;cueH_stimM&quot; = &quot;gray&quot;, &quot;cueL_stimM&quot; = &quot;gray&quot;, &quot;cueH_stimL&quot; = &quot;gray&quot;, &quot;cueL_stimL&quot; = &quot;gray&quot; ) MEDSTIM_COLOR &lt;- c( &quot;cueH_stimH&quot; = &quot;gray&quot;, &quot;cueL_stimH&quot; = &quot;gray&quot;, &quot;cueH_stimM&quot; = &quot;#bc3908&quot;, &quot;cueL_stimM&quot; = &quot;#f6aa1c&quot;, &quot;cueH_stimL&quot; = &quot;gray&quot;, &quot;cueL_stimL&quot; = &quot;gray&quot; ) LOWSTIM_COLOR &lt;- c( &quot;cueH_stimH&quot; = &quot;gray&quot;, &quot;cueL_stimH&quot; = &quot;gray&quot;, &quot;cueH_stimM&quot; = &quot;gray&quot;, &quot;cueL_stimM&quot; = &quot;gray&quot;, &quot;cueH_stimL&quot; = &quot;#2541b2&quot;, &quot;cueL_stimL&quot; = &quot;#00a6fb&quot; ) AXIS_FONTSIZE &lt;- 10 COMMONAXIS_FONTSIZE &lt;- 15 TITLE_FONTSIZE &lt;- 20 figure_width &lt;- 10 # Adjust this to your actual figure width figure_height &lt;- 10 # Adjust this to your actual figure height GEOMPOINT_SIZE &lt;- calculate_point_size(figure_width, figure_height) # ---------------------------------------------------------------------------- # plot intensity per task # ---------------------------------------------------------------------------- p3H &lt;- plot_timeseries_bar( groupwise, LINEIV1, LINEIV2, MEAN, ERROR, XLAB, YLAB, ggtitle = paste0(tools::toTitleCase(run_type), &quot;\\n High intensity (N = &quot;, unique(groupwise$N), &quot;)&quot;), color_mapping = HIGHSTIM_COLOR, show_legend = FALSE, geompoint_size = GEOMPOINT_SIZE ) # Assuming tr_sequence is correct and has been added to groupwise # Calculate breaks to show every 10th TR breaks_to_show &lt;- seq(0, max(groupwise$tr_sequence), by = 0.46 * 5) labels_to_show &lt;- seq(0, max(groupwise$tr_sequence), by = 0.46 * 5) # It&#39;s important to ensure that both &#39;breaks_to_show&#39; and &#39;labels_to_show&#39; have the same length # If the lengths differ, we need to adjust them so they match if (length(breaks_to_show) != length(labels_to_show)) { # Assuming you want to keep all the breaks and just adjust the labels labels_to_show &lt;- labels_to_show[seq_along(breaks_to_show)] } # High intensity plot_list[[&quot;H&quot;]] &lt;- p3H + scale_x_continuous( breaks = breaks_to_show, # Set breaks at every 10th point labels = labels_to_show, # Use the calculated labels limits = range(groupwise$tr_sequence) # Set the limits based on the sequence ) + theme_classic() # Medium intensity p3M &lt;- plot_timeseries_bar( groupwise, LINEIV1, LINEIV2, MEAN, ERROR, XLAB, YLAB, ggtitle = paste0( tools::toTitleCase(run_type), &quot;\\n Medium intensity (N = &quot;, unique(groupwise$N), &quot;)&quot; ), color_mapping = MEDSTIM_COLOR, show_legend = FALSE, geompoint_size = GEOMPOINT_SIZE ) plot_list[[&quot;M&quot;]] &lt;- p3M + scale_x_continuous( breaks = breaks_to_show, # Set breaks at every 10th point labels = labels_to_show, # Use the calculated labels limits = range(groupwise$tr_sequence) # Set the limits based on the sequence ) + theme_classic() # Low intensity p3L &lt;- plot_timeseries_bar( groupwise, LINEIV1, LINEIV2, MEAN, ERROR, XLAB, YLAB, ggtitle = paste0(tools::toTitleCase(run_type), &quot;\\n Low intensity (N = &quot;, unique(groupwise$N), &quot;)&quot;), color_mapping = LOWSTIM_COLOR, show_legend = FALSE, geompoint_size = GEOMPOINT_SIZE ) plot_list[[&quot;L&quot;]] &lt;- p3L + scale_x_continuous( breaks = breaks_to_show, # Set breaks at every 10th point labels = labels_to_show, # Use the calculated labels limits = range(groupwise$tr_sequence) # Set the limits based on the sequence ) + theme_classic() # ---------------------------------------------------------------------------- # combine three tasks in one panel per ROI # ---------------------------------------------------------------------------- library(gridExtra) plot_list &lt;- lapply(plot_list, function(plot) { plot + theme( plot.margin = margin(5, 5, 5, 5), # Adjust plot margins if needed axis.title.y = element_blank(), # Remove y-axis title axis.title.x = element_blank(), axis.text.y = element_text(size = AXIS_FONTSIZE), # Increase y-axis text size axis.text.x = element_text(size = AXIS_FONTSIZE, angle = 30) ) }) combined_plot_per_run &lt;- ggpubr::ggarrange( plot_list[[&quot;H&quot;]], plot_list[[&quot;M&quot;]], plot_list[[&quot;L&quot;]], common.legend = FALSE, legend = &quot;none&quot;, ncol = 3, nrow = 1, widths = c(3, 3, 3), heights = c(.5, .5, .5), align = &quot;v&quot; ) # Add the combined plot for this run type to the list for the current ROI plot_list_per_roi[[run_type]] &lt;- combined_plot_per_run } # end of run loop # ---------------------------------------------------------------------------- # add commom legend # ---------------------------------------------------------------------------- legend_data &lt;- data.frame( sixcond = factor( c( &quot;cueH_stimH&quot;, &quot;cueL_stimH&quot;, &quot;cueH_stimM&quot;, &quot;cueL_stimM&quot;, &quot;cueH_stimL&quot;, &quot;cueL_stimL&quot; ) ), color = c(&quot;red&quot;, &quot;#5f0f40&quot;, &quot;#bc3908&quot;, &quot;#f6aa1c&quot;, &quot;#2541b2&quot;, &quot;#00a6fb&quot;), stringsAsFactors = FALSE ) legend_plot &lt;- ggplot(legend_data, aes(x = sixcond, y = 1, color = sixcond)) + geom_point() + scale_color_manual(values = legend_data$color) + theme_void() + theme(legend.position = &quot;bottom&quot;) + guides(color = guide_legend(title = &quot;Condition&quot;)) legend_grob &lt;- ggplotGrob(legend_plot)$grobs[[which(sapply(ggplotGrob(legend_plot)$grobs, function(x) x$name) == &quot;guide-box&quot;)]] heights &lt;- c(rep(1, length(run_types)), 2) # ---------------------------------------------------------------------------- # common axes for the 9 panels # ---------------------------------------------------------------------------- y_axis_label &lt;- textGrob( &quot;FIR BOLD \\n(high &gt; low cue; stimulus epoch)&quot;, rot = 90, gp = gpar(fontsize = COMMONAXIS_FONTSIZE) ) x_axis_label &lt;- textGrob( &quot;TR (0.46s per TR)&quot;, rot = 0, gp = gpar(fontsize = COMMONAXIS_FONTSIZE) ) num_rows &lt;- length(plot_list_per_roi) + 1 # +1 for the legend # ---------------------------------------------------------------------------- # combined plots across 3 tasks # ---------------------------------------------------------------------------- roi_combined_plot &lt;- do.call(grid.arrange, c(plot_list_per_roi, ncol = 1)) final_plot &lt;- grid.arrange( y_axis_label, arrangeGrob( roi_combined_plot, x_axis_label, legend_grob, ncol = 1, heights = c(11,.5, 1) ), ncol = 2, widths = c(1, 10), # Relative widths for the label, plots, and legend, top = textGrob(sprintf(&quot;%s Time series per task&quot;, roi), gp = gpar( fontsize = TITLE_FONTSIZE, fontface = &quot;bold&quot; )) # title parameter ) grid.draw(final_plot) # ---------------------------------------------------------------------------- # save all plots # ---------------------------------------------------------------------------- ggsave(file.path( save_dir, paste0(&quot;roi-&quot;, roi , &quot;_epoch-stim_desc-stimcuecomparison.png&quot;) ), all_plots[[roi]], width = 12, height = 20) } "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
